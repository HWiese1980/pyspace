<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Documentation of External and Wrapped Nodes &#8212; pySPACE documentation</title>
    
    <link rel="stylesheet" href="_static/pySPACE.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.3 release',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/pyspace-logo.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="ipmarkers" href="api/generated/pySPACE.tools.live.ipmarkers.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api/generated/pySPACE.tools.live.ipmarkers.html" title="ipmarkers"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pySPACE documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="content.html" accesskey="U">Table of Contents</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/pyspace-logo_small.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Documentation of External and Wrapped Nodes</a><ul>
<li><a class="reference internal" href="#scikit-nodes">Scikit Nodes</a><ul>
<li><a class="reference internal" href="#pyspace-missions-nodes-regression-scikit-decorators-optsvrregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.regression.scikit_decorators.OptSVRRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-ardregressionsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ARDRegressionSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-adaboostclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.AdaBoostClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-adaboostregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.AdaBoostRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-additivechi2samplertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.AdditiveChi2SamplerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-baggingclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BaggingClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-baggingregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BaggingRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-bayesianridgeregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BayesianRidgeRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-bernoullinbclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BernoulliNBClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-bernoullirbmtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BernoulliRBMTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-binarizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BinarizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-birchtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BirchTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-calibratedclassifiercvsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.CalibratedClassifierCVSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-countvectorizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.CountVectorizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-decisiontreeclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DecisionTreeClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-decisiontreeregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DecisionTreeRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-dictvectorizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DictVectorizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-dictionarylearningtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DictionaryLearningTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-elasticnetcvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ElasticNetCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-elasticnetregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ElasticNetRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-extratreeclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreeClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-extratreeregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreeRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-extratreesclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreesClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-extratreesregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreesRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-factoranalysistransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FactorAnalysisTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-featureagglomerationtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FeatureAgglomerationTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-featurehashertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FeatureHasherTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-forestregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ForestRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-functiontransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FunctionTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gaussiannbclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianNBClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gaussianprocessregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianProcessRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gaussianrandomprojectionhashtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionHashTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gaussianrandomprojectiontransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-genericunivariateselecttransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GenericUnivariateSelectTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gradientboostingclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GradientBoostingClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gradientboostingregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GradientBoostingRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-gridsearchcvtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GridSearchCVTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-hashingvectorizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.HashingVectorizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-imputertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ImputerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-incrementalpcatransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.IncrementalPCATransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-isomaptransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.IsomapTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-isotonicregressionsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.IsotonicRegressionSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-kneighborsclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KNeighborsClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-kneighborsregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KNeighborsRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-kernelcenterertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KernelCentererTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-kernelpcatransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KernelPCATransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-kernelridgeregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KernelRidgeRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-labelbinarizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelBinarizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-labelencodertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelEncoderTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-labelpropagationclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelPropagationClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-labelspreadingclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelSpreadingClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-larscvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LarsCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-larsregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LarsRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-lassocvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-lassolarscvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoLarsCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-lassolarsicregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoLarsICRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-lassolarsregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoLarsRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-lassoregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-latentdirichletallocationtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LatentDirichletAllocationTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-lineardiscriminantanalysisclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearDiscriminantAnalysisClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-linearregressionsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearRegressionSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-linearsvcclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearSVCClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-linearsvrregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearSVRRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-locallylinearembeddingtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LocallyLinearEmbeddingTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-logisticregressioncvclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LogisticRegressionCVClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-logisticregressionclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LogisticRegressionClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-maxabsscalertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MaxAbsScalerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-minmaxscalertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MinMaxScalerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-minibatchdictionarylearningtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MiniBatchDictionaryLearningTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-minibatchsparsepcatransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MiniBatchSparsePCATransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-multilabelbinarizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiLabelBinarizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-multitaskelasticnetcvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-multitaskelasticnetregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-multitasklassocvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-multitasklassoregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-multinomialnbclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultinomialNBClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-nmftransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NMFTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-nearestcentroidclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NearestCentroidClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-normalizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NormalizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-nusvcclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NuSVCClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-nusvrregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NuSVRRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-nystroemtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NystroemTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-onehotencodertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OneHotEncoderTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-onevsoneclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OneVsOneClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-onevsrestclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OneVsRestClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-orthogonalmatchingpursuitcvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-orthogonalmatchingpursuitregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-outputcodeclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OutputCodeClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-pcatransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PCATransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-passiveaggressiveclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-passiveaggressiveregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-patchextractortransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PatchExtractorTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-perceptronclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PerceptronClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-polynomialfeaturestransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PolynomialFeaturesTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-projectedgradientnmftransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ProjectedGradientNMFTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-quadraticdiscriminantanalysisclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.QuadraticDiscriminantAnalysisClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-ransacregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RANSACRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-rbfsamplertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RBFSamplerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-rfecvtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RFECVTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-rfetransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RFETransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-radiusneighborsclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-radiusneighborsregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomforestclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomForestClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomforestregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomForestRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomtreesembeddingtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomTreesEmbeddingTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomizedlassotransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedLassoTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomizedlogisticregressiontransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedLogisticRegressionTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomizedpcatransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedPCATransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-randomizedsearchcvtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedSearchCVTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-ridgecvregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeCVRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-ridgeclassifiercvsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeClassifierCVSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-ridgeclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-ridgeregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-robustscalertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RobustScalerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-sgdclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SGDClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-sgdregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SGDRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-svcclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SVCClassifierSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-svrregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-selectfdrtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFdrTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-selectfprtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFprTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-selectfrommodeltransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFromModelTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-selectfwetransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFweTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-selectkbesttransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectKBestTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-selectpercentiletransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectPercentileTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-skewedchi2samplertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SkewedChi2SamplerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-sparsecodertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SparseCoderTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-sparsepcatransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SparsePCATransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-sparserandomprojectiontransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SparseRandomProjectionTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-standardscalertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.StandardScalerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-tfidftransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TfidfTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-tfidfvectorizertransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TfidfVectorizerTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-theilsenregressorsklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TheilSenRegressorSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-truncatedsvdtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TruncatedSVDTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-variancethresholdtransformersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.VarianceThresholdTransformerSklearnNode</span></code></a></li>
<li><a class="reference internal" href="#pyspace-missions-nodes-scikit-nodes-votingclassifiersklearnnode"><code class="docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.VotingClassifierSklearnNode</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="api/generated/pySPACE.tools.live.ipmarkers.html"
                        title="previous chapter">ipmarkers</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/external_nodes.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="documentation-of-external-and-wrapped-nodes">
<span id="external-nodes"></span><h1>Documentation of External and Wrapped Nodes<a class="headerlink" href="#documentation-of-external-and-wrapped-nodes" title="Permalink to this headline">¶</a></h1>
<p>pySPACE comes along with wrappers to external algorithms.</p>
<p>For details on the usage of the nodes and for getting usage examples,
have a look at their documentation.
Module for external node wrapping: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.external.html#module-pySPACE.missions.nodes.external" title="pySPACE.missions.nodes.external"><code class="xref py py-mod docutils literal"><span class="pre">pySPACE.missions.nodes.external</span></code></a></p>
<div class="section" id="scikit-nodes">
<span id="id1"></span><h2>Scikit Nodes<a class="headerlink" href="#scikit-nodes" title="Permalink to this headline">¶</a></h2>
<p>Nodes from <a class="reference internal" href="api/generated/pySPACE.missions.nodes.scikit_nodes.html#module-pySPACE.missions.nodes.scikit_nodes" title="pySPACE.missions.nodes.scikit_nodes"><code class="xref py py-mod docutils literal"><span class="pre">scikits</span> <span class="pre">wrapper</span></code></a></p>
<div class="section" id="pyspace-missions-nodes-regression-scikit-decorators-optsvrregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.regression.scikit_decorators.OptSVRRegressorSklearnNode" title="pySPACE.missions.nodes.regression.scikit_decorators.OptSVRRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.regression.scikit_decorators.OptSVRRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-regression-scikit-decorators-optsvrregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.regression.scikit_decorators.OptSVRRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.regression.scikit_decorators.</code><code class="descname">OptSVRRegressorSklearnNode</code><span class="sig-paren">(</span><em>C=1</em>, <em>epsilon=0.1</em>, <em>kernel='rbf'</em>, <em>degree=3</em>, <em>gamma='auto'</em>, <em>coef0=0.0</em>, <em>shrinking=True</em>, <em>tol=0.001</em>, <em>verbose=False</em>, <em>max_iter=-1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pySPACE/missions/nodes/regression/scikit_decorators.html#OptSVRRegressorSklearnNode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pySPACE.missions.nodes.regression.scikit_decorators.OptSVRRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode</span></code></a></p>
<p>Decorator wrapper around SVRRegressorSklearnNode</p>
<p>Epsilon-Support Vector Regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.SVR.html">sklearn.svm.classes.SVR</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The free parameters in the model are C and epsilon.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Penalty parameter C of the error term.</dd>
<dt>epsilon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span><dd>Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
within which no penalty is associated in the training loss function
with points predicted within a distance epsilon from the actual
value.</dd>
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span><dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span><dd>Degree of the polynomial kernel function (&#8216;poly&#8217;).
Ignored by all other kernels.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=&#8217;auto&#8217;)</span><dd>Kernel coefficient for &#8216;rbf&#8217;, &#8216;poly&#8217; and &#8216;sigmoid&#8217;.
If gamma is &#8216;auto&#8217; then 1/n_features will be used instead.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span><dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>shrinking</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to use the shrinking heuristic.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span><dd>Tolerance for stopping criterion.</dd>
<dt>cache_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Specify the size of the kernel cache (in MB).</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span><dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span><dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [nSV, n_features]</span><dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_SV]</span><dd>Coefficients of the support vector in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=&#39;auto&#39;,</span>
<span class="go">    kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>NuSVR</dt>
<dd>Support Vector Machine for regression implemented using libsvm
using a parameter to control the number of support vectors.</dd>
<dt>LinearSVR</dt>
<dd>Scalable Linear Support Vector Machine for regression
implemented using liblinear.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OptSVRRegressorSklearnNode</strong></li>
<li><strong>OptSVRRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-ardregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ARDRegressionSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ARDRegressionSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ARDRegressionSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-ardregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ARDRegressionSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ARDRegressionSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ARDRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Bayesian ARD regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.bayes.ARDRegression.html">sklearn.linear_model.bayes.ARDRegression</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Fit the weights of a regression model, using an ARD prior. The weights of
the regression model are assumed to be in Gaussian distributions.
Also estimate the parameters lambda (precisions of the distributions of the
weights) and alpha (precision of the distribution of the noise).
The estimation is done by an iterative procedures (Evidence Maximization)</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of iterations. Default is 300</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Stop the algorithm if w has converged. Default is 1.e-3.</dd>
<dt>alpha_1</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the alpha parameter. Default is 1.e-6.</dd>
<dt>alpha_2</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the alpha parameter. Default is 1.e-6.</dd>
<dt>lambda_1</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the lambda parameter. Default is 1.e-6.</dd>
<dt>lambda_2</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the lambda parameter. Default is 1.e-6.</dd>
<dt>compute_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>If True, compute the objective function at each step of the model.
Default is False.</dd>
<dt>threshold_lambda</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>threshold for removing (pruning) weights with high precision from
the computation. Default is 1.e+4.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).
Default is True.</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True.</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>Verbose mode when fitting the model.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span><dd>Coefficients of the regression model (mean of distribution)</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>estimated precision of the noise.</dd>
<dt><code class="docutils literal"><span class="pre">lambda_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span><dd>estimated precisions of the weights.</dd>
<dt><code class="docutils literal"><span class="pre">sigma_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features, n_features)</span><dd>estimated variance-covariance matrix of the weights</dd>
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>if computed, value of the objective function (to be maximized)</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">ARDRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,</span>
<span class="go">        copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,</span>
<span class="go">        n_iter=300, normalize=False, threshold_lambda=10000.0, tol=0.001,</span>
<span class="go">        verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 1.])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_ard.py for an example.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ARDRegressionSklearn</strong></li>
<li><strong>ARDRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-adaboostclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.AdaBoostClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.AdaBoostClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.AdaBoostClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-adaboostclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.AdaBoostClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">AdaBoostClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.AdaBoostClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An AdaBoost classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.weight_boosting.AdaBoostClassifier.html">sklearn.ensemble.weight_boosting.AdaBoostClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost-SAMME [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object, optional (default=DecisionTreeClassifier)</span><dd>The base estimator from which the boosted ensemble is built.
Support for sample weighting is required, as well as proper <cite>classes_</cite>
and <cite>n_classes_</cite> attributes.</dd>
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=50)</span><dd>The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.)</span><dd>Learning rate shrinks the contribution of each classifier by
<code class="docutils literal"><span class="pre">learning_rate</span></code>. There is a trade-off between <code class="docutils literal"><span class="pre">learning_rate</span></code> and
<code class="docutils literal"><span class="pre">n_estimators</span></code>.</dd>
<dt>algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;SAMME&#8217;, &#8216;SAMME.R&#8217;}, optional (default=&#8217;SAMME.R&#8217;)</span><dd>If &#8216;SAMME.R&#8217; then use the SAMME.R real boosting algorithm.
<code class="docutils literal"><span class="pre">base_estimator</span></code> must support calculation of class probabilities.
If &#8216;SAMME&#8217; then use the SAMME discrete boosting algorithm.
The SAMME.R algorithm typically converges faster than SAMME,
achieving a lower test error with fewer boosting iterations.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of classifiers</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes]</span><dd>The classes labels.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of classes.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_weights_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span><dd>Weights for each estimator in the boosted ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_errors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span><dd>Classification error for each estimator in the boosted
ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances if supported by the <code class="docutils literal"><span class="pre">base_estimator</span></code>.</dd>
</dl>
<p>See also</p>
<p>AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Y. Freund, R. Schapire, &#8220;A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting&#8221;, 1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><ol class="first last upperalpha simple" start="10">
<li>Zhu, H. Zou, S. Rosset, T. Hastie, &#8220;Multi-class AdaBoost&#8221;, 2009.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>AdaBoostClassifierSklearnNode</strong></li>
<li><strong>AdaBoostClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-adaboostregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.AdaBoostRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.AdaBoostRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.AdaBoostRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-adaboostregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.AdaBoostRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">AdaBoostRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.AdaBoostRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An AdaBoost regressor.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.weight_boosting.AdaBoostRegressor.html">sklearn.ensemble.weight_boosting.AdaBoostRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost.R2 [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object, optional (default=DecisionTreeRegressor)</span><dd>The base estimator from which the boosted ensemble is built.
Support for sample weighting is required.</dd>
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=50)</span><dd>The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.)</span><dd>Learning rate shrinks the contribution of each regressor by
<code class="docutils literal"><span class="pre">learning_rate</span></code>. There is a trade-off between <code class="docutils literal"><span class="pre">learning_rate</span></code> and
<code class="docutils literal"><span class="pre">n_estimators</span></code>.</dd>
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;linear&#8217;, &#8216;square&#8217;, &#8216;exponential&#8217;}, optional (default=&#8217;linear&#8217;)</span><dd>The loss function to use when updating the weights after each
boosting iteration.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of classifiers</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_weights_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span><dd>Weights for each estimator in the boosted ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_errors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span><dd>Regression error for each estimator in the boosted ensemble.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances if supported by the <code class="docutils literal"><span class="pre">base_estimator</span></code>.</dd>
</dl>
<p>See also</p>
<p>AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Y. Freund, R. Schapire, &#8220;A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting&#8221;, 1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><ol class="first last upperalpha simple" start="8">
<li>Drucker, &#8220;Improving Regressors using Boosting Techniques&#8221;, 1997.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>AdaBoostRegressorSklearnNode</strong></li>
<li><strong>AdaBoostRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-additivechi2samplertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.AdditiveChi2SamplerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.AdditiveChi2SamplerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.AdditiveChi2SamplerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-additivechi2samplertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.AdditiveChi2SamplerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">AdditiveChi2SamplerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.AdditiveChi2SamplerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Approximate feature map for additive chi2 kernel.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html">sklearn.kernel_approximation.AdditiveChi2Sampler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Uses sampling the fourier transform of the kernel characteristic
at regular intervals.</p>
<p>Since the kernel that is to be approximated is additive, the components of
the input vectors can be treated separately.  Each entry in the original
space is transformed into 2*sample_steps+1 features, where sample_steps is
a parameter of the method. Typical values of sample_steps include 1, 2 and
3.</p>
<p>Optimal choices for the sampling interval for certain data ranges can be
computed (see the reference). The default values should be reasonable.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>sample_steps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Gives the number of (complex) sampling points.</dd>
<dt>sample_interval</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Sampling interval. Must be specified when sample_steps not in {1,2,3}.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>This estimator approximates a slightly different version of the additive
chi squared kernel then <code class="docutils literal"><span class="pre">metric.additive_chi2</span></code> computes.</p>
<p>See also</p>
<dl class="docutils">
<dt>SkewedChi2Sampler</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">A Fourier-approximation to a non-additive variant of</span><dd>the chi squared kernel.</dd>
</dl>
<p>sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.</p>
<dl class="docutils">
<dt>sklearn.metrics.pairwise.additive_chi2_kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">The exact additive chi</span><dd>squared kernel.</dd>
</dl>
<p><strong>References</strong></p>
<p>See <a class="reference external" href="http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf">&#8220;Efficient additive kernels via explicit feature maps&#8221;</a>
A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,
2011</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>AdditiveChi2SamplerTransformerSklearnNode</strong></li>
<li><strong>AdditiveChi2SamplerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-baggingclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BaggingClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BaggingClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BaggingClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-baggingclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BaggingClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BaggingClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BaggingClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>A Bagging classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.bagging.BaggingClassifier.html">sklearn.ensemble.bagging.BaggingClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id64"><span class="problematic" id="id65"><span id="id6"></span>[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id66"><span class="problematic" id="id67"><span id="id7"></span>[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id68"><span class="problematic" id="id69"><span id="id8"></span>[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id70"><span class="problematic" id="id71"><span id="id9"></span>[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object or None, optional (default=None)</span><dd>The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a decision tree.</dd>
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=10)</span><dd>The number of base estimators in the ensemble.</dd>
<dt>max_samples</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span><dd><dl class="first last docutils">
<dt>The number of samples to draw from X to train each base estimator.</dt>
<dd><ul class="first last simple">
<li>If int, then draw <cite>max_samples</cite> samples.</li>
<li>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</li>
</ul>
</dd>
</dl>
</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span><dd><dl class="first last docutils">
<dt>The number of features to draw from X to train each base estimator.</dt>
<dd><ul class="first last simple">
<li>If int, then draw <cite>max_features</cite> features.</li>
<li>If float, then draw <cite>max_features * X.shape[1]</cite> features.</li>
</ul>
</dd>
</dl>
</dd>
<dt>bootstrap</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether samples are drawn with replacement.</dd>
<dt>bootstrap_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd>Whether features are drawn with replacement.</dd>
<dt>oob_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd><p class="first">When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>warm_start</em> constructor parameter.</p>
</div>
</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">base_estimator_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of estimators</span><dd>The base estimator from which the ensemble is grown.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of estimators</span><dd>The collection of fitted base estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_samples_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span><dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span><dd>The subset of drawn features for each base estimator.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes]</span><dd>The classes labels.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span><dd>The number of classes.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_decision_function_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span><dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>L. Breiman, &#8220;Pasting small votes for classification in large
databases and on-line&#8221;, Machine Learning, 36(1), 85-103, 1999.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, &#8220;Bagging predictors&#8221;, Machine Learning, 24(2), 123-140,
1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Ho, &#8220;The random subspace method for constructing decision
forests&#8221;, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>G. Louppe and P. Geurts, &#8220;Ensembles on Random Patches&#8221;, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BaggingClassifierSklearn</strong></li>
<li><strong>BaggingClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-baggingregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BaggingRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BaggingRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BaggingRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-baggingregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BaggingRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BaggingRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BaggingRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>A Bagging regressor.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.bagging.BaggingRegressor.html">sklearn.ensemble.bagging.BaggingRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id72"><span class="problematic" id="id73"><span id="id14"></span>[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id74"><span class="problematic" id="id75"><span id="id15"></span>[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id76"><span class="problematic" id="id77"><span id="id16"></span>[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id78"><span class="problematic" id="id79"><span id="id17"></span>[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object or None, optional (default=None)</span><dd>The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a decision tree.</dd>
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=10)</span><dd>The number of base estimators in the ensemble.</dd>
<dt>max_samples</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span><dd><dl class="first last docutils">
<dt>The number of samples to draw from X to train each base estimator.</dt>
<dd><ul class="first last simple">
<li>If int, then draw <cite>max_samples</cite> samples.</li>
<li>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</li>
</ul>
</dd>
</dl>
</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1.0)</span><dd><dl class="first last docutils">
<dt>The number of features to draw from X to train each base estimator.</dt>
<dd><ul class="first last simple">
<li>If int, then draw <cite>max_features</cite> features.</li>
<li>If float, then draw <cite>max_features * X.shape[1]</cite> features.</li>
</ul>
</dd>
</dl>
</dd>
<dt>bootstrap</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether samples are drawn with replacement.</dd>
<dt>bootstrap_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd>Whether features are drawn with replacement.</dd>
<dt>oob_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the building process.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of estimators</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_samples_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span><dd>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span><dd>The subset of drawn features for each base estimator.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_prediction_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span><dd>Prediction computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_prediction_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>L. Breiman, &#8220;Pasting small votes for classification in large
databases and on-line&#8221;, Machine Learning, 36(1), 85-103, 1999.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, &#8220;Bagging predictors&#8221;, Machine Learning, 24(2), 123-140,
1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Ho, &#8220;The random subspace method for constructing decision
forests&#8221;, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>G. Louppe and P. Geurts, &#8220;Ensembles on Random Patches&#8221;, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BaggingRegressorSklearnNode</strong></li>
<li><strong>BaggingRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-bayesianridgeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BayesianRidgeRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BayesianRidgeRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BayesianRidgeRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-bayesianridgeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BayesianRidgeRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BayesianRidgeRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BayesianRidgeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Bayesian ridge regression</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.bayes.BayesianRidge.html">sklearn.linear_model.bayes.BayesianRidge</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Fit a Bayesian ridge model and optimize the regularization parameters
lambda (precision of the weights) and alpha (precision of the noise).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of iterations.  Default is 300.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Stop the algorithm if w has converged. Default is 1.e-3.</dd>
<dt>alpha_1</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the alpha parameter. Default is 1.e-6</dd>
<dt>alpha_2</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the alpha parameter.
Default is 1.e-6.</dd>
<dt>lambda_1</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : shape parameter for the Gamma distribution prior
over the lambda parameter. Default is 1.e-6.</dd>
<dt>lambda_2</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the lambda parameter.
Default is 1.e-6</dd>
<dt>compute_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>If True, compute the objective function at each step of the model.
Default is False</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).
Default is True.</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>Verbose mode when fitting the model.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span><dd>Coefficients of the regression model (mean of distribution)</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>estimated precision of the noise.</dd>
<dt><code class="docutils literal"><span class="pre">lambda_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span><dd>estimated precisions of the weights.</dd>
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>if computed, value of the objective function (to be maximized)</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,</span>
<span class="go">        copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06,</span>
<span class="go">        n_iter=300, normalize=False, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 1.])</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_bayesian_ridge.py for an example.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BayesianRidgeRegressorSklearn</strong></li>
<li><strong>BayesianRidgeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-bernoullinbclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BernoulliNBClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BernoulliNBClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BernoulliNBClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-bernoullinbclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BernoulliNBClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BernoulliNBClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BernoulliNBClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Naive Bayes classifier for multivariate Bernoulli models.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html">sklearn.naive_bayes.BernoulliNB</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>binarize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float or None, optional</span><dd>Threshold for binarizing (mapping to booleans) of sample features.
If None, input is presumed to already consist of binary vectors.</dd>
<dt>fit_prior</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size=[n_classes,]</span><dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_log_prior_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span><dd>Log probability of each class (smoothed).</dd>
<dt><code class="docutils literal"><span class="pre">feature_log_prob_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span><dd>Empirical log probability of features given a class, P(x_i|y).</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span><dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><code class="docutils literal"><span class="pre">feature_count_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes, n_features]</span><dd>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">BernoulliNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
<a class="reference external" href="http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</a></p>
<p>A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41-48.</p>
<p>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes &#8211; Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BernoulliNBClassifierSklearnNode</strong></li>
<li><strong>BernoulliNBClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-bernoullirbmtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BernoulliRBMTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BernoulliRBMTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BernoulliRBMTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-bernoullirbmtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BernoulliRBMTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BernoulliRBMTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BernoulliRBMTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Bernoulli Restricted Boltzmann Machine (RBM).</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.rbm.BernoulliRBM.html">sklearn.neural_network.rbm.BernoulliRBM</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A Restricted Boltzmann Machine with binary visible units and
binary hiddens. Parameters are estimated using Stochastic Maximum
Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)
[2].</p>
<p>The time complexity of this implementation is <code class="docutils literal"><span class="pre">O(d</span> <span class="pre">**</span> <span class="pre">2)</span></code> assuming
d ~ n_features ~ n_components.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of binary hidden units.</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The learning rate for weight updates. It is <em>highly</em> recommended
to tune this hyper-parameter. Reasonable values are in the
10**[0., -3.] range.</dd>
<dt>batch_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of examples per minibatch.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of iterations/sweeps over the training dataset to perform
during training.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The verbosity level. The default, zero, means silent mode.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or numpy.RandomState, optional</span><dd>A random number generator instance to define the state of the
random permutations generator. If an integer is given, it fixes the
seed. Defaults to the global numpy random number generator.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_hidden_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components,)</span><dd>Biases of the hidden units.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_visible_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features,)</span><dd>Biases of the visible units.</dd>
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_components, n_features)</span><dd>Weight matrix, where n_features in the number of
visible units and n_components is the number of hidden units.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="k">import</span> <span class="n">BernoulliRBM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BernoulliRBM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,</span>
<span class="go">       random_state=None, verbose=0)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for</dt>
<dd>deep belief nets. Neural Computation 18, pp 1527-1554.
<a class="reference external" href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf</a></dd>
<dt>[2] Tieleman, T. Training Restricted Boltzmann Machines using</dt>
<dd>Approximations to the Likelihood Gradient. International Conference
on Machine Learning (ICML) 2008</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BernoulliRBMTransformerSklearnNode</strong></li>
<li><strong>BernoulliRBMTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-binarizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BinarizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BinarizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BinarizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-binarizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BinarizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BinarizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BinarizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Binarize data (set feature values to 0 or 1) according to a threshold</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.Binarizer.html">sklearn.preprocessing.data.Binarizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Values greater than the threshold map to 1, while values less than
or equal to the threshold map to 0. With the default threshold of 0,
only positive values map to 1.</p>
<p>Binarization is a common operation on text count data where the
analyst can decide to only consider the presence or absence of a
feature rather than a quantified number of occurrences for instance.</p>
<p>It can also be used as a pre-processing step for estimators that
consider boolean random variables (e.g. modelled using the Bernoulli
distribution in a Bayesian setting).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (0.0 by default)</span><dd>Feature values below or equal to this are replaced by 0, above it by 1.
Threshold may not be less than 0 for operations on sparse matrices.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>set to False to perform inplace binarization and avoid a copy (if
the input is already a numpy array or a scipy.sparse CSR matrix).</dd>
</dl>
<p><strong>Notes</strong></p>
<p>If the input is a sparse matrix, only the non-zero values are subject
to update by the Binarizer class.</p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BinarizerTransformerSklearn</strong></li>
<li><strong>BinarizerTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-birchtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.BirchTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.BirchTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.BirchTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-birchtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.BirchTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">BirchTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.BirchTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Implements the Birch clustering algorithm.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.birch.Birch.html">sklearn.cluster.birch.Birch</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Every new sample is inserted into the root of the Clustering Feature
Tree. It is then clubbed together with the subcluster that has the
centroid closest to the new sample. This is done recursively till it
ends up at the subcluster of the leaf of the tree has the closest centroid.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default 0.5</span><dd>The radius of the subcluster obtained by merging a new sample and the
closest subcluster should be lesser than the threshold. Otherwise a new
subcluster is started.</dd>
<dt>branching_factor</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default 50</span><dd>Maximum number of CF subclusters in each node. If a new samples enters
such that the number of subclusters exceed the branching_factor then
the node has to be split. The corresponding parent also has to be
split and if the number of subclusters in the parent is greater than
the branching factor, then it has to be split recursively.</dd>
<dt>n_clusters</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, instance of sklearn.cluster model, default None</span><dd>Number of clusters after the final clustering step, which treats the
subclusters from the leaves as new samples. By default, this final
clustering step is not performed and the subclusters are returned
as they are. If a model is provided, the model is fit treating
the subclusters as new samples and the initial data is mapped to the
label of the closest subcluster. If an int is provided, the model
fit is AgglomerativeClustering with n_clusters set to the int.</dd>
<dt>compute_labels</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default True</span><dd>Whether or not to compute labels for each fit.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default True</span><dd>Whether or not to make a copy of the given data. If set to False,
the initial data will be overwritten.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">root_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">_CFNode</span><dd>Root of the CFTree.</dd>
<dt><code class="docutils literal"><span class="pre">dummy_leaf_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">_CFNode</span><dd>Start pointer to all the leaves.</dd>
<dt><code class="docutils literal"><span class="pre">subcluster_centers_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray,</span><dd>Centroids of all subclusters read directly from the leaves.</dd>
<dt><code class="docutils literal"><span class="pre">subcluster_labels_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray,</span><dd>Labels assigned to the centroids of the subclusters after
they are clustered globally.</dd>
<dt><code class="docutils literal"><span class="pre">labels_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_samples,)</span><dd>Array of labels assigned to the input data.
if partial_fit is used instead of fit, they are assigned to the
last batch of data.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">Birch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brc</span> <span class="o">=</span> <span class="n">Birch</span><span class="p">(</span><span class="n">branching_factor</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span><span class="n">compute_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">Birch(branching_factor=50, compute_labels=True, copy=True, n_clusters=None,</span>
<span class="go">   threshold=0.5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 0, 0, 1, 1, 1])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<ul class="simple">
<li>Tian Zhang, Raghu Ramakrishnan, Maron Livny
BIRCH: An efficient data clustering method for large databases.
<a class="reference external" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf</a></li>
<li>Roberto Perdisci
JBirch - Java implementation of BIRCH clustering algorithm
<a class="reference external" href="https://code.google.com/p/jbirch/">https://code.google.com/p/jbirch/</a></li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>BirchTransformerSklearnNode</strong></li>
<li><strong>BirchTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-calibratedclassifiercvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.CalibratedClassifierCVSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.CalibratedClassifierCVSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.CalibratedClassifierCVSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-calibratedclassifiercvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.CalibratedClassifierCVSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">CalibratedClassifierCVSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.CalibratedClassifierCVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Probability calibration with isotonic regression or sigmoid.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html">sklearn.calibration.CalibratedClassifierCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>With this class, the base_estimator is fit on the train set of the
cross-validation generator and the test set is used for calibration.
The probabilities for each of the folds are then averaged
for prediction. In case that cv=&#8221;prefit&#8221; is passed to <code class="docutils literal"><span class="pre">__init__</span></code>,
it is it is assumed that base_estimator has been
fitted already and all data is used for calibration. Note that
data for fitting the classifier and for calibrating it must be disjoint.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">instance BaseEstimator</span><dd>The classifier whose output decision function needs to be calibrated
to offer more accurate predict_proba outputs. If cv=prefit, the
classifier must have been fit already on data.</dd>
<dt>method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;sigmoid&#8217; or &#8216;isotonic&#8217;</span><dd>The method to use for calibration. Can be &#8216;sigmoid&#8217; which
corresponds to Platt&#8217;s method or &#8216;isotonic&#8217; which is a
non-parameteric approach. It is not advised to use isotonic calibration
with too few calibration samples <code class="docutils literal"><span class="pre">(&lt;&lt;1000)</span></code> since it tends to overfit.
Use sigmoids (Platt&#8217;s calibration) in this case.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, cross-validation generator, iterable or &#8220;prefit&#8221;, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> used. If <code class="docutils literal"><span class="pre">y</span></code> is neither binary nor
multiclass, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<p class="last">If &#8220;prefit&#8221; is passed, it is assumed that base_estimator has been
fitted already and all data is used for calibration.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes)</span><dd>The class labels.</dd>
<dt><a href="#id80"><span class="problematic" id="id81">calibrated_classifiers_</span></a>: list (len() equal to cv or 1 if cv == &#8220;prefit&#8221;)</dt>
<dd>The list of calibrated classifiers, one for each crossvalidation fold,
which has been fitted on all but the validation fold and calibrated
on the validation fold.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Obtaining calibrated probability estimates from decision trees
and naive Bayesian classifiers, B. Zadrozny &amp; C. Elkan, ICML 2001</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Transforming Classifier Scores into Accurate Multiclass
Probability Estimates, B. Zadrozny &amp; C. Elkan, (KDD 2002)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>Probabilistic Outputs for Support Vector Machines and Comparisons to
Regularized Likelihood Methods, J. Platt, (1999)</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>Predicting Good Probabilities with Supervised Learning,
A. Niculescu-Mizil &amp; R. Caruana, ICML 2005</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>CalibratedClassifierCVSklearnNode</strong></li>
<li><strong>CalibratedClassifierCVSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-countvectorizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.CountVectorizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.CountVectorizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.CountVectorizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-countvectorizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.CountVectorizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">CountVectorizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.CountVectorizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Convert a collection of text documents to a matrix of token counts</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">sklearn.feature_extraction.text.CountVectorizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This implementation produces a sparse representation of the counts using
scipy.sparse.coo_matrix.</p>
<p>If you do not provide an a-priori dictionary and you do not use an analyzer
that does some kind of feature selection then the number of features will
be equal to the vocabulary size found by analyzing the data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span><dd><p class="first">If &#8216;filename&#8217;, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have a &#8216;read&#8217; method (file-like
object) that is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span><dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span><dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span><dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;, &#8216;char_wb&#8217;} or callable</span><dd><p class="first">Whether the feature should be made of word or character n-grams.
Option &#8216;char_wb&#8217; creates character n-grams only from text inside
word boundaries.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
</dd>
<dt>preprocessor</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span><dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span><dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</dd>
<dt>ngram_range</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span><dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span><dd><p class="first">If &#8216;english&#8217;, a built-in stop word list for English is used.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>lowercase</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span><dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>token_pattern</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string</span><dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>. The default regexp select tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>max_df</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, default=1.0</span><dd>When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, default=1</span><dd>When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, default=None</span><dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span><dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents. Indices
in the mapping should not be repeated and should not have any gap
between 0 and the largest index.</dd>
<dt>binary</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span><dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span><dd>Type of the matrix returned by fit_transform() or transform().</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">vocabulary_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>A mapping of terms to feature indices.</dd>
<dt><code class="docutils literal"><span class="pre">stop_words_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">set</span><dd><p class="first">Terms that were ignored because they either:</p>
<blockquote>
<div><ul class="simple">
<li>occurred in too many documents (<cite>max_df</cite>)</li>
<li>occurred in too few documents (<cite>min_df</cite>)</li>
<li>were cut off by feature selection (<cite>max_features</cite>).</li>
</ul>
</div></blockquote>
<p class="last">This is only available if no vocabulary was given.</p>
</dd>
</dl>
<p>See also</p>
<p>HashingVectorizer, TfidfVectorizer</p>
<p><strong>Notes</strong></p>
<p>The <code class="docutils literal"><span class="pre">stop_words_</span></code> attribute can get large and increase the model size
when pickling. This attribute is provided only for introspection and can
be safely removed using delattr or set to None before pickling.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>CountVectorizerTransformerSklearn</strong></li>
<li><strong>CountVectorizerTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-decisiontreeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.DecisionTreeClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.DecisionTreeClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DecisionTreeClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-decisiontreeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.DecisionTreeClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">DecisionTreeClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.DecisionTreeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>A decision tree classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.tree.DecisionTreeClassifier.html">sklearn.tree.tree.DecisionTreeClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span><dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.</dd>
<dt>splitter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;best&#8221;)</span><dd>The strategy used to choose the split at each node. Supported
strategies are &#8220;best&#8221; to choose the best split and &#8220;random&#8221; to choose
the best random split.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1)</span><dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, &#8220;balanced&#8221; or None, optional (default=None)</span><dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p class="last">Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>presort</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>Whether to presort the data to speed up the finding of best splits in
fitting. For the default settings of a decision tree on large
datasets, setting this to true may slow down the training process.
When using either a smaller dataset or a restricted depth, this may
speed up the training.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span><dd>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances. The higher, the more important the
feature. The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance <a href="#id82"><span class="problematic" id="id83"><span id="id26"></span>[4]_</span></a>.</dd>
<dt><code class="docutils literal"><span class="pre">max_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>The inferred value of max_features.</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span><dd>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">tree_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span><dd>The underlying Tree object.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &#8220;Classification
and Regression Trees&#8221;, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. &#8220;Elements of Statistical
Learning&#8221;, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id30" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, &#8220;Random Forests&#8221;,
<a class="reference external" href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="gp">...</span>
<span class="go">array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,</span>
<span class="go">        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>DecisionTreeClassifierSklearnNode</strong></li>
<li><strong>DecisionTreeClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-decisiontreeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.DecisionTreeRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.DecisionTreeRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DecisionTreeRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-decisiontreeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.DecisionTreeRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">DecisionTreeRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.DecisionTreeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>A decision tree regressor.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.tree.DecisionTreeRegressor.html">sklearn.tree.tree.DecisionTreeRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span><dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error, which is equal to
variance reduction as feature selection criterion.</dd>
<dt>splitter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;best&#8221;)</span><dd>The strategy used to choose the split at each node. Supported
strategies are &#8220;best&#8221; to choose the best split and &#8220;random&#8221; to choose
the best random split.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1)</span><dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>presort</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>Whether to presort the data to speed up the finding of best splits in
fitting. For the default settings of a decision tree on large
datasets, setting this to true may slow down the training process.
When using either a smaller dataset or a restricted depth, this may
speed up the training.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the
(normalized) total reduction of the criterion brought
by that feature. It is also known as the Gini importance <a href="#id84"><span class="problematic" id="id85"><span id="id31"></span>[4]_</span></a>.</dd>
<dt><code class="docutils literal"><span class="pre">max_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>The inferred value of max_features.</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">tree_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span><dd>The underlying Tree object.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeClassifier</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id32" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &#8220;Classification
and Regression Trees&#8221;, Wadsworth, Belmont, CA, 1984.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id34" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>T. Hastie, R. Tibshirani and J. Friedman. &#8220;Elements of Statistical
Learning&#8221;, Springer, 2009.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>L. Breiman, and A. Cutler, &#8220;Random Forests&#8221;,
<a class="reference external" href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></td></tr>
</tbody>
</table>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_boston</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                   
<span class="gp">...</span>
<span class="go">array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,</span>
<span class="go">        0.07..., 0.29..., 0.33..., -1.42..., -1.77...])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>DecisionTreeRegressorSklearn</strong></li>
<li><strong>DecisionTreeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-dictvectorizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.DictVectorizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.DictVectorizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DictVectorizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-dictvectorizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.DictVectorizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">DictVectorizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.DictVectorizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Transforms lists of feature-value mappings to vectors.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.dict_vectorizer.DictVectorizer.html">sklearn.feature_extraction.dict_vectorizer.DictVectorizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This transformer turns lists of mappings (dict-like objects) of feature
names to feature values into Numpy arrays or scipy.sparse matrices for use
with scikit-learn estimators.</p>
<p>When feature values are strings, this transformer will do a binary one-hot
(aka one-of-K) coding: one boolean-valued feature is constructed for each
of the possible string values that the feature can take on. For instance,
a feature &#8220;f&#8221; that can take on the values &#8220;ham&#8221; and &#8220;spam&#8221; will become two
features in the output, one signifying &#8220;f=ham&#8221;, the other &#8220;f=spam&#8221;.</p>
<p>Features that do not occur in a sample (mapping) will have a zero value
in the resulting array/matrix.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dtype</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span><dd>The type of feature values. Passed to Numpy array/scipy.sparse matrix
constructors as the dtype argument.</dd>
<dt>separator: string, optional</dt>
<dd>Separator string used when constructing new features for one-hot
coding.</dd>
<dt>sparse: boolean, optional.</dt>
<dd>Whether transform should produce scipy.sparse matrices.
True by default.</dd>
<dt>sort: boolean, optional.</dt>
<dd>Whether <code class="docutils literal"><span class="pre">feature_names_</span></code> and <code class="docutils literal"><span class="pre">vocabulary_</span></code> should be sorted when fitting.
True by default.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">vocabulary_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>A dictionary mapping feature names to feature indices.</dd>
<dt><code class="docutils literal"><span class="pre">feature_names_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list</span><dd>A list of length n_features containing the feature names (e.g., &#8220;f=ham&#8221;
and &#8220;f=spam&#8221;).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="k">import</span> <span class="n">DictVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;baz&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[ 2.,  0.,  1.],</span>
<span class="go">       [ 0.,  1.,  3.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span>         <span class="p">[{</span><span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span> <span class="p">{</span><span class="s1">&#39;baz&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mf">3.0</span><span class="p">}]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">transform</span><span class="p">({</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;unseen_feature&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">})</span>
<span class="go">array([[ 0.,  0.,  4.]])</span>
</pre></div>
</div>
<p>See also</p>
<p>FeatureHasher : performs vectorization using only a hash function.
sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features</p>
<blockquote>
<div>encoded as columns of integers.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>DictVectorizerTransformerSklearnNode</strong></li>
<li><strong>DictVectorizerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-dictionarylearningtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.DictionaryLearningTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.DictionaryLearningTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.DictionaryLearningTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-dictionarylearningtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.DictionaryLearningTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">DictionaryLearningTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.DictionaryLearningTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Dictionary learning</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning.DictionaryLearning.html">sklearn.decomposition.dict_learning.DictionaryLearning</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">U</span><span class="o">^*</span><span class="p">,</span><span class="n">V</span><span class="o">^*</span><span class="p">)</span> <span class="o">=</span> <span class="n">argmin</span> <span class="mf">0.5</span> <span class="o">||</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">U</span> <span class="n">V</span> <span class="o">||</span><span class="n">_2</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span> <span class="n">U</span> <span class="o">||</span><span class="n">_1</span>
            <span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
            <span class="k">with</span> <span class="o">||</span> <span class="n">V_k</span> <span class="o">||</span><span class="n">_2</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">for</span> <span class="nb">all</span>  <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n_components</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of dictionary elements to extract</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>sparsity controlling parameter</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>maximum number of iterations to perform</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>tolerance for numerical error</dd>
<dt>fit_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span><dd><p class="first">lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>cd</em> coordinate descent method to improve speed.</p>
</div>
</dd>
<dt>transform_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span><dd><p class="first">Algorithm used to transform the data
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection <code class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></code></p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>lasso_cd</em> coordinate descent method to improve speed.</p>
</div>
</dd>
<dt>transform_n_nonzero_coefs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, <code class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></code> by default</span><dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span><dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span><dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of parallel jobs to run</dd>
<dt>code_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span><dd>initial value for the code, for warm restart</dd>
<dt>dict_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span><dd>initial values for the dictionary, for warm restart</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of verbosity of the printed output</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>dictionary atoms extracted from the data</dd>
<dt><code class="docutils literal"><span class="pre">error_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>vector of errors at each iteration</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>DictionaryLearningTransformerSklearnNode</strong></li>
<li><strong>DictionaryLearningTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-elasticnetcvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ElasticNetCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ElasticNetCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ElasticNetCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-elasticnetcvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ElasticNetCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ElasticNetCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ElasticNetCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Elastic Net model with iterative fitting along a regularization path</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.ElasticNetCV.html">sklearn.linear_model.coordinate_descent.ElasticNetCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The best model is selected by cross-validation.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float or array of floats, optional</span><dd>float between 0 and 1 passed to ElasticNet (scaling between
l1 and l2 penalties). For <code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code>
the penalty is an L2 penalty. For <code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it is an L1 penalty.
For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1 and L2
This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of
values for l1_ratio is often to put more values close to 1
(i.e. Lasso) and less close to 0 (i.e. Ridge), as in <code class="docutils literal"><span class="pre">[.1,</span> <span class="pre">.5,</span> <span class="pre">.7,</span>
<span class="pre">.9,</span> <span class="pre">.95,</span> <span class="pre">.99,</span> <span class="pre">1]</span></code></dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>n_alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of alphas along the regularization path, used for each l1_ratio.</dd>
<dt>alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, optional</span><dd>List of alphas where to compute the models.
If None alphas are set automatically</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span><dd>Amount of verbosity.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs.</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">l1_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The compromise between l1 and l2 penalization chosen by
cross validation</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span><dd>Parameter vector (w in the cost function formula),</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets, n_features)</span><dd>Independent term in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_l1_ratio, n_alpha, n_folds)</span><dd>Mean square error for the test set on each fold, varying l1_ratio and
alpha.</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)</span><dd>The grid of alphas used for fitting, for each l1_ratio.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/lasso_path_with_crossvalidation.py
for an example.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package
while alpha corresponds to the lambda parameter in glmnet.
More specifically, the optimization objective is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">L2</span>
</pre></div>
</div>
<p>for:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>See also</p>
<p>enet_path
ElasticNet</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ElasticNetCVRegressorSklearnNode</strong></li>
<li><strong>ElasticNetCVRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-elasticnetregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ElasticNetRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ElasticNetRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ElasticNetRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-elasticnetregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ElasticNetRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ElasticNetRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ElasticNetRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear regression with combined L1 and L2 priors as regularizer.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.ElasticNet.html">sklearn.linear_model.coordinate_descent.ElasticNet</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Minimizes the objective function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">L2</span>
</pre></div>
</div>
<p>where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package while
alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio
= 1 is the lasso penalty. Currently, l1_ratio &lt;= 0.01 is not reliable,
unless you supply your own sequence of alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Constant that multiplies the penalty terms. Defaults to 1.0
See the notes for the exact mathematical meaning of this
parameter.
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object. For numerical
reasons, using <code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the Lasso object is not advised
and you should prefer the LinearRegression object.</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The ElasticNet mixing parameter, with <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;=</span> <span class="pre">1</span></code>. For
<code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code> the penalty is an L2 penalty. <code class="docutils literal"><span class="pre">For</span> <span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it
is an L1 penalty.  For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a
combination of L1 and L2.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether the intercept should be estimated or not. If <code class="docutils literal"><span class="pre">False</span></code>, the
data is assumed to be already centered.</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument. For sparse input
this option is always <code class="docutils literal"><span class="pre">True</span></code> to preserve sparsity.
WARNING : The <code class="docutils literal"><span class="pre">'auto'</span></code> option is deprecated and will
be removed in 0.18.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span><dd>parameter vector (w in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">sparse_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)</span><dd><code class="docutils literal"><span class="pre">sparse_coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">coef_</span></code></dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_targets,)</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>See also</p>
<p>SGDRegressor: implements elastic net regression with incremental training.
SGDClassifier: implements logistic regression with elastic net penalty</p>
<blockquote>
<div>(<code class="docutils literal"><span class="pre">SGDClassifier(loss=&quot;log&quot;,</span> <span class="pre">penalty=&quot;elasticnet&quot;)</span></code>).</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ElasticNetRegressorSklearnNode</strong></li>
<li><strong>ElasticNetRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-extratreeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreeClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ExtraTreeClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreeClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-extratreeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ExtraTreeClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ExtraTreeClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An extremely randomized tree classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.tree.ExtraTreeClassifier.html">sklearn.tree.tree.ExtraTreeClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p>See also</p>
<p>ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id36" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ExtraTreeClassifierSklearn</strong></li>
<li><strong>ExtraTreeClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-extratreeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreeRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ExtraTreeRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreeRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-extratreeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ExtraTreeRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ExtraTreeRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An extremely randomized tree regressor.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.tree.ExtraTreeRegressor.html">sklearn.tree.tree.ExtraTreeRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p>See also</p>
<p>ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id37" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ExtraTreeRegressorSklearn</strong></li>
<li><strong>ExtraTreeRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-extratreesclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreesClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ExtraTreesClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreesClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-extratreesclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ExtraTreesClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ExtraTreesClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreesClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An extra-trees classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.forest.ExtraTreesClassifier.html">sklearn.ensemble.forest.ExtraTreesClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and use averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span><dd>The number of trees in the forest.</dd>
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span><dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a percentage and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.
Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span><dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<code class="docutils literal"><span class="pre">min_samples_leaf</span></code> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.
Note: this parameter is tree-specific.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd>Whether bootstrap samples are used when building trees.</dd>
<dt>oob_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the tree building process.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest.</dd>
</dl>
<p>class_weight : dict, list of dicts, &#8220;balanced&#8221;, &#8220;balanced_subsample&#8221; or None, optional</p>
<blockquote>
<div><p>Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The &#8220;balanced_subsample&#8221; mode is the same as &#8220;balanced&#8221; except that weights are
computed based on the bootstrap sample for every tree grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</div></blockquote>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span><dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span><dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_decision_function_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span><dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id38" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.
RandomForestClassifier : Ensemble Classifier based on trees with optimal</p>
<blockquote>
<div>splits.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ExtraTreesClassifierSklearnNode</strong></li>
<li><strong>ExtraTreesClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-extratreesregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreesRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ExtraTreesRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ExtraTreesRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-extratreesregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ExtraTreesRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ExtraTreesRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ExtraTreesRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An extra-trees regressor.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.forest.ExtraTreesRegressor.html">sklearn.ensemble.forest.ExtraTreesRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and use averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span><dd>The number of trees in the forest.</dd>
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span><dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.
Note: this parameter is tree-specific.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a percentage and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.
Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span><dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<code class="docutils literal"><span class="pre">min_samples_leaf</span></code> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.
Note: this parameter is tree-specific.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd>Whether bootstrap samples are used when building trees.
Note: this parameter is tree-specific.</dd>
<dt>oob_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the tree building process.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeRegressor</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of features.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of outputs.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_prediction_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span><dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<p>See also</p>
<p>sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.
RandomForestRegressor: Ensemble regressor using trees with optimal splits.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ExtraTreesRegressorSklearnNode</strong></li>
<li><strong>ExtraTreesRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-factoranalysistransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.FactorAnalysisTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.FactorAnalysisTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FactorAnalysisTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-factoranalysistransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.FactorAnalysisTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">FactorAnalysisTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.FactorAnalysisTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Factor Analysis (FA)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.factor_analysis.FactorAnalysis.html">sklearn.decomposition.factor_analysis.FactorAnalysis</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A simple linear generative model with Gaussian latent variables.</p>
<p>The observations are assumed to be caused by a linear transformation of
lower dimensional latent factors and added Gaussian noise.
Without loss of generality the factors are distributed according to a
Gaussian with zero mean and unit covariance. The noise is also zero mean
and has an arbitrary diagonal covariance matrix.</p>
<p>If we would restrict the model further, by assuming that the Gaussian
noise is even isotropic (all diagonal entries are the same) we would obtain
<code class="xref py py-class docutils literal"><span class="pre">PPCA</span></code>.</p>
<p>FactorAnalysis performs a maximum likelihood estimate of the so-called
<cite>loading</cite> matrix, the transformation of the latent variables to the
observed ones, using expectation-maximization (EM).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int | None</span><dd>Dimensionality of latent space, the number of components
of <code class="docutils literal"><span class="pre">X</span></code> that are obtained after <code class="docutils literal"><span class="pre">transform</span></code>.
If None, n_components is set to the number of features.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Stopping tolerance for EM algorithm.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether to make a copy of X. If <code class="docutils literal"><span class="pre">False</span></code>, the input X gets overwritten
during fitting.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Maximum number of iterations.</dd>
<dt>noise_variance_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">None | array, shape=(n_features,)</span><dd>The initial guess of the noise variance for each feature.
If None, it defaults to np.ones(n_features)</dd>
<dt>svd_method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lapack&#8217;, &#8216;randomized&#8217;}</span><dd>Which SVD method to use. If &#8216;lapack&#8217; use standard SVD from
scipy.linalg, if &#8216;randomized&#8217; use fast <code class="docutils literal"><span class="pre">randomized_svd</span></code> function.
Defaults to &#8216;randomized&#8217;. For most applications &#8216;randomized&#8217; will
be sufficiently precise while providing significant speed gains.
Accuracy can also be improved by setting higher values for
<cite>iterated_power</cite>. If this is not sufficient, for maximum precision
you should choose &#8216;lapack&#8217;.</dd>
<dt>iterated_power</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of iterations for the power method. 3 by default. Only used
if <code class="docutils literal"><span class="pre">svd_method</span></code> equals &#8216;randomized&#8217;</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo number generator state used for random sampling. Only used
if <code class="docutils literal"><span class="pre">svd_method</span></code> equals &#8216;randomized&#8217;</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Components with maximum variance.</dd>
<dt><code class="docutils literal"><span class="pre">loglike_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list, [n_iterations]</span><dd>The log likelihood at each iteration.</dd>
<dt><code class="docutils literal"><span class="pre">noise_variance_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape=(n_features,)</span><dd>The estimated noise variance for each feature.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p><strong>References</strong></p>
<p>See also</p>
<dl class="docutils">
<dt>PCA: Principal component analysis is also a latent linear variable model</dt>
<dd>which however assumes equal noise variance for each feature.
This extra assumption makes probabilistic PCA faster as it can be
computed in closed form.</dd>
<dt>FastICA: Independent component analysis, a latent variable model with</dt>
<dd>non-Gaussian latent variables.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>FactorAnalysisTransformerSklearn</strong></li>
<li><strong>FactorAnalysisTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-featureagglomerationtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.FeatureAgglomerationTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.FeatureAgglomerationTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FeatureAgglomerationTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-featureagglomerationtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.FeatureAgglomerationTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">FeatureAgglomerationTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.FeatureAgglomerationTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Agglomerate features.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.hierarchical.FeatureAgglomeration.html">sklearn.cluster.hierarchical.FeatureAgglomeration</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Similar to AgglomerativeClustering, but recursively merges features
instead of samples.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_clusters</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default 2</span><dd>The number of clusters to find.</dd>
<dt>connectivity</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like or callable, optional</span><dd>Connectivity matrix. Defines for each feature the neighboring
features following a given structure of the data.
This can be a connectivity matrix itself or a callable that transforms
the data into a connectivity matrix, such as derived from
kneighbors_graph. Default is None, i.e, the
hierarchical clustering algorithm is unstructured.</dd>
<dt>affinity</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default &#8220;euclidean&#8221;</span><dd>Metric used to compute the linkage. Can be &#8220;euclidean&#8221;, &#8220;l1&#8221;, &#8220;l2&#8221;,
&#8220;manhattan&#8221;, &#8220;cosine&#8221;, or &#8216;precomputed&#8217;.
If linkage is &#8220;ward&#8221;, only &#8220;euclidean&#8221; is accepted.</dd>
<dt>memory</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string, optional</span><dd>Used to cache the output of the computation of the tree.
By default, no caching is done. If a string is given, it is the
path to the caching directory.</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (optional)</span><dd>Number of connected components. If None the number of connected
components is estimated from the connectivity matrix.
NOTE: This parameter is now directly determined from the connectivity
matrix and will be removed in 0.18</dd>
<dt>compute_full_tree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or &#8216;auto&#8217;, optional, default &#8220;auto&#8221;</span><dd>Stop early the construction of the tree at n_clusters. This is
useful to decrease computation time if the number of clusters is
not small compared to the number of features. This option is
useful only when specifying a connectivity matrix. Note also that
when varying the number of clusters and using caching, it may
be advantageous to compute the full tree.</dd>
<dt>linkage</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8220;ward&#8221;, &#8220;complete&#8221;, &#8220;average&#8221;}, optional, default &#8220;ward&#8221;</span><dd><p class="first">Which linkage criterion to use. The linkage criterion determines which
distance to use between sets of features. The algorithm will merge
the pairs of cluster that minimize this criterion.</p>
<ul class="last simple">
<li>ward minimizes the variance of the clusters being merged.</li>
<li>average uses the average of the distances of each feature of
the two sets.</li>
<li>complete or maximum linkage uses the maximum distances between
all features of the two sets.</li>
</ul>
</dd>
<dt>pooling_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable, default np.mean</span><dd>This combines the values of agglomerated features into a single
value, and should accept an array of shape [M, N] and the keyword
argument <cite>axis=1</cite>, and reduce it to an array of size [M].</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">labels_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, (n_features,)</span><dd>cluster labels for each feature.</dd>
<dt><code class="docutils literal"><span class="pre">n_leaves_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of leaves in the hierarchical tree.</dd>
<dt><code class="docutils literal"><span class="pre">n_components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The estimated number of connected components in the graph.</dd>
<dt><code class="docutils literal"><span class="pre">children_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_nodes-1, 2)</span><dd>The children of each non-leaf node. Values less than <cite>n_features</cite>
correspond to leaves of the tree which are the original samples.
A node <cite>i</cite> greater than or equal to <cite>n_features</cite> is a non-leaf
node and has children <cite>children_[i - n_features]</cite>. Alternatively
at the i-th iteration, children[i][0] and children[i][1]
are merged to form node <cite>n_features + i</cite></dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>FeatureAgglomerationTransformerSklearn</strong></li>
<li><strong>FeatureAgglomerationTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-featurehashertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.FeatureHasherTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.FeatureHasherTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FeatureHasherTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-featurehashertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.FeatureHasherTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">FeatureHasherTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.FeatureHasherTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Implements feature hashing, aka the hashing trick.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.hashing.FeatureHasher.html">sklearn.feature_extraction.hashing.FeatureHasher</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This class turns sequences of symbolic feature names (strings) into
scipy.sparse matrices, using a hash function to compute the matrix column
corresponding to a name. The hash function employed is the signed 32-bit
version of Murmurhash3.</p>
<p>Feature names of type byte string are used as-is. Unicode strings are
converted to UTF-8 first, but no Unicode normalization is done.
Feature values must be (finite) numbers.</p>
<p>This class is a low-memory alternative to DictVectorizer and
CountVectorizer, intended for large-scale (online) learning and situations
where memory is tight, e.g. when running prediction code on embedded
devices.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>dtype</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy type, optional, default np.float64</span><dd>The type of feature values. Passed to scipy.sparse matrix constructors
as the dtype argument. Do not set this to bool, np.boolean or any
unsigned integer type.</dd>
<dt>input_type</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional, default &#8220;dict&#8221;</span><dd>Either &#8220;dict&#8221; (the default) to accept dictionaries over
(feature_name, value); &#8220;pair&#8221; to accept pairs of (feature_name, value);
or &#8220;string&#8221; to accept single strings.
feature_name should be a string, while value should be a number.
In the case of &#8220;string&#8221;, a value of 1 is implied.
The feature_name is hashed to find the appropriate column for the
feature. The value&#8217;s sign might be flipped in the output (but see
non_negative, below).</dd>
<dt>non_negative</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values can be interpreted as frequencies.
When False, output values will have expected value zero.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="k">import</span> <span class="n">FeatureHasher</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">FeatureHasher</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;dog&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;elephant&#39;</span><span class="p">:</span><span class="mi">4</span><span class="p">},{</span><span class="s1">&#39;dog&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],</span>
<span class="go">       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])</span>
</pre></div>
</div>
<p>See also</p>
<p>DictVectorizer : vectorizes string-valued features using a hash table.
sklearn.preprocessing.OneHotEncoder : handles nominal/categorical features</p>
<blockquote>
<div>encoded as columns of integers.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>FeatureHasherTransformerSklearnNode</strong></li>
<li><strong>FeatureHasherTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-forestregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ForestRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ForestRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ForestRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-forestregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ForestRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ForestRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ForestRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Base class for forest of trees-based regressors.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.forest.ForestRegressor.html">sklearn.ensemble.forest.ForestRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ForestRegressorSklearn</strong></li>
<li><strong>ForestRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-functiontransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.FunctionTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.FunctionTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.FunctionTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-functiontransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.FunctionTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">FunctionTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.FunctionTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Constructs a transformer from an arbitrary callable.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing._function_transformer.FunctionTransformer.html">sklearn.preprocessing._function_transformer.FunctionTransformer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A FunctionTransformer forwards its X (and optionally y) arguments to a
user-defined function or function object and returns the result of this
function. This is useful for stateless transformations such as taking the
log of frequencies, doing custom scaling, etc.</p>
<p>A FunctionTransformer will not do any checks on its function&#8217;s output.</p>
<p>Note: If a lambda is used as the function, then the resulting
transformer will not be pickleable.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional default=None</span><dd>The callable to use for the transformation. This will be passed
the same arguments as transform, with args and kwargs forwarded.
If func is None, then func will be the identity function.</dd>
<dt>validate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional default=True</span><dd>Indicate that the input X array should be checked before calling
func. If validate is false, there will be no input validation.
If it is true, then X will be converted to a 2-dimensional NumPy
array or sparse matrix. If this conversion is not possible or X
contains NaN or infinity, an exception is raised.</dd>
<dt>accept_sparse</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>Indicate that func accepts a sparse matrix as input. If validate is
False, this has no effect. Otherwise, if accept_sparse is false,
sparse matrix inputs will cause an exception to be raised.</dd>
<dt>pass_y: bool, optional default=False</dt>
<dd>Indicate that transform should forward the y argument to the
inner callable.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>FunctionTransformerSklearn</strong></li>
<li><strong>FunctionTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gaussiannbclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GaussianNBClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GaussianNBClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianNBClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gaussiannbclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GaussianNBClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GaussianNBClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GaussianNBClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Gaussian Naive Bayes (GaussianNB)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">sklearn.naive_bayes.GaussianNB</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Can perform online updates to model parameters via <cite>partial_fit</cite> method.
For details on algorithm used to update feature means and variance online,
see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:</p>
<blockquote>
<div><a class="reference external" href="http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf">http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf</a></div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_prior_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span><dd>probability of each class.</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span><dd>number of training samples observed in each class.</dd>
<dt><code class="docutils literal"><span class="pre">theta_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span><dd>mean of each feature per class</dd>
<dt><code class="docutils literal"><span class="pre">sigma_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span><dd>variance of each feature per class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">GaussianNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf_pf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf_pf</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
<span class="go">GaussianNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf_pf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GaussianNBClassifierSklearn</strong></li>
<li><strong>GaussianNBClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gaussianprocessregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GaussianProcessRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GaussianProcessRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianProcessRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gaussianprocessregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GaussianProcessRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GaussianProcessRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GaussianProcessRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>The Gaussian Process model class.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.gaussian_process.GaussianProcess.html">sklearn.gaussian_process.gaussian_process.GaussianProcess</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>regr</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, optional</span><dd><p class="first">A regression function returning an array of outputs of the linear
regression functional basis. The number of observations n_samples
should be greater than the size p of this basis.
Default assumes a simple constant regression trend.
Available built-in regression models are:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;quadratic&#39;</span>
</pre></div>
</div>
</dd>
<dt>corr</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, optional</span><dd><p class="first">A stationary autocorrelation function returning the autocorrelation
between two points x and x&#8217;.
Default assumes a squared-exponential autocorrelation model.
Built-in correlation models are:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;absolute_exponential&#39;</span><span class="p">,</span> <span class="s1">&#39;squared_exponential&#39;</span><span class="p">,</span>
<span class="s1">&#39;generalized_exponential&#39;</span><span class="p">,</span> <span class="s1">&#39;cubic&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span>
</pre></div>
</div>
</dd>
<dt>beta0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double array_like, optional</span><dd>The regression weight vector to perform Ordinary Kriging (OK).
Default assumes Universal Kriging (UK) so that the vector beta of
regression weights is estimated using the maximum likelihood
principle.</dd>
<dt>storage_mode</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd>A string specifying whether the Cholesky decomposition of the
correlation matrix should be stored in the class (storage_mode =
&#8216;full&#8217;) or not (storage_mode = &#8216;light&#8217;).
Default assumes storage_mode = &#8216;full&#8217;, so that the
Cholesky decomposition of the correlation matrix is stored.
This might be a useful parameter when one is not interested in the
MSE and only plan to estimate the BLUP, for which the correlation
matrix is not required.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>A boolean specifying the verbose level.
Default is verbose = False.</dd>
<dt>theta0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double array_like, optional</span><dd>An array with shape (n_features, ) or (1, ).
The parameters in the autocorrelation model.
If thetaL and thetaU are also specified, theta0 is considered as
the starting point for the maximum likelihood estimation of the
best set of parameters.
Default assumes isotropic autocorrelation model with theta0 = 1e-1.</dd>
<dt>thetaL</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double array_like, optional</span><dd>An array with shape matching theta0&#8217;s.
Lower bound on the autocorrelation parameters for maximum
likelihood estimation.
Default is None, so that it skips maximum likelihood estimation and
it uses theta0.</dd>
<dt>thetaU</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double array_like, optional</span><dd>An array with shape matching theta0&#8217;s.
Upper bound on the autocorrelation parameters for maximum
likelihood estimation.
Default is None, so that it skips maximum likelihood estimation and
it uses theta0.</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>Input X and observations y are centered and reduced wrt
means and standard deviations estimated from the n_samples
observations provided.
Default is normalize = True so that data is normalized to ease
maximum likelihood estimation.</dd>
<dt>nugget</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double or ndarray, optional</span><dd>Introduce a nugget effect to allow smooth predictions from noisy
data.  If nugget is an ndarray, it must be the same length as the
number of data points used for the fit.
The nugget is added to the diagonal of the assumed training covariance;
in this way it acts as a Tikhonov regularization in the problem.  In
the special case of the squared exponential correlation function, the
nugget mathematically represents the variance of the input values.
Default assumes a nugget close to machine precision for the sake of
robustness (nugget = 10. * MACHINE_EPSILON).</dd>
<dt>optimizer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd><p class="first">A string specifying the optimization algorithm to be used.
Default uses &#8216;fmin_cobyla&#8217; algorithm from scipy.optimize.
Available optimizers are:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;fmin_cobyla&#39;</span><span class="p">,</span> <span class="s1">&#39;Welch&#39;</span>
</pre></div>
</div>
<p class="last">&#8216;Welch&#8217; optimizer is dued to Welch et al., see reference <a class="reference internal" href="#wbswm1992" id="id40">[WBSWM1992]</a>.
It consists in iterating over several one-dimensional optimizations
instead of running one single multi-dimensional optimization.</p>
</dd>
<dt>random_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The number of times the Maximum Likelihood Estimation should be
performed from a random starting point.
The first MLE always uses the specified starting point (theta0),
the next starting points are picked at random according to an
exponential distribution (log-uniform on [thetaL, thetaU]).
Default does not use random starting point (random_start = 1).</dd>
<dt>random_state: integer or numpy.RandomState, optional</dt>
<dd>The generator used to shuffle the sequence of coordinates of theta in
the Welch optimizer. If an integer is given, it fixes the seed.
Defaults to the global numpy random number generator.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">theta_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>Specified theta OR the best set of autocorrelation parameters (the         sought maximizer of the reduced likelihood function).</dd>
<dt><code class="docutils literal"><span class="pre">reduced_likelihood_function_value_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>The optimal reduced likelihood function value.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="k">import</span> <span class="n">GaussianProcess</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gp</span> <span class="o">=</span> <span class="n">GaussianProcess</span><span class="p">(</span><span class="n">theta0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">thetaL</span><span class="o">=.</span><span class="mi">001</span><span class="p">,</span> <span class="n">thetaU</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                                      
<span class="go">GaussianProcess(beta0=None...</span>
<span class="go">        ...</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The presentation implementation is based on a translation of the DACE
Matlab toolbox, see reference <a class="reference internal" href="#nlns2002" id="id41">[NLNS2002]</a>.</p>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="nlns2002" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id41">[NLNS2002]</a></td><td><cite>H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.
Sondergaard.  DACE - A MATLAB Kriging Toolbox.</cite> (2002)
<a class="reference external" href="http://www2.imm.dtu.dk/~hbn/dace/dace.pdf">http://www2.imm.dtu.dk/~hbn/dace/dace.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="wbswm1992" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id40">[WBSWM1992]</a></td><td><cite>W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,
and M.D.  Morris (1992). Screening, predicting, and computer
experiments.  Technometrics, 34(1) 15&#8211;25.</cite>
<a class="reference external" href="http://www.jstor.org/pss/1269548">http://www.jstor.org/pss/1269548</a></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GaussianProcessRegressorSklearn</strong></li>
<li><strong>GaussianProcessRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gaussianrandomprojectionhashtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionHashTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionHashTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionHashTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gaussianrandomprojectionhashtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionHashTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GaussianRandomProjectionHashTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionHashTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.approximate.GaussianRandomProjectionHash.html">sklearn.neighbors.approximate.GaussianRandomProjectionHash</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GaussianRandomProjectionHashTransformerSklearn</strong></li>
<li><strong>GaussianRandomProjectionHashTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gaussianrandomprojectiontransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gaussianrandomprojectiontransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GaussianRandomProjectionTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GaussianRandomProjectionTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Reduce dimensionality through Gaussian random projection</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.random_projection.GaussianRandomProjection.html">sklearn.random_projection.GaussianRandomProjection</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The components of the random matrix are drawn from N(0, 1 / n_components).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or &#8216;auto&#8217;, optional (default = &#8216;auto&#8217;)</span><dd><p class="first">Dimensionality of the target projection space.</p>
<p>n_components can be automatically adjusted according to the
number of samples in the dataset and the bound given by the
Johnson-Lindenstrauss lemma. In that case the quality of the
embedding is controlled by the <code class="docutils literal"><span class="pre">eps</span></code> parameter.</p>
<p class="last">It should be noted that Johnson-Lindenstrauss lemma can yield
very conservative estimated of the required number of components
as it makes no assumption on the structure of the dataset.</p>
</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">strictly positive float, optional (default=0.1)</span><dd><p class="first">Parameter to control the quality of the embedding according to
the Johnson-Lindenstrauss lemma when n_components is set to
&#8216;auto&#8217;.</p>
<p class="last">Smaller values lead to better embedding and higher number of
dimensions (n_components) in the target projection space.</p>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, RandomState instance or None (default=None)</span><dd>Control the pseudo random number generator used to generate the
matrix at fit time.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_component_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Concrete number of components computed when n_components=&#8221;auto&#8221;.</dd>
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_components, n_features]</span><dd>Random matrix used for the projection.</dd>
</dl>
<p>See Also</p>
<p>SparseRandomProjection</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GaussianRandomProjectionTransformerSklearn</strong></li>
<li><strong>GaussianRandomProjectionTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-genericunivariateselecttransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GenericUnivariateSelectTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GenericUnivariateSelectTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GenericUnivariateSelectTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-genericunivariateselecttransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GenericUnivariateSelectTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GenericUnivariateSelectTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GenericUnivariateSelectTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Univariate feature selector with configurable strategy.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.html">sklearn.feature_selection.univariate_selection.GenericUnivariateSelect</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable</span><dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>mode</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;percentile&#8217;, &#8216;k_best&#8217;, &#8216;fpr&#8217;, &#8216;fdr&#8217;, &#8216;fwe&#8217;}</span><dd>Feature selection mode.</dd>
<dt>param</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float or int depending on the feature selection mode</span><dd>Parameter of the corresponding mode.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>p-values of feature scores.</dd>
</dl>
<p>See also</p>
<p>f_classif: ANOVA F-value between labe/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GenericUnivariateSelectTransformerSklearnNode</strong></li>
<li><strong>GenericUnivariateSelectTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gradientboostingclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GradientBoostingClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GradientBoostingClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GradientBoostingClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gradientboostingclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GradientBoostingClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GradientBoostingClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GradientBoostingClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Gradient Boosting for classification.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.html">sklearn.ensemble.gradient_boosting.GradientBoostingClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <code class="docutils literal"><span class="pre">n_classes_</span></code>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;deviance&#8217;, &#8216;exponential&#8217;}, optional (default=&#8217;deviance&#8217;)</span><dd>loss function to be optimized. &#8216;deviance&#8217; refers to
deviance (= logistic regression) for classification
with probabilistic outputs. For loss &#8216;exponential&#8217; gradient
boosting recovers the AdaBoost algorithm.</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span><dd>learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</dd>
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span><dd>The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=3)</span><dd>maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.</dd>
<dt>subsample</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.</dd>
<dt>init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator, None, optional (default=None)</span><dd>An estimator object that is used to compute the initial
predictions. <code class="docutils literal"><span class="pre">init</span></code> has to provide <code class="docutils literal"><span class="pre">fit</span></code> and <code class="docutils literal"><span class="pre">predict</span></code>.
If None it uses <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span><dd>Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>presort</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or &#8216;auto&#8217;, optional (default=&#8217;auto&#8217;)</span><dd><p class="first">Whether to presort the data to speed up the finding of best splits in
fitting. Auto mode by default will use presorting on dense data and
default to normal sorting on sparse data. Setting presort to true on
sparse data will raise an error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>presort</em> parameter.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span><dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">oob_improvement_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span><dd>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal"><span class="pre">init</span></code> estimator.</dd>
<dt><code class="docutils literal"><span class="pre">train_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span><dd>The i-th score <code class="docutils literal"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</dd>
<dt><code class="docutils literal"><span class="pre">loss_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">LossFunction</span><dd>The concrete <code class="docutils literal"><span class="pre">LossFunction</span></code> object.</dd>
<dt>init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator</span><dd>The estimator that provides the initial predictions.
Set via the <code class="docutils literal"><span class="pre">init</span></code> argument or <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray of DecisionTreeRegressor, shape = [n_estimators, <code class="docutils literal"><span class="pre">loss_.K</span></code>]</span><dd>The collection of fitted sub-estimators. <code class="docutils literal"><span class="pre">loss_.K</span></code> is 1 for binary
classification, otherwise n_classes.</dd>
</dl>
<p>See also</p>
<p>sklearn.tree.DecisionTreeClassifier, RandomForestClassifier
AdaBoostClassifier</p>
<p><strong>References</strong></p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li>Friedman, Stochastic Gradient Boosting, 1999</li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GradientBoostingClassifierSklearn</strong></li>
<li><strong>GradientBoostingClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gradientboostingregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GradientBoostingRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GradientBoostingRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GradientBoostingRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gradientboostingregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GradientBoostingRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GradientBoostingRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GradientBoostingRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Gradient Boosting for regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.html">sklearn.ensemble.gradient_boosting.GradientBoostingRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>GB builds an additive model in a forward stage-wise fashion;
it allows for the optimization of arbitrary differentiable loss functions.
In each stage a regression tree is fit on the negative gradient of the
given loss function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ls&#8217;, &#8216;lad&#8217;, &#8216;huber&#8217;, &#8216;quantile&#8217;}, optional (default=&#8217;ls&#8217;)</span><dd>loss function to be optimized. &#8216;ls&#8217; refers to least squares
regression. &#8216;lad&#8217; (least absolute deviation) is a highly robust
loss function solely based on order information of the input
variables. &#8216;huber&#8217; is a combination of the two. &#8216;quantile&#8217;
allows quantile regression (use <cite>alpha</cite> to specify the quantile).</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span><dd>learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</dd>
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span><dd>The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=3)</span><dd>maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples required to be at a leaf node.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.</dd>
<dt>subsample</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
</ul>
</li>
<li><ul class="first">
<li>If float, then <cite>max_features</cite> is a percentage and</li>
</ul>
</li>
<li><cite>int(max_features * n_features)</cite> features are considered at each</li>
<li>split.</li>
<li><ul class="first">
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
</ul>
</li>
<li><ul class="first">
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float (default=0.9)</span><dd>The alpha-quantile of the huber loss function and the quantile
loss function. Only if <code class="docutils literal"><span class="pre">loss='huber'</span></code> or <code class="docutils literal"><span class="pre">loss='quantile'</span></code>.</dd>
<dt>init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator, None, optional (default=None)</span><dd>An estimator object that is used to compute the initial
predictions. <code class="docutils literal"><span class="pre">init</span></code> has to provide <code class="docutils literal"><span class="pre">fit</span></code> and <code class="docutils literal"><span class="pre">predict</span></code>.
If None it uses <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span><dd>Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>presort</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or &#8216;auto&#8217;, optional (default=&#8217;auto&#8217;)</span><dd><p class="first">Whether to presort the data to speed up the finding of best splits in
fitting. Auto mode by default will use presorting on dense data and
default to normal sorting on sparse data. Setting presort to true on
sparse data will raise an error.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>optional parameter <em>presort</em>.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span><dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">oob_improvement_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span><dd>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal"><span class="pre">init</span></code> estimator.</dd>
<dt><code class="docutils literal"><span class="pre">train_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_estimators]</span><dd>The i-th score <code class="docutils literal"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</dd>
<dt><code class="docutils literal"><span class="pre">loss_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">LossFunction</span><dd>The concrete <code class="docutils literal"><span class="pre">LossFunction</span></code> object.</dd>
<dt><cite>init</cite></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">BaseEstimator</span><dd>The estimator that provides the initial predictions.
Set via the <code class="docutils literal"><span class="pre">init</span></code> argument or <code class="docutils literal"><span class="pre">loss.init_estimator</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]</span><dd>The collection of fitted sub-estimators.</dd>
</dl>
<p>See also</p>
<p>DecisionTreeRegressor, RandomForestRegressor</p>
<p><strong>References</strong></p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li>Friedman, Stochastic Gradient Boosting, 1999</li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GradientBoostingRegressorSklearn</strong></li>
<li><strong>GradientBoostingRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-gridsearchcvtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.GridSearchCVTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.GridSearchCVTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.GridSearchCVTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-gridsearchcvtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.GridSearchCVTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">GridSearchCVTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.GridSearchCVTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Exhaustive search over specified parameter values for an estimator.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html">sklearn.grid_search.GridSearchCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Important members are fit, predict.</p>
<p>GridSearchCV implements a &#8220;fit&#8221; and a &#8220;score&#8221; method.
It also implements &#8220;predict&#8221;, &#8220;predict_proba&#8221;, &#8220;decision_function&#8221;,
&#8220;transform&#8221; and &#8220;inverse_transform&#8221; if they are implemented in the
estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated grid-search over a parameter grid.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator object.</span><dd>A object of that type is instantiated for each grid point.
This is assumed to implement the scikit-learn estimator interface.
Either estimator needs to provide a <code class="docutils literal"><span class="pre">score</span></code> function,
or <code class="docutils literal"><span class="pre">scoring</span></code> must be passed.</dd>
<dt>param_grid</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict or list of dictionaries</span><dd>Dictionary with parameters names (string) as keys and lists of
parameter settings to try as values, or a list of such
dictionaries, in which case the grids spanned by each dictionary
in the list are explored. This enables searching over any sequence
of parameter settings.</dd>
<dt>scoring</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, default=None</span><dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.
If <code class="docutils literal"><span class="pre">None</span></code>, the <code class="docutils literal"><span class="pre">score</span></code> method of the estimator is used.</dd>
<dt>fit_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span><dd>Parameters to pass to the fit method.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default=1</span><dd><p class="first">Number of jobs to run in parallel.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Upgraded to joblib 0.9.3.</p>
</div>
</dd>
<dt>pre_dispatch</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span><dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>iid</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>If True, the data is assumed to be identically distributed across
the folds, and the loss minimized is the total loss per sample,
and not the mean loss across the folds.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> used. If the estimator is a classifier
or if <code class="docutils literal"><span class="pre">y</span></code> is neither binary nor multiclass, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>refit</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Refit the best estimator with the entire dataset.
If &#8220;False&#8221;, it is impossible to make predictions using
this GridSearchCV instance after fitting.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>Controls the verbosity: the higher, the more messages.</dd>
<dt>error_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;raise&#8217; (default) or numeric</span><dd>Value to assign to the score if an error occurs in estimator fitting.
If set to &#8216;raise&#8217;, the error is raised. If a numeric value is given,
FitFailedWarning is raised. This parameter does not affect the refit
step, which will always raise the error.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">svm</span><span class="p">,</span> <span class="n">grid_search</span><span class="p">,</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;kernel&#39;</span><span class="p">:(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">),</span> <span class="s1">&#39;C&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svr</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">svr</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="go">GridSearchCV(cv=None, error_score=...,</span>
<span class="go">       estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,</span>
<span class="go">                     decision_function_shape=None, degree=..., gamma=...,</span>
<span class="go">                     kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="go">                     random_state=None, shrinking=True, tol=...,</span>
<span class="go">                     verbose=False),</span>
<span class="go">       fit_params={}, iid=..., n_jobs=1,</span>
<span class="go">       param_grid=..., pre_dispatch=..., refit=...,</span>
<span class="go">       scoring=..., verbose=...)</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">grid_scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of named tuples</span><dd><p class="first">Contains scores for all parameter combinations in param_grid.
Each entry corresponds to one parameter setting.
Each named tuple has the attributes:</p>
<blockquote class="last">
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">parameters</span></code>, a dict of parameter settings</li>
<li><code class="docutils literal"><span class="pre">mean_validation_score</span></code>, the mean score over the
cross-validation folds</li>
<li><code class="docutils literal"><span class="pre">cv_validation_scores</span></code>, the list of scores for each fold</li>
</ul>
</div></blockquote>
</dd>
<dt><code class="docutils literal"><span class="pre">best_estimator_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span><dd>Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data. Not available if refit=False.</dd>
<dt><code class="docutils literal"><span class="pre">best_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of best_estimator on the left out data.</dd>
<dt><code class="docutils literal"><span class="pre">best_params_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>Parameter setting that gave the best results on the hold out data.</dd>
<dt><code class="docutils literal"><span class="pre">scorer_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">function</span><dd>Scorer function used on the held out data to choose the best
parameters for the model.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The parameters selected are those that maximize the score of the left out
data, unless an explicit score is passed in which case it is used instead.</p>
<p>If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
point in the grid (and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<p>See Also</p>
<p><code class="xref py py-class docutils literal"><span class="pre">ParameterGrid</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>generates all the combinations of a an hyperparameter grid.</li>
</ul>
</div></blockquote>
<p><code class="xref py py-func docutils literal"><span class="pre">sklearn.cross_validation.train_test_split()</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>utility function to split the data into a development set usable</li>
<li>for fitting a GridSearchCV instance and an evaluation set for</li>
<li>its final evaluation.</li>
</ul>
</div></blockquote>
<p><code class="xref py py-func docutils literal"><span class="pre">sklearn.metrics.make_scorer()</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>Make a scorer from a performance metric or loss function.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>GridSearchCVTransformerSklearn</strong></li>
<li><strong>GridSearchCVTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-hashingvectorizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.HashingVectorizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.HashingVectorizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.HashingVectorizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-hashingvectorizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.HashingVectorizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">HashingVectorizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.HashingVectorizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Convert a collection of text documents to a matrix of token occurrences</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html">sklearn.feature_extraction.text.HashingVectorizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>It turns a collection of text documents into a scipy.sparse matrix holding
token occurrence counts (or binary occurrence information), possibly
normalized as token frequencies if norm=&#8217;l1&#8217; or projected on the euclidean
unit sphere if norm=&#8217;l2&#8217;.</p>
<p>This text vectorizer implementation uses the hashing trick to find the
token string name to feature integer index mapping.</p>
<p>This strategy has several advantages:</p>
<ul class="simple">
<li>it is very low memory scalable to large datasets as there is no need to
store a vocabulary dictionary in memory</li>
<li>it is fast to pickle and un-pickle as it holds no state besides the
constructor parameters</li>
<li>it can be used in a streaming (partial fit) or parallel pipeline as there
is no state computed during fit.</li>
</ul>
<p>There are also a couple of cons (vs using a CountVectorizer with an
in-memory vocabulary):</p>
<ul class="simple">
<li>there is no way to compute the inverse transform (from feature indices to
string feature names) which can be a problem when trying to introspect
which features are most important to a model.</li>
<li>there can be collisions: distinct tokens can be mapped to the same
feature index. However in practice this is rarely an issue if n_features
is large enough (e.g. 2 ** 18 for text classification problems).</li>
<li>no IDF weighting as this would render the transformer stateful.</li>
</ul>
<p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span><dd><p class="first">If &#8216;filename&#8217;, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have a &#8216;read&#8217; method (file-like
object) that is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, default=&#8217;utf-8&#8217;</span><dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span><dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span><dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;, &#8216;char_wb&#8217;} or callable</span><dd><p class="first">Whether the feature should be made of word or character n-grams.
Option &#8216;char_wb&#8217; creates character n-grams only from text inside
word boundaries.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span><dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span><dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</dd>
<dt>ngram_range</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n), default=(1, 1)</span><dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span><dd><p class="first">If &#8216;english&#8217;, a built-in stop word list for English is used.</p>
<p class="last">If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
</dd>
<dt>lowercase</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>token_pattern</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string</span><dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>. The default regexp selects tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>n_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, default=(2 ** 20)</span><dd>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</dd>
<dt>norm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span><dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>binary: boolean, default=False.</dt>
<dd>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</dd>
<dt>dtype: type, optional</dt>
<dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>non_negative</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span><dd>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values can be interpreted as frequencies.
When False, output values will have expected value zero.</dd>
</dl>
<p>See also</p>
<p>CountVectorizer, TfidfVectorizer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>HashingVectorizerTransformerSklearn</strong></li>
<li><strong>HashingVectorizerTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-imputertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ImputerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ImputerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ImputerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-imputertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ImputerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ImputerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ImputerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Imputation transformer for completing missing values.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.imputation.Imputer.html">sklearn.preprocessing.imputation.Imputer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>missing_values</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or &#8220;NaN&#8221;, optional (default=&#8221;NaN&#8221;)</span><dd>The placeholder for the missing values. All occurrences of
<cite>missing_values</cite> will be imputed. For missing values encoded as np.nan,
use the string value &#8220;NaN&#8221;.</dd>
<dt>strategy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mean&#8221;)</span><dd><p class="first">The imputation strategy.</p>
<ul class="last simple">
<li>If &#8220;mean&#8221;, then replace missing values using the mean along
the axis.</li>
<li>If &#8220;median&#8221;, then replace missing values using the median along
the axis.</li>
<li>If &#8220;most_frequent&#8221;, then replace missing using the most frequent
value along the axis.</li>
</ul>
</dd>
<dt>axis</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=0)</span><dd><p class="first">The axis along which to impute.</p>
<ul class="last simple">
<li>If <cite>axis=0</cite>, then impute along columns.</li>
<li>If <cite>axis=1</cite>, then impute along rows.</li>
</ul>
</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=0)</span><dd>Controls the verbosity of the imputer.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd><p class="first">If True, a copy of X will be created. If False, imputation will
be done in-place whenever possible. Note that, in the following cases,
a new copy will always be made, even if <cite>copy=False</cite>:</p>
<ul class="last simple">
<li>If X is not an array of floating values;</li>
<li>If X is sparse and <cite>missing_values=0</cite>;</li>
<li>If <cite>axis=0</cite> and X is encoded as a CSR matrix;</li>
<li>If <cite>axis=1</cite> and X is encoded as a CSC matrix.</li>
</ul>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">statistics_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span><dd>The imputation fill value for each feature if axis == 0.</dd>
</dl>
<p><strong>Notes</strong></p>
<ul class="simple">
<li>When <code class="docutils literal"><span class="pre">axis=0</span></code>, columns which only contained missing values at <cite>fit</cite>
are discarded upon <cite>transform</cite>.</li>
<li>When <code class="docutils literal"><span class="pre">axis=1</span></code>, an exception is raised if there are rows for which it is
not possible to fill in the missing values (e.g., because they only
contain missing values).</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ImputerTransformerSklearnNode</strong></li>
<li><strong>ImputerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-incrementalpcatransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.IncrementalPCATransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.IncrementalPCATransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.IncrementalPCATransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-incrementalpcatransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.IncrementalPCATransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">IncrementalPCATransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.IncrementalPCATransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Incremental principal components analysis (IPCA).</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.incremental_pca.IncrementalPCA.html">sklearn.decomposition.incremental_pca.IncrementalPCA</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of
centered data, keeping only the most significant singular vectors to
project the data to a lower dimensional space.</p>
<p>Depending on the size of the input data, this algorithm can be much more
memory efficient than a PCA.</p>
<p>This algorithm has constant memory complexity, on the order
of <code class="docutils literal"><span class="pre">batch_size</span></code>, enabling use of np.memmap files without loading the
entire file into memory.</p>
<p>The computational overhead of each SVD is
<code class="docutils literal"><span class="pre">O(batch_size</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2)</span></code>, but only 2 * batch_size samples
remain in memory at a time. There will be <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">batch_size</span></code> SVD
computations to get the principal components, versus 1 large SVD of
complexity <code class="docutils literal"><span class="pre">O(n_samples</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">**</span> <span class="pre">2)</span></code> for PCA.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, (default=None)</span><dd>Number of components to keep. If <code class="docutils literal"><span class="pre">n_components</span> <span class="pre">``</span> <span class="pre">is</span> <span class="pre">``None</span></code>,
then <code class="docutils literal"><span class="pre">n_components</span></code> is set to <code class="docutils literal"><span class="pre">min(n_samples,</span> <span class="pre">n_features)</span></code>.</dd>
<dt>batch_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, (default=None)</span><dd>The number of samples to use for each batch. Only used when calling
<code class="docutils literal"><span class="pre">fit</span></code>. If <code class="docutils literal"><span class="pre">batch_size</span></code> is <code class="docutils literal"><span class="pre">None</span></code>, then <code class="docutils literal"><span class="pre">batch_size</span></code>
is inferred from the data and set to <code class="docutils literal"><span class="pre">5</span> <span class="pre">*</span> <span class="pre">n_features</span></code>, to provide a
balance between approximation accuracy and memory consumption.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span><dd>If False, X will be overwritten. <code class="docutils literal"><span class="pre">copy=False</span></code> can be used to
save memory but is unsafe for general use.</dd>
<dt>whiten</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd><p class="first">When True (False by default) the <code class="docutils literal"><span class="pre">components_</span></code> vectors are divided
by <code class="docutils literal"><span class="pre">n_samples</span></code> times <code class="docutils literal"><span class="pre">components_</span></code> to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometimes
improve the predictive accuracy of the downstream estimators by
making data respect some hard-wired assumptions.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_features)</span><dd>Components with maximum variance.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span><dd>Variance explained by each of the selected components.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span><dd>Percentage of variance explained by each of the selected components.
If all components are stored, the sum of explained variances is equal
to 1.0</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>Per-feature empirical mean, aggregate over calls to <code class="docutils literal"><span class="pre">partial_fit</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">var_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>Per-feature empirical variance, aggregate over calls to
<code class="docutils literal"><span class="pre">partial_fit</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">noise_variance_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See &#8220;Pattern Recognition and
Machine Learning&#8221; by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>.</dd>
<dt><code class="docutils literal"><span class="pre">n_components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The estimated number of components. Relevant when
<code class="docutils literal"><span class="pre">n_components=None</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_samples_seen_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal"><span class="pre">partial_fit</span></code> calls.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Implements the incremental PCA model from:</p>
<p><cite>D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual
Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,
pp. 125-141, May 2008.</cite>
See <a class="reference external" href="http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf">http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf</a></p>
<p>This model is an extension of the Sequential Karhunen-Loeve Transform from:</p>
<p><cite>A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and
its Application to Images, IEEE Transactions on Image Processing, Volume 9,
Number 8, pp. 1371-1374, August 2000.</cite>
See <a class="reference external" href="http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf">http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf</a></p>
<p>We have specifically abstained from an optimization used by authors of both
papers, a QR decomposition used in specific situations to reduce the
algorithmic complexity of the SVD. The source for this technique is
<cite>Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,
section 5.4.4, pp 252-253.</cite>. This technique has been omitted because it is
advantageous only when decomposing a matrix with <code class="docutils literal"><span class="pre">n_samples</span></code> (rows)
&gt;= 5/3 * <code class="docutils literal"><span class="pre">n_features</span></code> (columns), and hurts the readability of the
implemented algorithm. This would be a good opportunity for future
optimization, if it is deemed necessary.</p>
<p><strong>References</strong></p>
<ol class="upperalpha simple" start="4">
<li><dl class="first docutils">
<dt>Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual</dt>
<dd>Tracking, International Journal of Computer Vision, Volume 77,
Issue 1-3, pp. 125-141, May 2008.</dd>
</dl>
</li>
</ol>
<ol class="upperalpha simple" start="7">
<li><dl class="first docutils">
<dt>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</dt>
<dd>Section 5.4.4, pp. 252-253.</dd>
</dl>
</li>
</ol>
<p>See also</p>
<p>PCA
RandomizedPCA
KernelPCA
SparsePCA
TruncatedSVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>IncrementalPCATransformerSklearnNode</strong></li>
<li><strong>IncrementalPCATransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-isomaptransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.IsomapTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.IsomapTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.IsomapTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-isomaptransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.IsomapTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">IsomapTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.IsomapTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Isomap Embedding</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.isomap.Isomap.html">sklearn.manifold.isomap.Isomap</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Non-linear dimensionality reduction through Isometric Mapping</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>number of neighbors to consider for each point.</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>number of coordinates for the manifold</dd>
<dt>eigen_solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">[&#8216;auto&#8217;|&#8217;arpack&#8217;|&#8217;dense&#8217;]</span><dd><p class="first">&#8216;auto&#8217; : Attempt to choose the most efficient solver
for the given problem.</p>
<p>&#8216;arpack&#8217; : Use Arnoldi decomposition to find the eigenvalues
and eigenvectors.</p>
<p class="last">&#8216;dense&#8217; : Use a direct solver (i.e. LAPACK)
for the eigenvalue decomposition.</p>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Convergence tolerance passed to arpack or lobpcg.
not used if eigen_solver == &#8216;dense&#8217;.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>Maximum number of iterations for the arpack solver.
not used if eigen_solver == &#8216;dense&#8217;.</dd>
<dt>path_method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;FW&#8217;|&#8217;D&#8217;]</span><dd><p class="first">Method to use in finding shortest path.</p>
<p>&#8216;auto&#8217; : attempt to choose the best algorithm automatically.</p>
<p>&#8216;FW&#8217; : Floyd-Warshall algorithm.</p>
<p class="last">&#8216;D&#8217; : Dijkstra&#8217;s algorithm.</p>
</dd>
<dt>neighbors_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;brute&#8217;|&#8217;kd_tree&#8217;|&#8217;ball_tree&#8217;]</span><dd>Algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">embedding_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_components)</span><dd>Stores the embedding vectors.</dd>
<dt><code class="docutils literal"><span class="pre">kernel_pca_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd><cite>KernelPCA</cite> object used to implement the embedding.</dd>
<dt><code class="docutils literal"><span class="pre">training_data_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_features)</span><dd>Stores the training data.</dd>
<dt><code class="docutils literal"><span class="pre">nbrs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">sklearn.neighbors.NearestNeighbors instance</span><dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
<dt><code class="docutils literal"><span class="pre">dist_matrix_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_samples, n_samples)</span><dd>Stores the geodesic distance matrix of training data.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id42" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C. A global geometric
framework for nonlinear dimensionality reduction. Science 290 (5500)</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>IsomapTransformerSklearn</strong></li>
<li><strong>IsomapTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-isotonicregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.IsotonicRegressionSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.IsotonicRegressionSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.IsotonicRegressionSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-isotonicregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.IsotonicRegressionSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">IsotonicRegressionSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.IsotonicRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Isotonic regression model.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html">sklearn.isotonic.IsotonicRegression</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The isotonic regression optimization problem is defined by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="nb">min</span> <span class="nb">sum</span> <span class="n">w_i</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">subject</span> <span class="n">to</span> <span class="n">y_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">y_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="n">whenever</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="ow">and</span> <span class="nb">min</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span> <span class="o">=</span> <span class="n">y_min</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span> <span class="o">=</span> <span class="n">y_max</span>
</pre></div>
</div>
<p>where:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">y[i]</span></code> are inputs (real numbers)</li>
</ul>
</li>
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">y_[i]</span></code> are fitted</li>
</ul>
</li>
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">X</span></code> specifies the order.</li>
</ul>
</li>
<li>If <code class="docutils literal"><span class="pre">X</span></code> is non-decreasing then <code class="docutils literal"><span class="pre">y_</span></code> is non-decreasing.</li>
<li><ul class="first">
<li><code class="docutils literal"><span class="pre">w[i]</span></code> are optional strictly positive weights (default to 1.0)</li>
</ul>
</li>
</ul>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>y_min</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span><dd>If not None, set the lowest value of the fit to y_min.</dd>
<dt>y_max</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">optional, default: None</span><dd>If not None, set the highest value of the fit to y_max.</dd>
<dt>increasing</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or string, optional, default: True</span><dd><p class="first">If boolean, whether or not to fit the isotonic regression with y
increasing or decreasing.</p>
<p class="last">The string value &#8220;auto&#8221; determines whether y should
increase or decrease based on the Spearman correlation estimate&#8217;s
sign.</p>
</dd>
<dt>out_of_bounds</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional, default: &#8220;nan&#8221;</span><dd>The <code class="docutils literal"><span class="pre">out_of_bounds</span></code> parameter handles how x-values outside of the
training domain are handled.  When set to &#8220;nan&#8221;, predicted y-values
will be NaN.  When set to &#8220;clip&#8221;, predicted y-values will be
set to the value corresponding to the nearest train interval endpoint.
When set to &#8220;raise&#8221;, allow <code class="docutils literal"><span class="pre">interp1d</span></code> to throw ValueError.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (n_samples, )</span><dd>A copy of the input X.</dd>
<dt><code class="docutils literal"><span class="pre">y_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (n_samples, )</span><dd>Isotonic fit of y.</dd>
<dt><code class="docutils literal"><span class="pre">X_min_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Minimum value of input array <cite>X_</cite> for left bound.</dd>
<dt><code class="docutils literal"><span class="pre">X_max_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Maximum value of input array <cite>X_</cite> for right bound.</dd>
<dt><code class="docutils literal"><span class="pre">f_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">function</span><dd>The stepwise interpolating function that covers the domain <cite>X_</cite>.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties are broken using the secondary method from Leeuw, 1977.</p>
<p><strong>References</strong></p>
<p>Isotonic Median Regression: A Linear Programming Approach
Nilotpal Chakravarti
Mathematics of Operations Research
Vol. 14, No. 2 (May, 1989), pp. 303-308</p>
<p>Isotone Optimization in R : Pool-Adjacent-Violators
Algorithm (PAVA) and Active Set Methods
Leeuw, Hornik, Mair
Journal of Statistical Software 2009</p>
<p>Correctness of Kruskal&#8217;s algorithms for monotone regression with ties
Leeuw, Psychometrica, 1977</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>IsotonicRegressionSklearn</strong></li>
<li><strong>IsotonicRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-kneighborsclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.KNeighborsClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.KNeighborsClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KNeighborsClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-kneighborsclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.KNeighborsClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">KNeighborsClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.KNeighborsClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Classifier implementing the k-nearest neighbors vote.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.classification.KNeighborsClassifier.html">sklearn.neighbors.classification.KNeighborsClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 5)</span><dd>Number of neighbors to use by default for <code class="xref py py-meth docutils literal"><span class="pre">k_neighbors()</span></code> queries.</dd>
<dt>weights</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span><dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span><dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>&#8216;kd_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">KDTree</span></code></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span><dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>metric</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or DistanceMetric object (default = &#8216;minkowski&#8217;)</span><dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>p</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span><dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span><dd>Additional keyword arguments for the metric function.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 1)</span><dd>The number of parallel jobs to run for neighbors search.
If <code class="docutils literal"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.
Doesn&#8217;t affect <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">]]))</span>
<span class="go">[0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="go">[[ 0.66666667  0.33333333]]</span>
</pre></div>
</div>
<p>See also</p>
<p>RadiusNeighborsClassifier
KNeighborsRegressor
RadiusNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances but
but different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>KNeighborsClassifierSklearnNode</strong></li>
<li><strong>KNeighborsClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-kneighborsregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.KNeighborsRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.KNeighborsRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KNeighborsRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-kneighborsregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.KNeighborsRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">KNeighborsRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.KNeighborsRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Regression based on k-nearest neighbors.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.regression.KNeighborsRegressor.html">sklearn.neighbors.regression.KNeighborsRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 5)</span><dd>Number of neighbors to use by default for <code class="xref py py-meth docutils literal"><span class="pre">k_neighbors()</span></code> queries.</dd>
<dt>weights</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span><dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span><dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>&#8216;kd_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">KDtree</span></code></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span><dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>metric</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or DistanceMetric object (default=&#8217;minkowski&#8217;)</span><dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>p</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span><dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span><dd>Additional keyword arguments for the metric function.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 1)</span><dd>The number of parallel jobs to run for neighbors search.
If <code class="docutils literal"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.
Doesn&#8217;t affect <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KNeighborsRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[ 0.5]</span>
</pre></div>
</div>
<p>See also</p>
<p>NearestNeighbors
RadiusNeighborsRegressor
KNeighborsClassifier
RadiusNeighborsClassifier</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances but
but different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>KNeighborsRegressorSklearnNode</strong></li>
<li><strong>KNeighborsRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-kernelcenterertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.KernelCentererTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.KernelCentererTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KernelCentererTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-kernelcenterertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.KernelCentererTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">KernelCentererTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.KernelCentererTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Center a kernel matrix</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.KernelCenterer.html">sklearn.preprocessing.data.KernelCenterer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a
function mapping x to a Hilbert space. KernelCenterer centers (i.e.,
normalize to have zero mean) the data without explicitly computing phi(x).
It is equivalent to centering phi(x) with
sklearn.preprocessing.StandardScaler(with_std=False).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>KernelCentererTransformerSklearnNode</strong></li>
<li><strong>KernelCentererTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-kernelpcatransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.KernelPCATransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.KernelPCATransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KernelPCATransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-kernelpcatransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.KernelPCATransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">KernelPCATransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.KernelPCATransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Kernel Principal component analysis (KPCA)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.kernel_pca.KernelPCA.html">sklearn.decomposition.kernel_pca.KernelPCA</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Non-linear dimensionality reduction through the use of kernels (see
<span class="xref std std-ref">metrics</span>).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components: int or None</dt>
<dd>Number of components. If None, all non-zero components are kept.</dd>
<dt>kernel: &#8220;linear&#8221; | &#8220;poly&#8221; | &#8220;rbf&#8221; | &#8220;sigmoid&#8221; | &#8220;cosine&#8221; | &#8220;precomputed&#8221;</dt>
<dd>Kernel.
Default: &#8220;linear&#8221;</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default=3</span><dd>Degree for poly kernels. Ignored by other kernels.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Kernel coefficient for rbf and poly kernels. Default: 1/n_features.
Ignored by other kernels.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Independent term in poly and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>kernel_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, optional</span><dd>Parameters (keyword arguments) and values for kernel passed as
callable object. Ignored by other kernels.</dd>
<dt>alpha: int</dt>
<dd>Hyperparameter of the ridge regression that learns the
inverse transform (when fit_inverse_transform=True).
Default: 1.0</dd>
<dt>fit_inverse_transform: bool</dt>
<dd>Learn the inverse transform for non-precomputed kernels.
(i.e. learn to find the pre-image of a point)
Default: False</dd>
<dt>eigen_solver: string [&#8216;auto&#8217;|&#8217;dense&#8217;|&#8217;arpack&#8217;]</dt>
<dd>Select eigensolver to use.  If n_components is much less than
the number of training samples, arpack may be more efficient
than the dense eigensolver.</dd>
<dt>tol: float</dt>
<dd>convergence tolerance for arpack.
Default: 0 (optimal value will be chosen by arpack)</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>maximum number of iterations for arpack
Default: None (optimal value will be chosen by arpack)</dd>
<dt>remove_zero_eig</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>If True, then all components with zero eigenvalues are removed, so
that the number of components in the output may be &lt; n_components
(and sometimes even zero due to numerical instability).
When n_components is None, this parameter is ignored and components
with zero eigenvalues are removed regardless.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><code class="docutils literal"><span class="pre">lambdas_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Eigenvalues of the centered kernel matrix</li>
</ul>
</div></blockquote>
<p><code class="docutils literal"><span class="pre">alphas_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Eigenvectors of the centered kernel matrix</li>
</ul>
</div></blockquote>
<p><code class="docutils literal"><span class="pre">dual_coef_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Inverse transform matrix</li>
</ul>
</div></blockquote>
<p><code class="docutils literal"><span class="pre">X_transformed_fit_</span></code> :</p>
<blockquote>
<div><ul class="simple">
<li>Projection of the fitted data on the kernel principal components</li>
</ul>
</div></blockquote>
<p><strong>References</strong></p>
<p>Kernel PCA was introduced in:</p>
<blockquote>
<div><ul class="simple">
<li>Bernhard Schoelkopf, Alexander J. Smola,</li>
<li>and Klaus-Robert Mueller. 1999. Kernel principal</li>
<li>component analysis. In Advances in kernel methods,</li>
<li>MIT Press, Cambridge, MA, USA 327-352.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>KernelPCATransformerSklearn</strong></li>
<li><strong>KernelPCATransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-kernelridgeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.KernelRidgeRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.KernelRidgeRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.KernelRidgeRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-kernelridgeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.KernelRidgeRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">KernelRidgeRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.KernelRidgeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Kernel ridge regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html">sklearn.kernel_ridge.KernelRidge</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Kernel ridge regression (KRR) combines ridge regression (linear least
squares with l2-norm regularization) with the kernel trick. It thus
learns a linear function in the space induced by the respective kernel and
the data. For non-linear kernels, this corresponds to a non-linear
function in the original space.</p>
<p>The form of the model learned by KRR is identical to support vector
regression (SVR). However, different loss functions are used: KRR uses
squared error loss while support vector regression uses epsilon-insensitive
loss, both combined with l2 regularization. In contrast to SVR, fitting a
KRR model can be done in closed-form and is typically faster for
medium-sized datasets. On the other  hand, the learned model is non-sparse
and thus slower than SVR, which learns a sparse model for epsilon &gt; 0, at
prediction-time.</p>
<p>This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape [n_samples, n_targets]).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{float, array-like}, shape = [n_targets]</span><dd>Small positive values of alpha improve the conditioning of the problem
and reduce the variance of the estimates.  Alpha corresponds to
<code class="docutils literal"><span class="pre">(2*C)^-1</span></code> in other linear models such as LogisticRegression or
LinearSVC. If an array is passed, penalties are assumed to be specific
to the targets. Hence they must correspond in number.</dd>
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default=&#8221;linear&#8221;</span><dd>Kernel mapping used internally. A callable should accept two arguments
and the keyword arguments passed to this object as kernel_params, and
should return a floating point number.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=None</span><dd>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
and sigmoid kernels. Interpretation of the default value is left to
the kernel; see the documentation for sklearn.metrics.pairwise.
Ignored by other kernels.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=3</span><dd>Degree of the polynomial kernel. Ignored by other kernels.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span><dd>Zero coefficient for polynomial and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>kernel_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, optional</span><dd>Additional parameters (keyword arguments) for kernel function passed
as callable object.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span><dd>Weight vector(s) in kernel space</dd>
<dt><code class="docutils literal"><span class="pre">X_fit_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{array-like, sparse matrix}, shape = [n_samples, n_features]</span><dd>Training data, which is also required for prediction</dd>
</dl>
<p><strong>References</strong></p>
<ul class="simple">
<li>Kevin P. Murphy
&#8220;Machine Learning: A Probabilistic Perspective&#8221;, The MIT Press
chapter 14.4.3, pp. 492-493</li>
</ul>
<p>See also</p>
<dl class="docutils">
<dt>Ridge</dt>
<dd>Linear ridge regression.</dd>
<dt>SVR</dt>
<dd>Support Vector Regression implemented using libsvm.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="k">import</span> <span class="n">KernelRidge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel=&#39;linear&#39;,</span>
<span class="go">            kernel_params=None)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>KernelRidgeRegressorSklearnNode</strong></li>
<li><strong>KernelRidgeRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-labelbinarizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LabelBinarizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LabelBinarizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelBinarizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-labelbinarizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LabelBinarizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LabelBinarizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LabelBinarizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Binarize labels in a one-vs-all fashion</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label.LabelBinarizer.html">sklearn.preprocessing.label.LabelBinarizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Several regression and binary classification algorithms are
available in the scikit. A simple way to extend these algorithms
to the multi-class classification case is to use the so-called
one-vs-all scheme.</p>
<p>At learning time, this simply consists in learning one regressor
or binary classifier per class. In doing so, one needs to convert
multi-class labels to binary labels (belong or does not belong
to the class). LabelBinarizer makes this process easy with the
transform method.</p>
<p>At prediction time, one assigns the class for which the corresponding
model gave the greatest confidence. LabelBinarizer makes this easy
with the inverse_transform method.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>neg_label</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (default: 0)</span><dd>Value with which negative labels must be encoded.</dd>
<dt>pos_label</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (default: 1)</span><dd>Value with which positive labels must be encoded.</dd>
<dt>sparse_output</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default: False)</span><dd>True if the returned array from transform is desired to be in sparse
CSR format.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_class]</span><dd>Holds the label for each class.</dd>
<dt><code class="docutils literal"><span class="pre">y_type_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str,</span><dd>Represents the type of the target data as evaluated by
utils.multiclass.type_of_target. Possible type are &#8216;continuous&#8217;,
&#8216;continuous-multioutput&#8217;, &#8216;binary&#8217;, &#8216;multiclass&#8217;,
&#8216;mutliclass-multioutput&#8217;, &#8216;multilabel-indicator&#8217;, and &#8216;unknown&#8217;.</dd>
<dt><code class="docutils literal"><span class="pre">multilabel_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>True if the transformer was fitted on a multilabel rather than a
multiclass set of labels. The <code class="docutils literal"><span class="pre">multilabel_</span></code> attribute is deprecated
and will be removed in 0.18</dd>
<dt><code class="docutils literal"><span class="pre">sparse_input_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean,</span><dd>True if the input data to transform is given as a sparse matrix, False
otherwise.</dd>
<dt><code class="docutils literal"><span class="pre">indicator_matrix_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str</span><dd>&#8216;sparse&#8217; when the input data to tansform is a multilable-indicator and
is sparse, None otherwise. The <code class="docutils literal"><span class="pre">indicator_matrix_</span></code> attribute is
deprecated as of version 0.16 and will be removed in 0.18</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 4, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">array([[1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1]])</span>
</pre></div>
</div>
<p>Binary targets transform to a column vector</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">])</span>
<span class="go">array([[1],</span>
<span class="go">       [0],</span>
<span class="go">       [0],</span>
<span class="go">       [1]])</span>
</pre></div>
</div>
<p>Passing a 2D matrix for multilabel classification</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([0, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">array([[1, 0, 0],</span>
<span class="go">       [0, 1, 0],</span>
<span class="go">       [0, 0, 1],</span>
<span class="go">       [0, 1, 0]])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>label_binarize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">function to perform the transform operation of</span><dd>LabelBinarizer with fixed classes.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LabelBinarizerTransformerSklearnNode</strong></li>
<li><strong>LabelBinarizerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-labelencodertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LabelEncoderTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LabelEncoderTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelEncoderTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-labelencodertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LabelEncoderTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LabelEncoderTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LabelEncoderTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Encode labels with value between 0 and n_classes-1.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label.LabelEncoder.html">sklearn.preprocessing.label.LabelEncoder</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_class,)</span><dd>Holds the label for each class.</dd>
</dl>
<p><strong>Examples</strong></p>
<p><cite>LabelEncoder</cite> can be used to normalize labels.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span> 
<span class="go">array([0, 0, 1, 2]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">array([1, 1, 2, 6])</span>
</pre></div>
</div>
<p>It can also be used to transform non-numerical labels (as long as they are
hashable and comparable) to numerical labels.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">([</span><span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="s2">&quot;paris&quot;</span><span class="p">,</span> <span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;amsterdam&quot;</span><span class="p">])</span>
<span class="go">LabelEncoder()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;amsterdam&#39;, &#39;paris&#39;, &#39;tokyo&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;tokyo&quot;</span><span class="p">,</span> <span class="s2">&quot;paris&quot;</span><span class="p">])</span> 
<span class="go">array([2, 2, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">[&#39;tokyo&#39;, &#39;tokyo&#39;, &#39;paris&#39;]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LabelEncoderTransformerSklearnNode</strong></li>
<li><strong>LabelEncoderTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-labelpropagationclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LabelPropagationClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LabelPropagationClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelPropagationClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-labelpropagationclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LabelPropagationClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LabelPropagationClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LabelPropagationClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Label Propagation classifier</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.label_propagation.LabelPropagation.html">sklearn.semi_supervised.label_propagation.LabelPropagation</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;knn&#8217;, &#8216;rbf&#8217;}</span><dd>String identifier for kernel function to use.
Only &#8216;rbf&#8217; and &#8216;knn&#8217; kernels are currently supported..</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Parameter for rbf kernel</dd>
<dt>n_neighbors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span><dd>Parameter for knn kernel</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Clamping factor</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Change maximum number of iterations allowed</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_features]</span><dd>Input array.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span><dd>The distinct labels used in classifying instances.</dd>
<dt><code class="docutils literal"><span class="pre">label_distributions_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_classes]</span><dd>Categorical distribution for each item.</dd>
<dt><code class="docutils literal"><span class="pre">transduction_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples]</span><dd>Label assigned to each item via the transduction.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="k">import</span> <span class="n">LabelPropagation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelPropagation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelPropagation(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data
with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon
University, 2002 <a class="reference external" href="http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf">http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf</a></p>
<p>See Also</p>
<p>LabelSpreading : Alternate label propagation strategy more robust to noise</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LabelPropagationClassifierSklearn</strong></li>
<li><strong>LabelPropagationClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-labelspreadingclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LabelSpreadingClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LabelSpreadingClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LabelSpreadingClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-labelspreadingclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LabelSpreadingClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LabelSpreadingClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LabelSpreadingClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>LabelSpreading model for semi-supervised learning</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.label_propagation.LabelSpreading.html">sklearn.semi_supervised.label_propagation.LabelSpreading</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This model is similar to the basic Label Propgation algorithm,
but uses affinity matrix based on the normalized graph Laplacian
and soft clamping across the labels.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;knn&#8217;, &#8216;rbf&#8217;}</span><dd>String identifier for kernel function to use.
Only &#8216;rbf&#8217; and &#8216;knn&#8217; kernels are currently supported.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>parameter for rbf kernel</dd>
<dt>n_neighbors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer &gt; 0</span><dd>parameter for knn kernel</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>clamping factor</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>maximum number of iterations allowed</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Convergence tolerance: threshold to consider the system at steady
state</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">X_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_features]</span><dd>Input array.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_classes]</span><dd>The distinct labels used in classifying instances.</dd>
<dt><code class="docutils literal"><span class="pre">label_distributions_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_classes]</span><dd>Categorical distribution for each item.</dd>
<dt><code class="docutils literal"><span class="pre">transduction_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples]</span><dd>Label assigned to each item via the transduction.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.semi_supervised</span> <span class="k">import</span> <span class="n">LabelSpreading</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span> <span class="o">=</span> <span class="n">LabelSpreading</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_unlabeled_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_integers</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span><span class="p">[</span><span class="n">random_unlabeled_points</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_prop_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">LabelSpreading(...)</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston,
Bernhard Schoelkopf. Learning with local and global consistency (2004)
<a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219</a></p>
<p>See Also</p>
<p>LabelPropagation : Unregularized graph based semi-supervised learning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LabelSpreadingClassifierSklearnNode</strong></li>
<li><strong>LabelSpreadingClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-larscvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LarsCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LarsCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LarsCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-larscvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LarsCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LarsCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LarsCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Cross-validated Least Angle Regression model</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.least_angle.LarsCV.html">sklearn.linear_model.least_angle.LarsCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span><dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter: integer, optional</dt>
<dd>Maximum number of iterations to perform.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>max_n_alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The maximum number of points on the path used to compute the
residuals in the cross-validation</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>parameter vector (w in the formulation formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>independent term in decision function</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas)</span><dd>the varying values of the coefficients along the path</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>the estimated regularization parameter alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas,)</span><dd>the different values of alpha along the path</dd>
<dt><code class="docutils literal"><span class="pre">cv_alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_cv_alphas,)</span><dd>all the values of alpha along the path for the different folds</dd>
<dt><code class="docutils literal"><span class="pre">cv_mse_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_folds, n_cv_alphas)</span><dd>the mean square error on left-out for each fold along the path
(alpha values given by <code class="docutils literal"><span class="pre">cv_alphas</span></code>)</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int</span><dd>the number of iterations run by Lars with the optimal alpha.</dd>
</dl>
<p>See also</p>
<p>lars_path, LassoLars, LassoLarsCV</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LarsCVRegressorSklearn</strong></li>
<li><strong>LarsCVRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-larsregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LarsRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LarsRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LarsRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-larsregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LarsRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LarsRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LarsRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Least Angle Regression model a.k.a. LAR</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.least_angle.Lars.html">sklearn.linear_model.least_angle.Lars</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_nonzero_coefs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Target number of non-zero coefficients. Use <code class="docutils literal"><span class="pre">np.inf</span></code> for no limit.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span><dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>fit_path</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>If True the full path is stored in the <code class="docutils literal"><span class="pre">coef_path_</span></code> attribute.
If you compute the solution for a large problem or many targets,
setting <code class="docutils literal"><span class="pre">fit_path</span></code> to <code class="docutils literal"><span class="pre">False</span></code> will lead to a speedup, especially
with a small alpha.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas + 1,) | list of n_targets such arrays</span><dd>Maximum of covariances (in absolute value) at each iteration.         <code class="docutils literal"><span class="pre">n_alphas</span></code> is either <code class="docutils literal"><span class="pre">n_nonzero_coefs</span></code> or <code class="docutils literal"><span class="pre">n_features</span></code>,         whichever is smaller.</dd>
<dt><code class="docutils literal"><span class="pre">active_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list, length = n_alphas | list of n_targets such lists</span><dd>Indices of active variables at the end of the path.</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas + 1)         | list of n_targets such arrays</span><dd>The varying values of the coefficients along the path. It is not
present if the <code class="docutils literal"><span class="pre">fit_path</span></code> parameter is <code class="docutils literal"><span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span><dd>Parameter vector (w in the formulation formula).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span><dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int</span><dd>The number of iterations taken by lars_path to find the
grid of alphas for each target.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lars</span><span class="p">(</span><span class="n">n_nonzero_coefs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1111</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1111</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">Lars(copy_X=True, eps=..., fit_intercept=True, fit_path=True,</span>
<span class="go">   n_nonzero_coefs=1, normalize=True, positive=False, precompute=&#39;auto&#39;,</span>
<span class="go">   verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 0. -1.11...]</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path, LarsCV
sklearn.decomposition.sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LarsRegressorSklearnNode</strong></li>
<li><strong>LarsRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-lassocvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LassoCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LassoCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-lassocvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LassoCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LassoCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LassoCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Lasso linear model with iterative fitting along a regularization path</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.LassoCV.html">sklearn.linear_model.coordinate_descent.LassoCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The best model is selected by cross-validation.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>n_alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of alphas along the regularization path</dd>
<dt>alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, optional</span><dd>List of alphas where to compute the models.
If <code class="docutils literal"><span class="pre">None</span></code> alphas are set automatically</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span><dd>Amount of verbosity.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs.</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>If positive, restrict regression coefficients to be positive</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span><dd>parameter vector (w in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas, n_folds)</span><dd>mean square error for the test set on each fold, varying alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,)</span><dd>The grid of alphas used for fitting</dd>
<dt><code class="docutils literal"><span class="pre">dual_gap_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape ()</span><dd>The dual gap at the end of the optimization for the optimal alpha
(<code class="docutils literal"><span class="pre">alpha_</span></code>).</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/lasso_path_with_crossvalidation.py
for an example.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>See also</p>
<p>lars_path
lasso_path
LassoLars
Lasso
LassoLarsCV</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LassoCVRegressorSklearnNode</strong></li>
<li><strong>LassoCVRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-lassolarscvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LassoLarsCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LassoLarsCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoLarsCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-lassolarscvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LassoLarsCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LassoLarsCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LassoLarsCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Cross-validated Lasso, using the LARS algorithm</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.least_angle.LassoLarsCV.html">sklearn.linear_model.least_angle.LassoLarsCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span><dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients do not converge
to the ordinary-least-squares solution for small values of alpha.
Only coeffiencts up to the smallest alpha value (<code class="docutils literal"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.
As a consequence using LassoLarsCV only makes sense for problems where
a sparse solution is expected and/or reached.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Maximum number of iterations to perform.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>max_n_alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The maximum number of points on the path used to compute the
residuals in the cross-validation</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>parameter vector (w in the formulation formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas)</span><dd>the varying values of the coefficients along the path</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>the estimated regularization parameter alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas,)</span><dd>the different values of alpha along the path</dd>
<dt><code class="docutils literal"><span class="pre">cv_alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_cv_alphas,)</span><dd>all the values of alpha along the path for the different folds</dd>
<dt><code class="docutils literal"><span class="pre">cv_mse_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_folds, n_cv_alphas)</span><dd>the mean square error on left-out for each fold along the path
(alpha values given by <code class="docutils literal"><span class="pre">cv_alphas</span></code>)</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int</span><dd>the number of iterations run by Lars with the optimal alpha.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The object solves the same problem as the LassoCV object. However,
unlike the LassoCV, it find the relevant alphas values by itself.
In general, because of this property, it will be more stable.
However, it is more fragile to heavily multicollinear datasets.</p>
<p>It is more efficient than the LassoCV if only a small number of
features are selected compared to the total number, for instance if
there are very few samples compared to the number of features.</p>
<p>See also</p>
<p>lars_path, LassoLars, LarsCV, LassoCV</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LassoLarsCVRegressorSklearnNode</strong></li>
<li><strong>LassoLarsCVRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-lassolarsicregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LassoLarsICRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LassoLarsICRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoLarsICRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-lassolarsicregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LassoLarsICRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LassoLarsICRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LassoLarsICRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Lasso model fit with Lars using BIC or AIC for model selection</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.least_angle.LassoLarsIC.html">sklearn.linear_model.least_angle.LassoLarsIC</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>AIC is the Akaike information criterion and BIC is the Bayes
Information criterion. Such criteria are useful to select the value
of the regularization parameter by making a trade-off between the
goodness of fit and the complexity of the model. A good model should
explain well the data while being simple.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;bic&#8217; | &#8216;aic&#8217;</span><dd>The type of criterion to use.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span><dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients do not converge
to the ordinary-least-squares solution for small values of alpha.
Only coeffiencts up to the smallest alpha value (<code class="docutils literal"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.
As a consequence using LassoLarsIC only makes sense for problems where
a sparse solution is expected and/or reached.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Maximum number of iterations to perform. Can be used for
early stopping.</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>parameter vector (w in the formulation formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>the alpha parameter chosen by the information criterion</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by lars_path to find the grid of
alphas.</dd>
<dt><code class="docutils literal"><span class="pre">criterion_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas,)</span><dd>The value of the information criteria (&#8216;aic&#8217;, &#8216;bic&#8217;) across all
alphas. The alpha which has the smallest information criteria
is chosen.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLarsIC</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1111</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1111</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">LassoLarsIC(copy_X=True, criterion=&#39;bic&#39;, eps=..., fit_intercept=True,</span>
<span class="go">      max_iter=500, normalize=True, positive=False, precompute=&#39;auto&#39;,</span>
<span class="go">      verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 0.  -1.11...]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>The estimation of the number of degrees of freedom is given by:</p>
<p>&#8220;On the degrees of freedom of the lasso&#8221;
Hui Zou, Trevor Hastie, and Robert Tibshirani
Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Akaike_information_criterion">http://en.wikipedia.org/wiki/Akaike_information_criterion</a>
<a class="reference external" href="http://en.wikipedia.org/wiki/Bayesian_information_criterion">http://en.wikipedia.org/wiki/Bayesian_information_criterion</a></p>
<p>See also</p>
<p>lars_path, LassoLars, LassoLarsCV</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LassoLarsICRegressorSklearn</strong></li>
<li><strong>LassoLarsICRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-lassolarsregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LassoLarsRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LassoLarsRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoLarsRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-lassolarsregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LassoLarsRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LassoLarsRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LassoLarsRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Lasso model fit with Least Angle Regression a.k.a. Lars</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.least_angle.LassoLars.html">sklearn.linear_model.least_angle.LassoLars</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>It is a Linear Model trained with an L1 prior as regularizer.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Constant that multiplies the penalty term. Defaults to 1.0.
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code>. For numerical reasons, using
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the LassoLars object is not advised and you
should prefer the LinearRegression object.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default=False)</span><dd>Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients will not converge
to the ordinary-least-squares solution for small values of alpha.
Only coeffiencts up to the smallest alpha value (<code class="docutils literal"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Maximum number of iterations to perform.</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>fit_path</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>If <code class="docutils literal"><span class="pre">True</span></code> the full path is stored in the <code class="docutils literal"><span class="pre">coef_path_</span></code> attribute.
If you compute the solution for a large problem or many targets,
setting <code class="docutils literal"><span class="pre">fit_path</span></code> to <code class="docutils literal"><span class="pre">False</span></code> will lead to a speedup, especially
with a small alpha.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas + 1,) | list of n_targets such arrays</span><dd>Maximum of covariances (in absolute value) at each iteration.         <code class="docutils literal"><span class="pre">n_alphas</span></code> is either <code class="docutils literal"><span class="pre">max_iter</span></code>, <code class="docutils literal"><span class="pre">n_features</span></code>, or the number of         nodes in the path with correlation greater than <code class="docutils literal"><span class="pre">alpha</span></code>, whichever         is smaller.</dd>
<dt><code class="docutils literal"><span class="pre">active_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list, length = n_alphas | list of n_targets such lists</span><dd>Indices of active variables at the end of the path.</dd>
<dt><code class="docutils literal"><span class="pre">coef_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, n_alphas + 1) or list</span><dd>If a list is passed it&#8217;s expected to be one of n_targets such arrays.
The varying values of the coefficients along the path. It is not
present if the <code class="docutils literal"><span class="pre">fit_path</span></code> parameter is <code class="docutils literal"><span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span><dd>Parameter vector (w in the formulation formula).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span><dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like or int.</span><dd>The number of iterations taken by lars_path to find the
grid of alphas for each target.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLars</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">LassoLars(alpha=0.01, copy_X=True, eps=..., fit_intercept=True,</span>
<span class="go">     fit_path=True, max_iter=500, normalize=True, positive=False,</span>
<span class="go">     precompute=&#39;auto&#39;, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 0.         -0.963257...]</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path
lasso_path
Lasso
LassoCV
LassoLarsCV
sklearn.decomposition.sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LassoLarsRegressorSklearn</strong></li>
<li><strong>LassoLarsRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-lassoregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LassoRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LassoRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LassoRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-lassoregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LassoRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LassoRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LassoRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear Model trained with L1 prior as regularizer (aka the Lasso)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.Lasso.html">sklearn.linear_model.coordinate_descent.Lasso</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Technically the Lasso model is optimizing the same objective function as
the Elastic Net with <code class="docutils literal"><span class="pre">l1_ratio=1.0</span></code> (no L2 penalty).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Constant that multiplies the L1 term. Defaults to 1.0.
<code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object. For numerical
reasons, using <code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is with the Lasso object is not advised
and you should prefer the LinearRegression object.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217; | array-like</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument. For sparse input
this option is always <code class="docutils literal"><span class="pre">True</span></code> to preserve sparsity.
WARNING : The <code class="docutils literal"><span class="pre">'auto'</span></code> option is deprecated and will
be removed in 0.18.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>positive</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span><dd>parameter vector (w in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">sparse_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)</span><dd><code class="docutils literal"><span class="pre">sparse_coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">coef_</span></code></dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int | array-like, shape (n_targets,)</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="go">   normalize=False, positive=False, precompute=False, random_state=None,</span>
<span class="go">   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[ 0.85  0.  ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">0.15</span>
</pre></div>
</div>
<p>See also</p>
<p>lars_path
lasso_path
LassoLars
LassoCV
LassoLarsCV
sklearn.decomposition.sparse_encode</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LassoRegressorSklearn</strong></li>
<li><strong>LassoRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-latentdirichletallocationtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LatentDirichletAllocationTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LatentDirichletAllocationTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LatentDirichletAllocationTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-latentdirichletallocationtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LatentDirichletAllocationTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LatentDirichletAllocationTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LatentDirichletAllocationTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Latent Dirichlet Allocation with online variational Bayes algorithm</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.online_lda.LatentDirichletAllocation.html">sklearn.decomposition.online_lda.LatentDirichletAllocation</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_topics</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=10)</span><dd>Number of topics.</dd>
<dt>doc_topic_prior</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=None)</span><dd>Prior of document topic distribution <cite>theta</cite>. If the value is None,
defaults to <cite>1 / n_topics</cite>.
In the literature, this is called <cite>alpha</cite>.</dd>
<dt>topic_word_prior</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=None)</span><dd>Prior of topic word distribution <cite>beta</cite>. If the value is None, defaults
to <cite>1 / n_topics</cite>.
In the literature, this is called <cite>eta</cite>.</dd>
<dt>learning_method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;batch&#8217; | &#8216;online&#8217;, default=&#8217;online&#8217;</span><dd><p class="first">Method used to update <cite>_component</cite>. Only used in <cite>fit</cite> method.
In general, if the data size is large, the online update will be much
faster than the batch update.
Valid options:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span>&#39;batch&#39;: Batch variational Bayes method. Use all training data in
    each EM update.
    Old `components_` will be overwritten in each iteration.
&#39;online&#39;: Online variational Bayes method. In each EM update, use
    mini-batch of training data to update the ``components_``
    variable incrementally. The learning rate is controlled by the
    ``learning_decay`` and the ``learning_offset`` parameters.
</pre></div>
</div>
</dd>
<dt>learning_decay</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.7)</span><dd>It is a parameter that control learning rate in the online learning
method. The value should be set between (0.5, 1.0] to guarantee
asymptotic convergence. When the value is 0.0 and batch_size is
<code class="docutils literal"><span class="pre">n_samples</span></code>, the update method is same as batch learning. In the
literature, this is called kappa.</dd>
<dt>learning_offset</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=10.)</span><dd>A (positive) parameter that downweights early iterations in online
learning.  It should be greater than 1.0. In the literature, this is
called tau_0.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span><dd>The maximum number of iterations.</dd>
<dt>total_samples</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1e6)</span><dd>Total number of documents. Only used in the <cite>partial_fit</cite> method.</dd>
<dt>batch_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=128)</span><dd>Number of documents to use in each EM iteration. Only used in online
learning.</dd>
<dt>evaluate_every</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int optional (default=0)</span><dd>How often to evaluate perplexity. Only used in <cite>fit</cite> method.
set it to 0 or and negative number to not evalute perplexity in
training at all. Evaluating perplexity can help you check convergence
in training process, but it will also increase total training time.
Evaluating perplexity in every iteration might increase training time
up to two-fold.</dd>
<dt>perp_tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-1)</span><dd>Perplexity tolerance in batch learning. Only used when
<code class="docutils literal"><span class="pre">evaluate_every</span></code> is greater than 0.</dd>
<dt>mean_change_tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span><dd>Stopping tolerance for updating document topic distribution in E-step.</dd>
<dt>max_doc_update_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (default=100)</span><dd>Max number of iterations for updating document topic distribution in
the E-step.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=1)</span><dd>The number of jobs to use in the E-step. If -1, all CPUs are used. For
<code class="docutils literal"><span class="pre">n_jobs</span></code> below -1, (n_cpus + 1 + n_jobs) are used.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Verbosity level.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState instance or None, optional (default=None)</span><dd>Pseudo-random number generator seed control.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_topics, n_features]</span><dd>Topic word distribution. <code class="docutils literal"><span class="pre">components_[i,</span> <span class="pre">j]</span></code> represents word j in
topic <cite>i</cite>. In the literature, this is called lambda.</dd>
<dt><code class="docutils literal"><span class="pre">n_batch_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations of the EM step.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of passes over the dataset.</dd>
</dl>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>[1] &#8220;Online Learning for Latent Dirichlet Allocation&#8221;, Matthew D. Hoffman,</dt>
<dd>David M. Blei, Francis Bach, 2010</dd>
<dt>[2] &#8220;Stochastic Variational Inference&#8221;, Matthew D. Hoffman, David M. Blei,</dt>
<dd>Chong Wang, John Paisley, 2013</dd>
</dl>
<p>[3] Matthew D. Hoffman&#8217;s onlineldavb code. Link:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.cs.princeton.edu/~mdhoffma/code/onlineldavb.tar">http://www.cs.princeton.edu/~mdhoffma/code/onlineldavb.tar</a></li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LatentDirichletAllocationTransformerSklearn</strong></li>
<li><strong>LatentDirichletAllocationTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-lineardiscriminantanalysisclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LinearDiscriminantAnalysisClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LinearDiscriminantAnalysisClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearDiscriminantAnalysisClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-lineardiscriminantanalysisclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LinearDiscriminantAnalysisClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LinearDiscriminantAnalysisClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LinearDiscriminantAnalysisClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear Discriminant Analysis</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A classifier with a linear decision boundary, generated by fitting class
conditional densities to the data and using Bayes&#8217; rule.</p>
<p>The model fits a Gaussian density to each class, assuming that all classes
share the same covariance matrix.</p>
<p>The fitted model can also be used to reduce the dimensionality of the input
by projecting it to the most discriminative directions.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>LinearDiscriminantAnalysis</em>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <code class="xref py py-class docutils literal"><span class="pre">lda.LDA</span></code> have been moved to <em>LinearDiscriminantAnalysis</em>.</p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd><p class="first">Solver to use, possible values:</p>
<blockquote class="last">
<div><ul class="simple">
<li><ul class="first">
<li>&#8216;svd&#8217;: Singular value decomposition (default). Does not compute the</li>
</ul>
</li>
<li>covariance matrix, therefore this solver is recommended for</li>
<li>data with a large number of features.</li>
<li><ul class="first">
<li>&#8216;lsqr&#8217;: Least squares solution, can be combined with shrinkage.</li>
</ul>
</li>
<li><ul class="first">
<li>&#8216;eigen&#8217;: Eigenvalue decomposition, can be combined with shrinkage.</li>
</ul>
</li>
</ul>
</div></blockquote>
</dd>
<dt>shrinkage</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or float, optional</span><dd><p class="first">Shrinkage parameter, possible values:</p>
<blockquote>
<div><ul class="simple">
<li><ul class="first">
<li>None: no shrinkage (default).</li>
</ul>
</li>
<li><ul class="first">
<li>&#8216;auto&#8217;: automatic shrinkage using the Ledoit-Wolf lemma.</li>
</ul>
</li>
<li><ul class="first">
<li>float between 0 and 1: fixed shrinkage parameter.</li>
</ul>
</li>
</ul>
</div></blockquote>
<p class="last">Note that shrinkage works only with &#8216;lsqr&#8217; and &#8216;eigen&#8217; solvers.</p>
</dd>
<dt>priors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape (n_classes,)</span><dd>Class priors.</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of components (&lt; n_classes - 1) for dimensionality reduction.</dd>
<dt>store_covariance</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd><p class="first">Additionally compute class covariance matrix (default False).</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd><p class="first">Threshold used for rank estimation in SVD solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_classes, n_features)</span><dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>Intercept term.</dd>
<dt><code class="docutils literal"><span class="pre">covariance_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features, n_features)</span><dd>Covariance matrix (shared by all classes).</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components,)</span><dd>Percentage of variance explained by each of the selected components.
If <code class="docutils literal"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of explained variances is equal to 1.0. Only available when eigen
solver is used.</dd>
<dt><code class="docutils literal"><span class="pre">means_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes, n_features)</span><dd>Class means.</dd>
<dt><code class="docutils literal"><span class="pre">priors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes,)</span><dd>Class priors (sum to 1).</dd>
<dt><code class="docutils literal"><span class="pre">scalings_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (rank, n_classes - 1)</span><dd>Scaling of the features in the space spanned by the class centroids.</dd>
<dt><code class="docutils literal"><span class="pre">xbar_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_features,)</span><dd>Overall mean.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_classes,)</span><dd>Unique class labels.</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic</dt>
<dd>Discriminant Analysis</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The default solver is &#8216;svd&#8217;. It can perform both classification and
transform, and it does not rely on the calculation of the covariance
matrix. This can be an advantage in situations where the number of features
is large. However, the &#8216;svd&#8217; solver cannot be used with shrinkage.</p>
<p>The &#8216;lsqr&#8217; solver is an efficient algorithm that only works for
classification. It supports shrinkage.</p>
<p>The &#8216;eigen&#8217; solver is based on the optimization of the between class
scatter to within class scatter ratio. It can be used for both
classification and transform, and it supports shrinkage. However, the
&#8216;eigen&#8217; solver needs to compute the covariance matrix, so it might not be
suitable for situations with a high number of features.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="k">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,</span>
<span class="go">              solver=&#39;svd&#39;, store_covariance=False, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LinearDiscriminantAnalysisClassifierSklearn</strong></li>
<li><strong>LinearDiscriminantAnalysisClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-linearregressionsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LinearRegressionSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LinearRegressionSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearRegressionSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-linearregressionsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LinearRegressionSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LinearRegressionSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LinearRegressionSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Ordinary least squares Linear Regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.base.LinearRegression.html">sklearn.linear_model.base.LinearRegression</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 1</span><dd>The number of jobs to use for the computation.
If -1 all CPUs are used. This will only provide speedup for
n_targets &gt; 1 and sufficient large problems.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features, ) or (n_targets, n_features)</span><dd>Estimated coefficients for the linear regression problem.
If multiple targets are passed during the fit (y 2D), this
is a 2D array of shape (n_targets, n_features), while if only
one target is passed, this is a 1D array of length n_features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>Independent term in the linear model.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>From the implementation point of view, this is just plain Ordinary
Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LinearRegressionSklearn</strong></li>
<li><strong>LinearRegressionSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-linearsvcclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LinearSVCClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LinearSVCClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearSVCClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-linearsvcclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LinearSVCClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LinearSVCClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LinearSVCClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.LinearSVC.html">sklearn.svm.classes.LinearSVC</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Similar to SVC with parameter kernel=&#8217;linear&#8217;, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input and the multiclass support
is handled according to a one-vs-the-rest scheme.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Penalty parameter C of the error term.</dd>
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;hinge&#8217; or &#8216;squared_hinge&#8217; (default=&#8217;squared_hinge&#8217;)</span><dd>Specifies the loss function. &#8216;hinge&#8217; is the standard SVM loss
(used e.g. by the SVC class) while &#8216;squared_hinge&#8217; is the
square of the hinge loss.</dd>
<dt>penalty</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;l1&#8217; or &#8216;l2&#8217; (default=&#8217;l2&#8217;)</span><dd>Specifies the norm used in the penalization. The &#8216;l2&#8217;
penalty is the standard used in SVC. The &#8216;l1&#8217; leads to <code class="docutils literal"><span class="pre">coef_</span></code>
vectors that are sparse.</dd>
<dt>dual</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span><dd>Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-4)</span><dd>Tolerance for stopping criteria.</dd>
<dt>multi_class: string, &#8216;ovr&#8217; or &#8216;crammer_singer&#8217; (default=&#8217;ovr&#8217;)</dt>
<dd>Determines the multi-class strategy if <cite>y</cite> contains more than
two classes.
<code class="docutils literal"><span class="pre">&quot;ovr&quot;</span></code> trains n_classes one-vs-rest classifiers, while <code class="docutils literal"><span class="pre">&quot;crammer_singer&quot;</span></code>
optimizes a joint objective over all classes.
While <cite>crammer_singer</cite> is interesting from a theoretical perspective
as it is consistent, it is seldom used in practice as it rarely leads
to better accuracy and is more expensive to compute.
If <code class="docutils literal"><span class="pre">&quot;crammer_singer&quot;</span></code> is chosen, the options loss, penalty and dual will
be ignored.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be already centered).</dd>
<dt>intercept_scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1)</span><dd>When self.fit_intercept is True, instance vector x becomes
<code class="docutils literal"><span class="pre">[x,</span> <span class="pre">self.intercept_scaling]</span></code>,
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;balanced&#8217;}, optional</span><dd>Set the parameter C of class i to <code class="docutils literal"><span class="pre">class_weight[i]*C</span></code> for
SVC. If not given, all classes are supposed to have
weight one.
The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=0)</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default=None)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=1000)</span><dd>The maximum number of iterations to be run.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><code class="docutils literal"><span class="pre">coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">raw_coef_</span></code> that
follows the internal memory layout of liblinear.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon
to have slightly different results for the same input data. If
that happens, try with a smaller <code class="docutils literal"><span class="pre">tol</span></code> parameter.</p>
<p>The underlying implementation, liblinear, uses a sparse internal
representation for the data that will incur a memory copy.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See <span class="xref std std-ref">differences from liblinear</span>
in the narrative documentation.</p>
<p><strong>References</strong></p>
<p><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR: A Library for Large Linear Classification</a></p>
<p>See also</p>
<dl class="docutils">
<dt>SVC</dt>
<dd><p class="first">Implementation of Support Vector Machine classifier using libsvm:</p>
<ul class="simple">
<li>the kernel can be non-linear but its SMO algorithm does not</li>
<li>scale to large number of samples as LinearSVC does.</li>
</ul>
<p>Furthermore SVC multi-class mode is implemented using one
vs one scheme while LinearSVC uses one vs the rest. It is
possible to implement one vs the rest with SVC by using the
<code class="xref py py-class docutils literal"><span class="pre">sklearn.multiclass.OneVsRestClassifier</span></code> wrapper.</p>
<p class="last">Finally SVC can fit dense data without memory copy if the input
is C-contiguous. Sparse data will still incur memory copy though.</p>
</dd>
<dt>sklearn.linear_model.SGDClassifier</dt>
<dd>SGDClassifier can optimize the same cost function as LinearSVC
by adjusting the penalty and loss parameters. In addition it requires
less memory, allows incremental (online) learning, and implements
various loss functions and regularization regimes.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LinearSVCClassifierSklearnNode</strong></li>
<li><strong>LinearSVCClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-linearsvrregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LinearSVRRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LinearSVRRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LinearSVRRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-linearsvrregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LinearSVRRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LinearSVRRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LinearSVRRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear Support Vector Regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.LinearSVR.html">sklearn.svm.classes.LinearSVR</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Similar to SVR with parameter kernel=&#8217;linear&#8217;, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Penalty parameter C of the error term. The penalty is a squared
l2 penalty. The bigger this parameter, the less regularization is used.</dd>
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;epsilon_insensitive&#8217; or &#8216;squared_epsilon_insensitive&#8217; (default=&#8217;epsilon_insensitive&#8217;)</span><dd>Specifies the loss function. &#8216;l1&#8217; is the epsilon-insensitive loss
(standard SVR) while &#8216;l2&#8217; is the squared epsilon-insensitive loss.</dd>
<dt>epsilon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span><dd>Epsilon parameter in the epsilon-insensitive loss function. Note
that the value of this parameter depends on the scale of the target
variable y. If unsure, set <code class="docutils literal"><span class="pre">epsilon=0</span></code>.</dd>
<dt>dual</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, (default=True)</span><dd>Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-4)</span><dd>Tolerance for stopping criteria.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be already centered).</dd>
<dt>intercept_scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1)</span><dd>When self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=0)</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default=None)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, (default=1000)</span><dd>The maximum number of iterations to be run.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is a readonly property derived from <cite>raw_coef_</cite> that
follows the internal memory layout of liblinear.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span><dd>Constants in decision function.</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>LinearSVC</dt>
<dd>Implementation of Support Vector Machine classifier using the
same library as this class (liblinear).</dd>
<dt>SVR</dt>
<dd><p class="first">Implementation of Support Vector Machine regression using libsvm:</p>
<ul class="last simple">
<li>the kernel can be non-linear but its SMO algorithm does not</li>
<li>scale to large number of samples as LinearSVC does.</li>
</ul>
</dd>
<dt>sklearn.linear_model.SGDRegressor</dt>
<dd>SGDRegressor can optimize the same cost function as LinearSVR
by adjusting the penalty and loss parameters. In addition it requires
less memory, allows incremental (online) learning, and implements
various loss functions and regularization regimes.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LinearSVRRegressorSklearn</strong></li>
<li><strong>LinearSVRRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-locallylinearembeddingtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LocallyLinearEmbeddingTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LocallyLinearEmbeddingTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LocallyLinearEmbeddingTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-locallylinearembeddingtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LocallyLinearEmbeddingTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LocallyLinearEmbeddingTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LocallyLinearEmbeddingTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Locally Linear Embedding</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear.LocallyLinearEmbedding.html">sklearn.manifold.locally_linear.LocallyLinearEmbedding</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_neighbors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>number of neighbors to consider for each point.</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>number of coordinates for the manifold</dd>
<dt>reg</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>regularization constant, multiplies the trace of the local covariance
matrix of the distances.</dd>
<dt>eigen_solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;auto&#8217;, &#8216;arpack&#8217;, &#8216;dense&#8217;}</span><dd><p class="first">auto : algorithm will attempt to choose the best method for input data</p>
<dl class="last docutils">
<dt>arpack</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">use arnoldi iteration in shift-invert mode.</span><dd>For this method, M may be a dense matrix, sparse matrix,
or general linear operator.
Warning: ARPACK can be unstable for some problems.  It is
best to try several random seeds in order to check results.</dd>
<dt>dense</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">use standard dense matrix operations for the eigenvalue</span><dd>decomposition.  For this method, M must be an array
or matrix type.  This method should be avoided for
large problems.</dd>
</dl>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Tolerance for &#8216;arpack&#8217; method
Not used if eigen_solver==&#8217;dense&#8217;.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>maximum number of iterations for the arpack solver.
Not used if eigen_solver==&#8217;dense&#8217;.</dd>
<dt>method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string (&#8216;standard&#8217;, &#8216;hessian&#8217;, &#8216;modified&#8217; or &#8216;ltsa&#8217;)</span><dd><dl class="first last docutils">
<dt>standard</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">use the standard locally linear embedding algorithm.  see</span><dd>reference [1]</dd>
<dt>hessian</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">use the Hessian eigenmap method. This method requires</span><dd><code class="docutils literal"><span class="pre">n_neighbors</span> <span class="pre">&gt;</span> <span class="pre">n_components</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">(n_components</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></code>
see reference [2]</dd>
<dt>modified</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">use the modified locally linear embedding algorithm.</span><dd>see reference [3]</dd>
<dt>ltsa</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">use local tangent space alignment algorithm</span><dd>see reference [4]</dd>
</dl>
</dd>
<dt>hessian_tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Tolerance for Hessian eigenmapping method.
Only used if <code class="docutils literal"><span class="pre">method</span> <span class="pre">==</span> <span class="pre">'hessian'</span></code></dd>
<dt>modified_tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Tolerance for modified LLE method.
Only used if <code class="docutils literal"><span class="pre">method</span> <span class="pre">==</span> <span class="pre">'modified'</span></code></dd>
<dt>neighbors_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string [&#8216;auto&#8217;|&#8217;brute&#8217;|&#8217;kd_tree&#8217;|&#8217;ball_tree&#8217;]</span><dd>algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance</dd>
<dt>random_state: numpy.RandomState or int, optional</dt>
<dd>The generator or seed used to determine the starting vector for arpack
iterations.  Defaults to numpy.random.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">embedding_vectors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape [n_components, n_samples]</span><dd>Stores the embedding vectors</dd>
<dt><code class="docutils literal"><span class="pre">reconstruction_error_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Reconstruction error associated with <cite>embedding_vectors_</cite></dd>
<dt><code class="docutils literal"><span class="pre">nbrs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">NearestNeighbors object</span><dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id43" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><cite>Roweis, S. &amp; Saul, L. Nonlinear dimensionality reduction
by locally linear embedding.  Science 290:2323 (2000).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id44" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><cite>Donoho, D. &amp; Grimes, C. Hessian eigenmaps: Locally
linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A.  100:5591 (2003).</cite></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id45" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><cite>Zhang, Z. &amp; Wang, J. MLLE: Modified Locally Linear
Embedding Using Multiple Weights.</cite>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id46" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><cite>Zhang, Z. &amp; Zha, H. Principal manifolds and nonlinear
dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ.  8:406 (2004)</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LocallyLinearEmbeddingTransformerSklearn</strong></li>
<li><strong>LocallyLinearEmbeddingTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-logisticregressioncvclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LogisticRegressionCVClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LogisticRegressionCVClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LogisticRegressionCVClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-logisticregressioncvclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LogisticRegressionCVClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LogisticRegressionCVClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LogisticRegressionCVClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Logistic Regression CV (aka logit, MaxEnt) classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.logistic.LogisticRegressionCV.html">sklearn.linear_model.logistic.LogisticRegressionCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This class implements logistic regression using liblinear, newton-cg, sag
of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
regularization with primal formulation. The liblinear solver supports both
L1 and L2 regularization, with a dual formulation only for the L2 penalty.</p>
<p>For the grid of Cs values (that are set by default to be ten values in
a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is
selected by the cross-validator StratifiedKFold, but it can be changed
using the cv parameter. In the case of newton-cg and lbfgs solvers,
we warm start along the path i.e guess the initial coefficients of the
present fit to be the coefficients got after convergence in the previous
fit, so it is supposed to be faster for high-dimensional dense data.</p>
<p>For a multiclass problem, the hyperparameters for each class are computed
using the best scores got by doing a one-vs-rest in parallel across all
folds and classes. Hence this is not the true multinomial loss.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>Cs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of floats | int</span><dd>Each of the values in Cs describes the inverse of regularization
strength. If Cs is as an int, then a grid of Cs values are chosen
in a logarithmic scale between 1e-4 and 1e4.
Like in support vector machines, smaller values specify stronger
regularization.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span><dd>Specifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict or &#8216;balanced&#8217;, optional</span><dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>class_weight == &#8216;balanced&#8217;</p>
</div>
</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or cross-validation generator</span><dd>The default cross-validation generator used is Stratified K-Folds.
If an integer is provided, then it is the number of folds used.
See the module <code class="xref py py-mod docutils literal"><span class="pre">sklearn.cross_validation</span></code> module for the
list of possible cross-validation objects.</dd>
<dt>penalty</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;l1&#8217; or &#8216;l2&#8217;</span><dd>Used to specify the norm used in the penalization. The newton-cg and
lbfgs solvers support only l2 penalties.</dd>
<dt>dual</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Dual or primal formulation. Dual formulation is only implemented for
l2 penalty with liblinear solver. Prefer dual=False when
n_samples &gt; n_features.</dd>
<dt>scoring</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callabale</span><dd>Scoring function to use as cross-validation criteria. For a list of
scoring functions that can be used, look at <code class="xref py py-mod docutils literal"><span class="pre">sklearn.metrics</span></code>.
The default scoring option used is accuracy_score.</dd>
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;newton-cg&#8217;, &#8216;lbfgs&#8217;, &#8216;liblinear&#8217;, &#8216;sag&#8217;}</span><dd><p class="first">Algorithm to use in the optimization problem.</p>
<ul class="last simple">
<li><dl class="first docutils">
<dt>For small datasets, &#8216;liblinear&#8217; is a good choice, whereas &#8216;sag&#8217; is</dt>
<dd>faster for large ones.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>For multiclass problems, only &#8216;newton-cg&#8217; and &#8216;lbfgs&#8217; handle</dt>
<dd>multinomial loss; &#8216;sag&#8217; and &#8216;liblinear&#8217; are limited to
one-versus-rest schemes.</dd>
</dl>
</li>
<li>&#8216;newton-cg&#8217;, &#8216;lbfgs&#8217; and &#8216;sag&#8217; only handle L2 penalty.</li>
<li><dl class="first docutils">
<dt>&#8216;liblinear&#8217; might be slower in LogisticRegressionCV because it does</dt>
<dd>not handle warm-starting.</dd>
</dl>
</li>
</ul>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Tolerance for stopping criteria.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of iterations of the optimization algorithm.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of CPU cores used during the cross-validation loop. If given
a value of -1, all cores are used.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>For the &#8216;liblinear&#8217;, &#8216;sag&#8217; and &#8216;lbfgs&#8217; solvers set verbose to any
positive number for verbosity.</dd>
<dt>refit</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>If set to True, the scores are averaged across all folds, and the
coefs and the C that corresponds to the best score is taken, and a
final refit is done using these parameters.
Otherwise the coefs, intercepts and C that correspond to the
best scores across folds are averaged.</dd>
<dt>multi_class</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, {&#8216;ovr&#8217;, &#8216;multinomial&#8217;}</span><dd>Multiclass option can be either &#8216;ovr&#8217; or &#8216;multinomial&#8217;. If the option
chosen is &#8216;ovr&#8217;, then a binary problem is fit for each label. Else
the loss minimised is the multinomial loss fit across
the entire probability distribution. Works only for &#8216;lbfgs&#8217; and
&#8216;newton-cg&#8217; solvers.</dd>
<dt>intercept_scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default 1.</span><dd>Useful only if solver is liblinear.
This parameter is useful only when the solver &#8216;liblinear&#8217; is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1, n_features) or (n_classes, n_features)</span><dd><p class="first">Coefficient of the features in the decision function.</p>
<p class="last"><cite>coef_</cite> is of shape (1, n_features) when the given problem
is binary.
<cite>coef_</cite> is readonly property derived from <cite>raw_coef_</cite> that
follows the internal memory layout of liblinear.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,) or (n_classes,)</span><dd>Intercept (a.k.a. bias) added to the decision function.
It is available only when parameter intercept is set to True
and is of shape(1,) when the problem is binary.</dd>
<dt><code class="docutils literal"><span class="pre">Cs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>Array of C i.e. inverse of regularization parameter values used
for cross-validation.</dd>
<dt><code class="docutils literal"><span class="pre">coefs_paths_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape <code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features)</span></code> or                    <code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features</span> <span class="pre">+</span> <span class="pre">1)</span></code></span><dd>dict with classes as the keys, and the path of coefficients obtained
during cross-validating across each fold and then across each Cs
after doing an OvR for the corresponding class as values.
If the &#8216;multi_class&#8217; option is set to &#8216;multinomial&#8217;, then
the coefs_paths are the coefficients corresponding to each class.
Each dict value has shape <code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features)</span></code> or
<code class="docutils literal"><span class="pre">(n_folds,</span> <span class="pre">len(Cs_),</span> <span class="pre">n_features</span> <span class="pre">+</span> <span class="pre">1)</span></code> depending on whether the
intercept is fit or not.</dd>
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>dict with classes as the keys, and the values as the
grid of scores obtained during cross-validating each fold, after doing
an OvR for the corresponding class. If the &#8216;multi_class&#8217; option
given is &#8216;multinomial&#8217; then the same scores are repeated across
all classes, since this is the multinomial class.
Each dict value has shape (n_folds, len(Cs))</dd>
<dt><code class="docutils literal"><span class="pre">C_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,) or (n_classes - 1,)</span><dd>Array of C that maps to the best scores across every class. If refit is
set to False, then for each class, the best C is the average of the
C&#8217;s that correspond to the best scores for each fold.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</span><dd>Actual number of iterations for all classes, folds and Cs.
In the binary or multinomial cases, the first dimension is equal to 1.</dd>
</dl>
<p>See also</p>
<p>LogisticRegression</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LogisticRegressionCVClassifierSklearnNode</strong></li>
<li><strong>LogisticRegressionCVClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-logisticregressionclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.LogisticRegressionClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.LogisticRegressionClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.LogisticRegressionClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-logisticregressionclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.LogisticRegressionClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">LogisticRegressionClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.LogisticRegressionClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Logistic Regression (aka logit, MaxEnt) classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.logistic.LogisticRegression.html">sklearn.linear_model.logistic.LogisticRegression</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the &#8216;multi_class&#8217; option is set to &#8216;ovr&#8217; and uses the
cross-entropy loss, if the &#8216;multi_class&#8217; option is set to &#8216;multinomial&#8217;.
(Currently the &#8216;multinomial&#8217; option is supported only by the &#8216;lbfgs&#8217; and
&#8216;newton-cg&#8217; solvers.)</p>
<p>This class implements regularized logistic regression using the
<cite>liblinear</cite> library, newton-cg and lbfgs solvers. It can handle both
dense and sparse input. Use C-ordered arrays or CSR matrices containing
64-bit floats for optimal performance; any other input format will be
converted (and copied).</p>
<p>The newton-cg and lbfgs solvers support only L2 regularization with primal
formulation. The liblinear solver supports both L1 and L2 regularization,
with a dual formulation only for the L2 penalty.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;l1&#8217; or &#8216;l2&#8217;</span><dd>Used to specify the norm used in the penalization. The newton-cg and
lbfgs solvers support only l2 penalties.</dd>
<dt>dual</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Dual or primal formulation. Dual formulation is only implemented for
l2 penalty with liblinear solver. Prefer dual=False when
n_samples &gt; n_features.</dd>
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span><dd>Specifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.</dd>
<dt>intercept_scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1</span><dd>Useful only if solver is liblinear.
when self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a &#8220;synthetic&#8221; feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict or &#8216;balanced&#8217;, optional</span><dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>class_weight=&#8217;balanced&#8217;</em> instead of deprecated <em>class_weight=&#8217;auto&#8217;</em>.</p>
</div>
</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Useful only for the newton-cg, sag and lbfgs solvers.
Maximum number of iterations taken for the solvers to converge.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;newton-cg&#8217;, &#8216;lbfgs&#8217;, &#8216;liblinear&#8217;, &#8216;sag&#8217;}</span><dd><p class="first">Algorithm to use in the optimization problem.</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>For small datasets, &#8216;liblinear&#8217; is a good choice, whereas &#8216;sag&#8217; is</dt>
<dd>faster for large ones.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>For multiclass problems, only &#8216;newton-cg&#8217; and &#8216;lbfgs&#8217; handle</dt>
<dd>multinomial loss; &#8216;sag&#8217; and &#8216;liblinear&#8217; are limited to
one-versus-rest schemes.</dd>
</dl>
</li>
<li>&#8216;newton-cg&#8217;, &#8216;lbfgs&#8217; and &#8216;sag&#8217; only handle L2 penalty.</li>
</ul>
<p>Note that &#8216;sag&#8217; fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Tolerance for stopping criteria.</dd>
<dt>multi_class</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, {&#8216;ovr&#8217;, &#8216;multinomial&#8217;}</span><dd>Multiclass option can be either &#8216;ovr&#8217; or &#8216;multinomial&#8217;. If the option
chosen is &#8216;ovr&#8217;, then a binary problem is fit for each label. Else
the loss minimised is the multinomial loss fit across
the entire probability distribution. Works only for the &#8216;lbfgs&#8217;
solver.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd><p class="first">When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>warm_start</em> to support <em>lbfgs</em>, <em>newton-cg</em>, <em>sag</em> solvers.</p>
</div>
</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of CPU cores used during the cross-validation loop. If given
a value of -1, all cores are used.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span><dd>Coefficient of the features in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span><dd>Intercept (a.k.a. bias) added to the decision function.
If <cite>fit_intercept</cite> is set to False, the intercept is set to zero.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,) or (1, )</span><dd>Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>SGDClassifier</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">incrementally trained logistic regression (when given</span><dd>the parameter <code class="docutils literal"><span class="pre">loss=&quot;log&quot;</span></code>).</dd>
</dl>
<p>sklearn.svm.LinearSVC : learns SVM models using the same algorithm.</p>
<p><strong>Notes</strong></p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See <span class="xref std std-ref">differences from liblinear</span>
in the narrative documentation.</p>
<p><strong>References</strong></p>
<dl class="docutils">
<dt>LIBLINEAR &#8211; A Library for Large Linear Classification</dt>
<dd><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">http://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></dd>
<dt>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</dt>
<dd>methods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf">http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>LogisticRegressionClassifierSklearn</strong></li>
<li><strong>LogisticRegressionClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-maxabsscalertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MaxAbsScalerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MaxAbsScalerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MaxAbsScalerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-maxabsscalertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MaxAbsScalerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MaxAbsScalerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MaxAbsScalerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Scale each feature by its maximum absolute value.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.MaxAbsScaler.html">sklearn.preprocessing.data.MaxAbsScaler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This estimator scales and translates each feature individually such
that the maximal absolute value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.</p>
<p>This scaler can also be applied to sparse CSR or CSC matrices.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span><dd>Set to False to perform inplace scaling and avoid a copy (if the input
is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scale_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd><p class="first">Per feature relative scaling of the data.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> attribute.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">max_abs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd>Per feature maximum absolute value.</dd>
<dt><code class="docutils literal"><span class="pre">n_samples_seen_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal"><span class="pre">partial_fit</span></code> calls.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MaxAbsScalerTransformerSklearnNode</strong></li>
<li><strong>MaxAbsScalerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-minmaxscalertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MinMaxScalerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MinMaxScalerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MinMaxScalerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-minmaxscalertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MinMaxScalerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MinMaxScalerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MinMaxScalerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Transforms features by scaling each feature to a given range.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.MinMaxScaler.html">sklearn.preprocessing.data.MinMaxScaler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This estimator scales and translates each feature individually such
that it is in the given range on the training set, i.e. between
zero and one.</p>
<p>The transformation is given by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">X_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_std</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">+</span> <span class="nb">min</span>
</pre></div>
</div>
<p>where min, max = feature_range.</p>
<p>This transformation is often used as an alternative to zero mean,
unit variance scaling.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>feature_range: tuple (min, max), default=(0, 1)</dt>
<dd>Desired range of transformed data.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>Set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">min_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd>Per feature adjustment for minimum.</dd>
<dt><code class="docutils literal"><span class="pre">scale_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd><p class="first">Per feature relative scaling of the data.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> attribute.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">data_min_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd><p class="first">Per feature minimum seen in the data</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>data_min_</em> instead of deprecated <em>data_min</em>.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">data_max_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd><p class="first">Per feature maximum seen in the data</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>data_max_</em> instead of deprecated <em>data_max</em>.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">data_range_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd><p class="first">Per feature range <code class="docutils literal"><span class="pre">(data_max_</span> <span class="pre">-</span> <span class="pre">data_min_)</span></code> seen in the data</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>data_range_</em> instead of deprecated <em>data_range</em>.</p>
</div>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MinMaxScalerTransformerSklearnNode</strong></li>
<li><strong>MinMaxScalerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-minibatchdictionarylearningtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MiniBatchDictionaryLearningTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MiniBatchDictionaryLearningTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MiniBatchDictionaryLearningTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-minibatchdictionarylearningtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MiniBatchDictionaryLearningTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MiniBatchDictionaryLearningTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MiniBatchDictionaryLearningTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Mini-batch dictionary learning</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.html">sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Finds a dictionary (a set of atoms) that can best be used to represent data
using a sparse code.</p>
<p>Solves the optimization problem:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">U</span><span class="o">^*</span><span class="p">,</span><span class="n">V</span><span class="o">^*</span><span class="p">)</span> <span class="o">=</span> <span class="n">argmin</span> <span class="mf">0.5</span> <span class="o">||</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">U</span> <span class="n">V</span> <span class="o">||</span><span class="n">_2</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span> <span class="n">U</span> <span class="o">||</span><span class="n">_1</span>
             <span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
             <span class="k">with</span> <span class="o">||</span> <span class="n">V_k</span> <span class="o">||</span><span class="n">_2</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">for</span> <span class="nb">all</span>  <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">n_components</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of dictionary elements to extract</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>sparsity controlling parameter</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>total number of iterations to perform</dd>
<dt>fit_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span><dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>transform_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span><dd>Algorithm used to transform the data.
lars: uses the least angle regression method (linear_model.lars_path)
lasso_lars: uses Lars to compute the Lasso solution
lasso_cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). lasso_lars will be faster if
the estimated components are sparse.
omp: uses orthogonal matching pursuit to estimate the sparse solution
threshold: squashes to zero all coefficients less than alpha from
the projection dictionary * X&#8217;</dd>
<dt>transform_n_nonzero_coefs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, <code class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></code> by default</span><dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span><dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span><dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of parallel jobs to run</dd>
<dt>dict_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span><dd>initial value of the dictionary for warm restart scenarios</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of verbosity of the printed output</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>batch_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of samples in each mini-batch</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool,</span><dd>whether to shuffle the samples before forming batches</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>components extracted from the data</dd>
<dt><code class="docutils literal"><span class="pre">inner_stats_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tuple of (A, B) ndarrays</span><dd>Internal sufficient statistics that are kept by the algorithm.
Keeping them is useful in online settings, to avoid loosing the
history of the evolution, but they shouldn&#8217;t have any use for the
end user.
A (n_components, n_components) is the dictionary covariance matrix.
B (n_features, n_components) is the data approximation matrix</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><strong>References:</strong></p>
<p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning
for sparse coding (<a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">http://www.di.ens.fr/sierra/pdfs/icml09.pdf</a>)</p>
<p>See also</p>
<p>SparseCoder
DictionaryLearning
SparsePCA
MiniBatchSparsePCA</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MiniBatchDictionaryLearningTransformerSklearn</strong></li>
<li><strong>MiniBatchDictionaryLearningTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-minibatchsparsepcatransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MiniBatchSparsePCATransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MiniBatchSparsePCATransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MiniBatchSparsePCATransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-minibatchsparsepcatransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MiniBatchSparsePCATransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MiniBatchSparsePCATransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MiniBatchSparsePCATransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Mini-batch Sparse Principal Components Analysis</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.html">sklearn.decomposition.sparse_pca.MiniBatchSparsePCA</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of sparse atoms to extract</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of iterations to perform for each mini batch</dd>
<dt>callback</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable,</span><dd>callable that gets invoked every five iterations</dd>
<dt>batch_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>the number of features to take in each mini batch</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>degree of output the procedure will print</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean,</span><dd>whether to shuffle the data before splitting it in batches</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of parallel jobs to run, or -1 to autodetect.</dd>
<dt>method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span><dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Sparse components extracted from the data.</dd>
<dt><code class="docutils literal"><span class="pre">error_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>Vector of errors at each iteration.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p>See also</p>
<p>PCA
SparsePCA
DictionaryLearning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MiniBatchSparsePCATransformerSklearnNode</strong></li>
<li><strong>MiniBatchSparsePCATransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-multilabelbinarizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MultiLabelBinarizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MultiLabelBinarizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiLabelBinarizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-multilabelbinarizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MultiLabelBinarizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MultiLabelBinarizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MultiLabelBinarizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Transform between iterable of iterables and a multilabel format</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label.MultiLabelBinarizer.html">sklearn.preprocessing.label.MultiLabelBinarizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Although a list of sets or tuples is a very intuitive format for multilabel
data, it is unwieldy to process. This transformer converts between this
intuitive format and the supported multilabel format: a (samples x classes)
binary matrix indicating the presence of a class label.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>classes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like of shape [n_classes] (optional)</span><dd>Indicates an ordering for the class labels</dd>
<dt>sparse_output</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean (default: False),</span><dd>Set to true if output binary array is desired in CSR sparse format</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of labels</span><dd>A copy of the <cite>classes</cite> parameter where provided,
or otherwise, the sorted set of classes found when fitting.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span> <span class="o">=</span> <span class="n">MultiLabelBinarizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span>
<span class="go">array([[1, 1, 0],</span>
<span class="go">       [0, 0, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span><span class="o">.</span><span class="n">classes_</span>
<span class="go">array([1, 2, 3])</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mlb</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="nb">set</span><span class="p">([</span><span class="s1">&#39;sci-fi&#39;</span><span class="p">,</span> <span class="s1">&#39;thriller&#39;</span><span class="p">]),</span> <span class="nb">set</span><span class="p">([</span><span class="s1">&#39;comedy&#39;</span><span class="p">])])</span>
<span class="go">array([[0, 1, 1],</span>
<span class="go">       [1, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">mlb</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="go">[&#39;comedy&#39;, &#39;sci-fi&#39;, &#39;thriller&#39;]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MultiLabelBinarizerTransformerSklearnNode</strong></li>
<li><strong>MultiLabelBinarizerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-multitaskelasticnetcvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-multitaskelasticnetcvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MultiTaskElasticNetCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Multi-task L1/L2 ElasticNet with built-in cross-validation.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.html">sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for MultiTaskElasticNet is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, optional</span><dd>List of alphas where to compute the models.
If not provided, set automatically.</dd>
<dt>n_alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of alphas along the regularization path</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float or array of floats</span><dd>The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it
is an L1 penalty.
For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1/L2 and L2.
This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of
values for l1_ratio is often to put more values close to 1
(i.e. Lasso) and less close to 0 (i.e. Ridge), as in <code class="docutils literal"><span class="pre">[.1,</span> <span class="pre">.5,</span> <span class="pre">.7,</span>
<span class="pre">.9,</span> <span class="pre">.95,</span> <span class="pre">.99,</span> <span class="pre">1]</span></code></dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span><dd>Amount of verbosity.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs. Note that this is used only if multiple values for
l1_ratio are given.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span><dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span><dd>Parameter vector (W in the cost function formula).</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)</span><dd>mean square error for the test set on each fold, varying alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)</span><dd>The grid of alphas used for fitting, for each l1_ratio</dd>
<dt><code class="docutils literal"><span class="pre">l1_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>best l1_ratio obtained by cross-validation.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskElasticNetCV</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
<span class="gp">... </span>        <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">... </span>
<span class="go">MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,</span>
<span class="go">       fit_intercept=True, l1_ratio=0.5, max_iter=1000, n_alphas=100,</span>
<span class="go">       n_jobs=1, normalize=False, random_state=None, selection=&#39;cyclic&#39;,</span>
<span class="go">       tol=0.0001, verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[ 0.52875032  0.46958558]</span>
<span class="go"> [ 0.52875032  0.46958558]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[ 0.00166409  0.00166409]</span>
</pre></div>
</div>
<p>See also</p>
<p>MultiTaskElasticNet
ElasticNetCV
MultiTaskLassoCV</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MultiTaskElasticNetCVRegressorSklearn</strong></li>
<li><strong>MultiTaskElasticNetCVRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-multitaskelasticnetregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-multitaskelasticnetregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MultiTaskElasticNetRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskElasticNetRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.html">sklearn.linear_model.coordinate_descent.MultiTaskElasticNet</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for MultiTaskElasticNet is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Constant that multiplies the L1/L2 term. Defaults to 1.0</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it
is an L1 penalty.
For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1/L2 and L2.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span><dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span><dd>Parameter vector (W in the cost function formula). If a 1D y is         passed in at fit (non multi-task usage), <code class="docutils literal"><span class="pre">coef_</span></code> is then a 1D array</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">... </span>
<span class="go">MultiTaskElasticNet(alpha=0.1, copy_X=True, fit_intercept=True,</span>
<span class="go">        l1_ratio=0.5, max_iter=1000, normalize=False, random_state=None,</span>
<span class="go">        selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[ 0.45663524  0.45612256]</span>
<span class="go"> [ 0.45663524  0.45612256]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[ 0.0872422  0.0872422]</span>
</pre></div>
</div>
<p>See also</p>
<p>ElasticNet, MultiTaskLasso</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MultiTaskElasticNetRegressorSklearn</strong></li>
<li><strong>MultiTaskElasticNetRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-multitasklassocvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-multitasklassocvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MultiTaskLassoCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Multi-task L1/L2 Lasso with built-in cross-validation.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.html">sklearn.linear_model.coordinate_descent.MultiTaskLassoCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for MultiTaskLasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Length of the path. <code class="docutils literal"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</dd>
<dt>alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, optional</span><dd>List of alphas where to compute the models.
If not provided, set automaticlly.</dd>
<dt>n_alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of alphas along the regularization path</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or integer</span><dd>Amount of verbosity.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs. Note that this is used only if multiple values for
l1_ratio are given.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span><dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span><dd>Parameter vector (W in the cost function formula).</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The amount of penalization chosen by cross validation</dd>
<dt><code class="docutils literal"><span class="pre">mse_path_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_alphas, n_folds)</span><dd>mean square error for the test set on each fold, varying alpha</dd>
<dt><code class="docutils literal"><span class="pre">alphas_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array, shape (n_alphas,)</span><dd>The grid of alphas used for fitting.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</dd>
</dl>
<p>See also</p>
<p>MultiTaskElasticNet
ElasticNetCV
MultiTaskElasticNetCV</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MultiTaskLassoCVRegressorSklearn</strong></li>
<li><strong>MultiTaskLassoCVRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-multitasklassoregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-multitasklassoregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MultiTaskLassoRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MultiTaskLassoRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.coordinate_descent.MultiTaskLasso.html">sklearn.linear_model.coordinate_descent.MultiTaskLasso</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="mi">2</span><span class="n">_Fro</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of earch row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Constant that multiplies the L1/L2 term. Defaults to 1.0</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, the regressors X will be normalized before regression.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The maximum number of iterations</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>selection</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, default &#8216;cyclic&#8217;</span><dd>If set to &#8216;random&#8217;, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to &#8216;random&#8217;) often leads to significantly faster convergence
especially when tol is higher than 1e-4</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator that selects
a random feature to update. Useful only when selection is set to
&#8216;random&#8217;.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks, n_features)</span><dd>parameter vector (W in the cost function formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_tasks,)</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskLasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">MultiTaskLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="go">        normalize=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001,</span>
<span class="go">        warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[ 0.89393398  0.        ]</span>
<span class="go"> [ 0.89393398  0.        ]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[ 0.10606602  0.10606602]</span>
</pre></div>
</div>
<p>See also</p>
<p>Lasso, MultiTaskElasticNet</p>
<p><strong>Notes</strong></p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MultiTaskLassoRegressorSklearnNode</strong></li>
<li><strong>MultiTaskLassoRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-multinomialnbclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.MultinomialNBClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.MultinomialNBClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.MultinomialNBClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-multinomialnbclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.MultinomialNBClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">MultinomialNBClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.MultinomialNBClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Naive Bayes classifier for multinomial models</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">sklearn.naive_bayes.MultinomialNB</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</dd>
<dt>fit_prior</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</dd>
<dt>class_prior</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, size (n_classes,)</span><dd>Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">class_log_prior_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, )</span><dd>Smoothed empirical log probability for each class.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">property</span><dd>Mirrors <code class="docutils literal"><span class="pre">class_log_prior_</span></code> for interpreting MultinomialNB
as a linear model.</dd>
<dt><code class="docutils literal"><span class="pre">feature_log_prob_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span><dd>Empirical log probability of features
given a class, <code class="docutils literal"><span class="pre">P(x_i|y)</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">property</span><dd>Mirrors <code class="docutils literal"><span class="pre">feature_log_prob_</span></code> for interpreting MultinomialNB
as a linear model.</dd>
<dt><code class="docutils literal"><span class="pre">class_count_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes,)</span><dd>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</dd>
<dt><code class="docutils literal"><span class="pre">feature_count_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_classes, n_features)</span><dd>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">MultinomialNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>For the rationale behind the names <cite>coef_</cite> and <cite>intercept_</cite>, i.e.
naive Bayes as a linear classifier, see J. Rennie et al. (2003),
Tackling the poor assumptions of naive Bayes text classifiers, ICML.</p>
<p><strong>References</strong></p>
<p>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
<a class="reference external" href="http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>MultinomialNBClassifierSklearn</strong></li>
<li><strong>MultinomialNBClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-nmftransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.NMFTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.NMFTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NMFTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-nmftransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.NMFTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">NMFTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.NMFTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Non-Negative Matrix Factorization (NMF)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.nmf.NMF.html">sklearn.decomposition.nmf.NMF</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Find two non-negative matrices (W, H) whose product approximates the non-
negative matrix X. This factorization can be used for example for
dimensionality reduction, source separation or topic extraction.</p>
<p>The objective function is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.5</span> <span class="o">*</span> <span class="o">||</span><span class="n">X</span> <span class="o">-</span> <span class="n">WH</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">H</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">A</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span> <span class="o">=</span> \<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">}</span> <span class="n">A_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span> <span class="p">(</span><span class="n">Frobenius</span> <span class="n">norm</span><span class="p">)</span>
<span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span> <span class="o">=</span> \<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">}</span> <span class="nb">abs</span><span class="p">(</span><span class="n">A_</span><span class="p">{</span><span class="n">ij</span><span class="p">})</span> <span class="p">(</span><span class="n">Elementwise</span> <span class="n">L1</span> <span class="n">norm</span><span class="p">)</span>
</pre></div>
</div>
<p>The objective function is minimized with an alternating minimization of W
and H.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span><dd>Number of components, if n_components is not set all features
are kept.</dd>
<dt>init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;random&#8217; | &#8216;nndsvd&#8217; |  &#8216;nndsvda&#8217; | &#8216;nndsvdar&#8217; | &#8216;custom&#8217;</span><dd><p class="first">Method used to initialize the procedure.
Default: &#8216;nndsvdar&#8217; if n_components &lt; n_features, otherwise random.
Valid options:</p>
<ul class="last">
<li><p class="first">&#8216;random&#8217;: non-negative random matrices, scaled with:</p>
<blockquote>
<div><ul class="simple">
<li>sqrt(X.mean() / n_components)</li>
</ul>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>&#8216;nndsvd&#8217;: Nonnegative Double Singular Value Decomposition (NNDSVD)</dt>
<dd><p class="first last">initialization (better for sparseness)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>&#8216;nndsvda&#8217;: NNDSVD with zeros filled with the average of X</dt>
<dd><p class="first last">(better when sparsity is not desired)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>&#8216;nndsvdar&#8217;: NNDSVD with zeros filled with small random values</dt>
<dd><p class="first last">(generally faster, less accurate alternative to NNDSVDa
for when sparsity is not desired)</p>
</dd>
</dl>
</li>
<li><p class="first">&#8216;custom&#8217;: use custom matrices W and H</p>
</li>
</ul>
</dd>
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;pg&#8217; | &#8216;cd&#8217;</span><dd><p class="first">Numerical solver to use:</p>
<ul class="simple">
<li>&#8216;pg&#8217; is a Projected Gradient solver (deprecated).</li>
<li>&#8216;cd&#8217; is a Coordinate Descent solver (recommended).</li>
</ul>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Coordinate Descent solver.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver.</p>
</div>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1e-4</span><dd>Tolerance value used in stopping conditions.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, default: 200</span><dd>Number of iterations to compute.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer seed, RandomState instance, or None (default)</span><dd>Random number generator seed control.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.</span><dd><p class="first">Constant that multiplies the regularization terms. Set it to zero to
have no regularization.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>alpha</em> used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.</span><dd><p class="first">The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an elementwise L2 penalty
(aka Frobenius Norm).
For l1_ratio = 1 it is an elementwise L1 penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Regularization parameter <em>l1_ratio</em> used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default: False</span><dd><p class="first">If true, randomize the order of coordinates in the CD solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>shuffle</em> parameter used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>nls_max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, default: 2000</span><dd><p class="first">Number of iterations in NLS subproblem.
Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
<dt>sparseness</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;data&#8217; | &#8216;components&#8217; | None, default: None</span><dd><p class="first">Where to enforce sparsity in the model.
Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
<dt>beta</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1</span><dd><p class="first">Degree of sparseness, if sparseness is not None. Larger values mean
more sparseness. Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
<dt>eta</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.1</span><dd><p class="first">Degree of correctness to maintain, if sparsity is not None. Smaller
values mean larger error. Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Non-negative components of the data.</dd>
<dt><code class="docutils literal"><span class="pre">reconstruction_err_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">number</span><dd>Frobenius norm of the matrix difference between
the training data and the reconstructed data from
the fit produced by the model. <code class="docutils literal"><span class="pre">||</span> <span class="pre">X</span> <span class="pre">-</span> <span class="pre">WH</span> <span class="pre">||_2</span></code></dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Actual number of iterations.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">NMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">NMF(alpha=0.0, beta=1, eta=0.1, init=&#39;random&#39;, l1_ratio=0.0, max_iter=200,</span>
<span class="go">  n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,</span>
<span class="go">  solver=&#39;cd&#39;, sparseness=None, tol=0.0001, verbose=0)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 2.09783018,  0.30560234],</span>
<span class="go">       [ 2.13443044,  2.13171694]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.00115993...</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>C.-J. Lin. Projected gradient methods for non-negative matrix
factorization. Neural Computation, 19(2007), 2756-2779.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/nmf/">http://www.csie.ntu.edu.tw/~cjlin/nmf/</a></p>
<p>Cichocki, Andrzej, and P. H. A. N. Anh-Huy. &#8220;Fast local algorithms for
large scale nonnegative matrix and tensor factorizations.&#8221;
IEICE transactions on fundamentals of electronics, communications and
computer sciences 92.3: 708-721, 2009.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>NMFTransformerSklearn</strong></li>
<li><strong>NMFTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-nearestcentroidclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.NearestCentroidClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.NearestCentroidClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NearestCentroidClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-nearestcentroidclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.NearestCentroidClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">NearestCentroidClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.NearestCentroidClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Nearest centroid classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.nearest_centroid.NearestCentroid.html">sklearn.neighbors.nearest_centroid.NearestCentroid</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Each class is represented by its centroid, with test samples classified to
the class with the nearest centroid.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>metric: string, or callable</dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string or callable, it must be one of
the options allowed by metrics.pairwise.pairwise_distances for its
metric parameter.
The centroids for the samples corresponding to each class is the point
from which the sum of the distances (according to the metric) of all
samples that belong to that particular class are minimized.
If the &#8220;manhattan&#8221; metric is provided, this centroid is the median and
for all other metrics, the centroid is now set to be the mean.</dd>
<dt>shrink_threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = None)</span><dd>Threshold for shrinking centroids to remove features.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">centroids_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span><dd>Centroid of each class</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="k">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid(metric=&#39;euclidean&#39;, shrink_threshold=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>sklearn.neighbors.KNeighborsClassifier: nearest neighbors classifier</p>
<p><strong>Notes</strong></p>
<p>When used for text classification with tf-idf vectors, this classifier is
also known as the Rocchio classifier.</p>
<p><strong>References</strong></p>
<p>Tibshirani, R., Hastie, T., Narasimhan, B., &amp; Chu, G. (2002). Diagnosis of
multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>NearestCentroidClassifierSklearnNode</strong></li>
<li><strong>NearestCentroidClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-normalizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.NormalizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.NormalizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NormalizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-normalizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.NormalizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">NormalizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.NormalizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Normalize samples individually to unit norm.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.Normalizer.html">sklearn.preprocessing.data.Normalizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Each sample (i.e. each row of the data matrix) with at least one
non zero component is rescaled independently of other samples so
that its norm (l1 or l2) equals one.</p>
<p>This transformer is able to work both with dense numpy arrays and
scipy.sparse matrix (use CSR format if you want to avoid the burden of
a copy / conversion).</p>
<p>Scaling inputs to unit norms is a common operation for text
classification or clustering for instance. For instance the dot
product of two l2-normalized TF-IDF vectors is the cosine similarity
of the vectors and is the base similarity metric for the Vector
Space Model commonly used by the Information Retrieval community.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217;, or &#8216;max&#8217;, optional (&#8216;l2&#8217; by default)</span><dd>The norm to use to normalize each non zero sample.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>set to False to perform inplace row normalization and avoid a
copy (if the input is already a numpy array or a scipy.sparse
CSR matrix).</dd>
</dl>
<p><strong>Notes</strong></p>
<p>This estimator is stateless (besides constructor parameters), the
fit method does nothing but is useful when used in a pipeline.</p>
<p>See also</p>
<p><code class="xref py py-func docutils literal"><span class="pre">sklearn.preprocessing.normalize()</span></code> equivalent function
without the object oriented API</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>NormalizerTransformerSklearnNode</strong></li>
<li><strong>NormalizerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-nusvcclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.NuSVCClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.NuSVCClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NuSVCClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-nusvcclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.NuSVCClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">NuSVCClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.NuSVCClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Nu-Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.NuSVC.html">sklearn.svm.classes.NuSVC</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Similar to SVC but uses a parameter to control the number of support
vectors.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>nu</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.5)</span><dd>An upper bound on the fraction of training errors and a lower
bound of the fraction of support vectors. Should be in the
interval (0, 1].</dd>
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span><dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span><dd>Degree of the polynomial kernel function (&#8216;poly&#8217;).
Ignored by all other kernels.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=&#8217;auto&#8217;)</span><dd>Kernel coefficient for &#8216;rbf&#8217;, &#8216;poly&#8217; and &#8216;sigmoid&#8217;.
If gamma is &#8216;auto&#8217; then 1/n_features will be used instead.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span><dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>probability</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd>Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, and will slow down that method.</dd>
<dt>shrinking</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to use the shrinking heuristic.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span><dd>Tolerance for stopping criterion.</dd>
<dt>cache_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Specify the size of the kernel cache (in MB).</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;auto&#8217;}, optional</span><dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The &#8216;auto&#8217; mode uses the values of y to
automatically adjust weights inversely proportional to
class frequencies.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span><dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>decision_function_shape</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;ovo&#8217;, &#8216;ovr&#8217; or None, default=None</span><dd><p class="first">Whether to return a one-vs-rest (&#8216;ovr&#8217;) ecision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (&#8216;ovo&#8217;) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2).
The default of None will currently behave as &#8216;ovo&#8217; for backward
compatibility and raise a deprecation warning, but will change &#8216;ovr&#8217;
in 0.18.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>decision_function_shape=&#8217;ovr&#8217;</em> is recommended.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=&#8217;ovo&#8217; and None</em>.</p>
</div>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data for probability estimation.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span><dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span><dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">n_support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span><dd>Number of support vectors for each class.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span><dd>Coefficients of the support vector in the decision function.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the section about multi-class classification in
the SVM section of the User Guide for details.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">NuSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NuSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">NuSVC(cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">      decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="go">      max_iter=-1, nu=0.5, probability=False, random_state=None,</span>
<span class="go">      shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVC</dt>
<dd>Support Vector Machine for classification using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable linear Support Vector Machine for classification using
liblinear.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>NuSVCClassifierSklearnNode</strong></li>
<li><strong>NuSVCClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-nusvrregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.NuSVRRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.NuSVRRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NuSVRRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-nusvrregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.NuSVRRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">NuSVRRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.NuSVRRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Nu Support Vector Regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.NuSVR.html">sklearn.svm.classes.NuSVR</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Similar to NuSVC, for regression, uses a parameter nu to control
the number of support vectors. However, unlike NuSVC, where nu
replaces C, here nu replaces the parameter epsilon of epsilon-SVR.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Penalty parameter C of the error term.</dd>
<dt>nu</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>An upper bound on the fraction of training errors and a lower bound of
the fraction of support vectors. Should be in the interval (0, 1].  By
default 0.5 will be taken.</dd>
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span><dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span><dd>Degree of the polynomial kernel function (&#8216;poly&#8217;).
Ignored by all other kernels.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=&#8217;auto&#8217;)</span><dd>Kernel coefficient for &#8216;rbf&#8217;, &#8216;poly&#8217; and &#8216;sigmoid&#8217;.
If gamma is &#8216;auto&#8217; then 1/n_features will be used instead.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span><dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>shrinking</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to use the shrinking heuristic.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span><dd>Tolerance for stopping criterion.</dd>
<dt>cache_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Specify the size of the kernel cache (in MB).</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span><dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span><dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [nSV, n_features]</span><dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_SV]</span><dd>Coefficients of the support vector in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">NuSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NuSVR</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
<span class="go">NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=&#39;auto&#39;,</span>
<span class="go">      kernel=&#39;rbf&#39;, max_iter=-1, nu=0.1, shrinking=True, tol=0.001,</span>
<span class="go">      verbose=False)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>NuSVC</dt>
<dd>Support Vector Machine for classification implemented with libsvm
with a parameter to control the number of support vectors.</dd>
<dt>SVR</dt>
<dd>epsilon Support Vector Machine for regression implemented with libsvm.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>NuSVRRegressorSklearn</strong></li>
<li><strong>NuSVRRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-nystroemtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.NystroemTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.NystroemTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.NystroemTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-nystroemtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.NystroemTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">NystroemTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.NystroemTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Approximate a kernel map using a subset of the training data.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html">sklearn.kernel_approximation.Nystroem</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Constructs an approximate feature map for an arbitrary kernel
using a subset of the data as basis.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or callable, default=&#8221;rbf&#8221;</span><dd>Kernel map to be approximated. A callable should accept two arguments
and the keyword arguments passed to this object as kernel_params, and
should return a floating point number.</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of features to construct.
How many data points will be used to construct the mapping.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=None</span><dd>Gamma parameter for the RBF, polynomial, exponential chi2 and
sigmoid kernels. Interpretation of the default value is left to
the kernel; see the documentation for sklearn.metrics.pairwise.
Ignored by other kernels.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=3</span><dd>Degree of the polynomial kernel. Ignored by other kernels.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, default=1</span><dd>Zero coefficient for polynomial and sigmoid kernels.
Ignored by other kernels.</dd>
<dt>kernel_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">mapping of string to any, optional</span><dd>Additional parameters (keyword arguments) for kernel function passed
as callable object.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{int, RandomState}, optional</span><dd>If int, random_state is the seed used by the random number generator;
if RandomState instance, random_state is the random number generator.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_features)</span><dd>Subset of training points used to construct the feature map.</dd>
<dt><code class="docutils literal"><span class="pre">component_indices_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components)</span><dd>Indices of <code class="docutils literal"><span class="pre">components_</span></code> in the training set.</dd>
<dt><code class="docutils literal"><span class="pre">normalization_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_components, n_components)</span><dd>Normalization matrix needed for embedding.
Square root of the kernel matrix on <code class="docutils literal"><span class="pre">components_</span></code>.</dd>
</dl>
<p><strong>References</strong></p>
<ul class="simple">
<li>Williams, C.K.I. and Seeger, M.
&#8220;Using the Nystroem method to speed up kernel machines&#8221;,
Advances in neural information processing systems 2001</li>
<li>T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou
&#8220;Nystroem Method vs Random Fourier Features: A Theoretical and Empirical
Comparison&#8221;,
Advances in Neural Information Processing Systems 2012</li>
</ul>
<p>See also</p>
<dl class="docutils">
<dt>RBFSampler</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">An approximation to the RBF kernel using random Fourier</span><dd>features.</dd>
</dl>
<p>sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>NystroemTransformerSklearn</strong></li>
<li><strong>NystroemTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-onehotencodertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.OneHotEncoderTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.OneHotEncoderTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OneHotEncoderTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-onehotencodertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.OneHotEncoderTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">OneHotEncoderTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.OneHotEncoderTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Encode categorical integer features using a one-hot aka one-of-K scheme.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.OneHotEncoder.html">sklearn.preprocessing.data.OneHotEncoder</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The input to this transformer should be a matrix of integers, denoting
the values taken on by categorical (discrete) features. The output will be
a sparse matrix where each column corresponds to one possible value of one
feature. It is assumed that input features take on values in the range
[0, n_values).</p>
<p>This encoding is needed for feeding categorical data to many scikit-learn
estimators, notably linear models and SVMs with the standard kernels.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_values</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;auto&#8217;, int or array of ints</span><dd><p class="first">Number of values per feature.</p>
<ul class="last simple">
<li>&#8216;auto&#8217; : determine value range from training data.</li>
<li>int : maximum value for all features.</li>
<li>array : maximum value per feature.</li>
</ul>
</dd>
<dt>categorical_features: &#8220;all&#8221; or array of indices or mask</dt>
<dd><p class="first">Specify what features are treated as categorical.</p>
<ul class="simple">
<li>&#8216;all&#8217; (default): All features are treated as categorical.</li>
<li>array of indices: Array of categorical feature indices.</li>
<li>mask: Array of length n_features and with dtype=bool.</li>
</ul>
<p class="last">Non-categorical features are always stacked to the right of the matrix.</p>
</dd>
<dt>dtype</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">number type, default=np.float</span><dd>Desired dtype of output.</dd>
<dt>sparse</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Will return sparse matrix if set True else will return an array.</dd>
<dt>handle_unknown</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;error&#8217; or &#8216;ignore&#8217;</span><dd>Whether to raise an error or ignore if a unknown categorical feature is
present during transform.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">active_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>Indices for active features, meaning values that actually occur
in the training set. Only available when n_values is <code class="docutils literal"><span class="pre">'auto'</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">feature_indices_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span><dd>Indices to feature ranges.
Feature <code class="docutils literal"><span class="pre">i</span></code> in the original data is mapped to features
from <code class="docutils literal"><span class="pre">feature_indices_[i]</span></code> to <code class="docutils literal"><span class="pre">feature_indices_[i+1]</span></code>
(and then potentially masked by <cite>active_features_</cite> afterwards)</dd>
<dt><code class="docutils literal"><span class="pre">n_values_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_features,)</span><dd>Maximum number of values per feature.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>Given a dataset with three features and two samples, we let the encoder
find the maximum value per feature and transform the data to a binary
one-hot encoding.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">OneHotEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  
<span class="go">OneHotEncoder(categorical_features=&#39;all&#39;, dtype=&lt;... &#39;float&#39;&gt;,</span>
<span class="go">       handle_unknown=&#39;error&#39;, n_values=&#39;auto&#39;, sparse=True)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">n_values_</span>
<span class="go">array([2, 3, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">feature_indices_</span>
<span class="go">array([0, 2, 5, 9])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">enc</span><span class="o">.</span><span class="n">transform</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.]])</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.feature_extraction.DictVectorizer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">performs a one-hot encoding of</span><dd>dictionary items (also handles string-valued features).</dd>
<dt>sklearn.feature_extraction.FeatureHasher</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">performs an approximate one-hot</span><dd>encoding of dictionary items or strings.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OneHotEncoderTransformerSklearn</strong></li>
<li><strong>OneHotEncoderTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-onevsoneclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.OneVsOneClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.OneVsOneClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OneVsOneClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-onevsoneclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.OneVsOneClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">OneVsOneClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.OneVsOneClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>One-vs-one multiclass strategy</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html">sklearn.multiclass.OneVsOneClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This strategy consists in fitting one classifier per class pair.
At prediction time, the class which received the most votes is selected.
Since it requires to fit <cite>n_classes * (n_classes - 1) / 2</cite> classifiers,
this method is usually slower than one-vs-the-rest, due to its
O(n_classes^2) complexity. However, this method may be advantageous for
algorithms such as kernel algorithms which don&#8217;t scale well with
<cite>n_samples</cite>. This is because each individual learning problem only involves
a small subset of the data whereas, with one-vs-the-rest, the complete
dataset is used <cite>n_classes</cite> times.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator object</span><dd>An estimator object implementing <cite>fit</cite> and one of <cite>decision_function</cite>
or <cite>predict_proba</cite>.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 1</span><dd>The number of jobs to use for the computation. If -1 all CPUs are used.
If 1 is given, no parallel computing code is used at all, which is
useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
used. Thus for n_jobs = -2, all CPUs but one are used.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of <cite>n_classes * (n_classes - 1) / 2</cite> estimators</span><dd>Estimators used for predictions.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_classes]</span><dd>Array containing labels.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OneVsOneClassifierSklearn</strong></li>
<li><strong>OneVsOneClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-onevsrestclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.OneVsRestClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.OneVsRestClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OneVsRestClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-onevsrestclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.OneVsRestClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">OneVsRestClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.OneVsRestClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>One-vs-the-rest (OvR) multiclass/multilabel strategy</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html">sklearn.multiclass.OneVsRestClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Also known as one-vs-all, this strategy consists in fitting one classifier
per class. For each classifier, the class is fitted against all the other
classes. In addition to its computational efficiency (only <cite>n_classes</cite>
classifiers are needed), one advantage of this approach is its
interpretability. Since each class is represented by one and one classifier
only, it is possible to gain knowledge about the class by inspecting its
corresponding classifier. This is the most commonly used strategy for
multiclass classification and is a fair default choice.</p>
<p>This strategy can also be used for multilabel learning, where a classifier
is used to predict multiple labels for instance, by fitting on a 2-d matrix
in which cell [i, j] is 1 if sample i has label j and 0 otherwise.</p>
<p>In the multilabel learning literature, OvR is also known as the binary
relevance method.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator object</span><dd>An estimator object implementing <cite>fit</cite> and one of <cite>decision_function</cite>
or <cite>predict_proba</cite>.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 1</span><dd>The number of jobs to use for the computation. If -1 all CPUs are used.
If 1 is given, no parallel computing code is used at all, which is
useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
used. Thus for n_jobs = -2, all CPUs but one are used.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of <cite>n_classes</cite> estimators</span><dd>Estimators used for predictions.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [<cite>n_classes</cite>]</span><dd>Class labels.</dd>
<dt><code class="docutils literal"><span class="pre">label_binarizer_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">LabelBinarizer object</span><dd>Object used to transform multiclass labels to binary labels and
vice-versa.</dd>
<dt><code class="docutils literal"><span class="pre">multilabel_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether a OneVsRestClassifier is a multilabel classifier.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OneVsRestClassifierSklearn</strong></li>
<li><strong>OneVsRestClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-orthogonalmatchingpursuitcvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-orthogonalmatchingpursuitcvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">OrthogonalMatchingPursuitCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Cross-validated Orthogonal Matching Pursuit model (OMP)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.html">sklearn.linear_model.omp.OrthogonalMatchingPursuitCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>Whether the design matrix X must be copied by the algorithm. A false
value is only helpful if X is already Fortran-ordered, otherwise a
copy is made anyway.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>If False, the regressors X are assumed to be already normalized.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Maximum numbers of iterations to perform, therefore maximum features
to include. 10% of <code class="docutils literal"><span class="pre">n_features</span></code> but at least 5 if available.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
</dl>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float or array, shape (n_targets,)</span><dd>Independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_features, n_targets)</span><dd>Parameter vector (w in the problem formulation).</dd>
<dt><code class="docutils literal"><span class="pre">n_nonzero_coefs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Estimated number of non-zero coefficients giving the best mean squared
error over the cross-validation folds.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or array-like</span><dd>Number of active features across every target for the model refit with
the best hyperparameters got by cross-validating across all folds.</dd>
</dl>
<p>See also</p>
<p>orthogonal_mp
orthogonal_mp_gram
lars_path
Lars
LassoLars
OrthogonalMatchingPursuit
LarsCV
LassoLarsCV
decomposition.sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OrthogonalMatchingPursuitCVRegressorSklearnNode</strong></li>
<li><strong>OrthogonalMatchingPursuitCVRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-orthogonalmatchingpursuitregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-orthogonalmatchingpursuitregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">OrthogonalMatchingPursuitRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.OrthogonalMatchingPursuitRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Orthogonal Matching Pursuit model (OMP)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.omp.OrthogonalMatchingPursuit.html">sklearn.linear_model.omp.OrthogonalMatchingPursuit</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_nonzero_coefs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Desired number of non-zero entries in the solution. If None (by
default) this value is set to 10% of n_features.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>If False, the regressors X are assumed to be already normalized.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{True, False, &#8216;auto&#8217;}, default &#8216;auto&#8217;</span><dd>Whether to use a precomputed Gram and Xy matrix to speed up
calculations. Improves performance when <cite>n_targets</cite> or <cite>n_samples</cite> is
very large. Note that if you already have such matrices, you can pass
them directly to the fit method.</dd>
</dl>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_features, n_targets)</span><dd>parameter vector (w in the formula)</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float or array, shape (n_targets,)</span><dd>independent term in decision function.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or array-like</span><dd>Number of active features across every target.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(<a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf</a>)</p>
<p>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
<a class="reference external" href="http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></p>
<p>See also</p>
<p>orthogonal_mp
orthogonal_mp_gram
lars_path
Lars
LassoLars
decomposition.sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OrthogonalMatchingPursuitRegressorSklearnNode</strong></li>
<li><strong>OrthogonalMatchingPursuitRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-outputcodeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.OutputCodeClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.OutputCodeClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.OutputCodeClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-outputcodeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.OutputCodeClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">OutputCodeClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.OutputCodeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>(Error-Correcting) Output-Code multiclass strategy</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OutputCodeClassifier.html">sklearn.multiclass.OutputCodeClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Output-code based strategies consist in representing each class with a
binary code (an array of 0s and 1s). At fitting time, one binary
classifier per bit in the code book is fitted.  At prediction time, the
classifiers are used to project new points in the class space and the class
closest to the points is chosen. The main advantage of these strategies is
that the number of classifiers used can be controlled by the user, either
for compressing the model (0 &lt; code_size &lt; 1) or for making the model more
robust to errors (code_size &gt; 1). See the documentation for more details.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator object</span><dd>An estimator object implementing <cite>fit</cite> and one of <cite>decision_function</cite>
or <cite>predict_proba</cite>.</dd>
<dt>code_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Percentage of the number of classes to be used to create the code book.
A number between 0 and 1 will require fewer classifiers than
one-vs-the-rest. A number greater than 1 will require more classifiers
than one-vs-the-rest.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy.RandomState, optional</span><dd>The generator used to initialize the codebook. Defaults to
numpy.random.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 1</span><dd>The number of jobs to use for the computation. If -1 all CPUs are used.
If 1 is given, no parallel computing code is used at all, which is
useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are
used. Thus for n_jobs = -2, all CPUs but one are used.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of <cite>int(n_classes * code_size)</cite> estimators</span><dd>Estimators used for predictions.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_classes]</span><dd>Array containing labels.</dd>
<dt><code class="docutils literal"><span class="pre">code_book_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_classes, code_size]</span><dd>Binary array containing the code of each class.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id47" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>&#8220;Solving multiclass learning problems via error-correcting output
codes&#8221;,
Dietterich T., Bakiri G.,
Journal of Artificial Intelligence Research 2,
1995.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id48" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>&#8220;The error coding method and PICTs&#8221;,
James G., Hastie T.,
Journal of Computational and Graphical statistics 7,
1998.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id49" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>&#8220;The Elements of Statistical Learning&#8221;,
Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)
2008.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>OutputCodeClassifierSklearnNode</strong></li>
<li><strong>OutputCodeClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-pcatransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.PCATransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.PCATransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PCATransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-pcatransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.PCATransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">PCATransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.PCATransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Principal component analysis (PCA)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.PCA.html">sklearn.decomposition.pca.PCA</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data and keeping only the most significant singular vectors to project the
data to a lower dimensional space.</p>
<p>This implementation uses the scipy.linalg implementation of the singular
value decomposition. It only works for dense arrays and is not scalable to
large dimensional data.</p>
<p>The time complexity of this implementation is <code class="docutils literal"><span class="pre">O(n</span> <span class="pre">**</span> <span class="pre">3)</span></code> assuming
n ~ n_samples ~ n_features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, None or string</span><dd><p class="first">Number of components to keep.
if n_components is not set all components are kept:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">n_components</span> <span class="o">==</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">if n_components == &#8216;mle&#8217;, Minka&#8217;s MLE is used to guess the dimension
if <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, select the number of components such that
the amount of variance that needs to be explained is greater than the
percentage specified by n_components</p>
</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt>whiten</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by n_samples times singular values to ensure uncorrelated outputs
with unit component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making there data respect some hard-wired assumptions.</p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Principal axes in feature space, representing the directions of
maximum variance in the data.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span><dd>Percentage of variance explained by each of the selected components.
If <code class="docutils literal"><span class="pre">n_components</span></code> is not set then all components are stored and the
sum of explained variances is equal to 1.0</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_features]</span><dd>Per-feature empirical mean, estimated from the training set.</dd>
<dt><code class="docutils literal"><span class="pre">n_components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The estimated number of components. Relevant when n_components is set
to &#8216;mle&#8217; or a number between 0 and 1 to select using explained
variance.</dd>
<dt><code class="docutils literal"><span class="pre">noise_variance_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See &#8220;Pattern Recognition and
Machine Learning&#8221; by C. Bishop, 12.2.1 p. 574 or
<a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to
computed the estimated data covariance and score samples.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>For n_components=&#8217;mle&#8217;, this class uses the method of <a href="#id50"><span class="problematic" id="id51">`</span></a>Thomas P. Minka:</p>
<p>Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`</p>
<p>Implements the probabilistic PCA model from:</p>
<p>M. Tipping and C. Bishop, Probabilistic Principal Component Analysis,
Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622
via the score and score_samples methods.
See <a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a></p>
<p>Due to implementation subtleties of the Singular Value Decomposition (SVD),
which is used in this implementation, running fit twice on the same matrix
can lead to principal components with signs flipped (change in direction).
For this reason, it is important to always use the same estimator object to
transform data in a consistent fashion.</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">PCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">PCA(copy=True, n_components=2, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>RandomizedPCA
KernelPCA
SparsePCA
TruncatedSVD</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>PCATransformerSklearn</strong></li>
<li><strong>PCATransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-passiveaggressiveclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-passiveaggressiveclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">PassiveAggressiveClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Passive Aggressive Classifier</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.html">sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Maximum step size (regularization). Defaults to 1.0.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=False</span><dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=True</span><dd>Whether or not the training data should be shuffled after each epoch.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The verbosity level</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd><p class="first">The loss function to be used:</p>
<ul class="last simple">
<li>hinge: equivalent to PA-I in the reference paper.</li>
<li>squared_hinge: equivalent to PA-II in the reference paper.</li>
</ul>
</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label: weight} or &#8220;balanced&#8221; or None, optional</span><dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p>The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>parameter <em>class_weight</em> to automatically weight samples.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</span><dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span><dd>Constants in decision function.</dd>
</dl>
<p>See also</p>
<p>SGDClassifier
Perceptron</p>
<p><strong>References</strong></p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>PassiveAggressiveClassifierSklearn</strong></li>
<li><strong>PassiveAggressiveClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-passiveaggressiveregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-passiveaggressiveregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">PassiveAggressiveRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.PassiveAggressiveRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Passive Aggressive Regressor</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.html">sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Maximum step size (regularization). Defaults to 1.0.</dd>
<dt>epsilon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>If the difference between the current prediction and the correct label
is below this threshold, the model is not updated.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default=True</span><dd>Whether or not the training data should be shuffled after each epoch.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The verbosity level</dd>
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd><p class="first">The loss function to be used:</p>
<ul class="last simple">
<li>epsilon_insensitive: equivalent to PA-I in the reference paper.</li>
<li>squared_epsilon_insensitive: equivalent to PA-II in the reference</li>
<li>paper.</li>
</ul>
</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</span><dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span><dd>Constants in decision function.</dd>
</dl>
<p>See also</p>
<p>SGDRegressor</p>
<p><strong>References</strong></p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>PassiveAggressiveRegressorSklearnNode</strong></li>
<li><strong>PassiveAggressiveRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-patchextractortransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.PatchExtractorTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.PatchExtractorTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PatchExtractorTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-patchextractortransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.PatchExtractorTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">PatchExtractorTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.PatchExtractorTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Extracts patches from a collection of images</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.PatchExtractor.html">sklearn.feature_extraction.image.PatchExtractor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>patch_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tuple of ints (patch_height, patch_width)</span><dd>the dimensions of one patch</dd>
<dt>max_patches</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or float, optional default is None</span><dd>The maximum number of patches per image to extract. If max_patches is a
float in (0, 1), it is taken to mean a proportion of the total number
of patches.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>PatchExtractorTransformerSklearn</strong></li>
<li><strong>PatchExtractorTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-perceptronclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.PerceptronClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.PerceptronClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PerceptronClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-perceptronclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.PerceptronClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">PerceptronClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.PerceptronClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Perceptron</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.perceptron.Perceptron.html">sklearn.linear_model.perceptron.Perceptron</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>penalty</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">None, &#8216;l2&#8217; or &#8216;l1&#8217; or &#8216;elasticnet&#8217;</span><dd>The penalty (aka regularization term) to be used. Defaults to None.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Constant that multiplies the regularization term if regularization is
used. Defaults to 0.0001</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The number of passes over the training data (aka epochs).
Defaults to 5.</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional, default True</span><dd>Whether or not the training data should be shuffled after each epoch.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The verbosity level</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>eta0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double</span><dd>Constant by which the updates are multiplied. Defaults to 1.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label: weight} or &#8220;balanced&#8221; or None, optional</span><dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</span><dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1] if n_classes == 2 else [n_classes]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Notes</strong></p>
<p><cite>Perceptron</cite> and <cite>SGDClassifier</cite> share the same underlying implementation.
In fact, <cite>Perceptron()</cite> is equivalent to <cite>SGDClassifier(loss=&#8221;perceptron&#8221;,
eta0=1, learning_rate=&#8221;constant&#8221;, penalty=None)</cite>.</p>
<p>See also</p>
<p>SGDClassifier</p>
<p><strong>References</strong></p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Perceptron">http://en.wikipedia.org/wiki/Perceptron</a> and references therein.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>PerceptronClassifierSklearn</strong></li>
<li><strong>PerceptronClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-polynomialfeaturestransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.PolynomialFeaturesTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.PolynomialFeaturesTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.PolynomialFeaturesTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-polynomialfeaturestransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.PolynomialFeaturesTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">PolynomialFeaturesTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.PolynomialFeaturesTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Generate polynomial and interaction features.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.PolynomialFeatures.html">sklearn.preprocessing.data.PolynomialFeatures</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Generate a new feature matrix consisting of all polynomial combinations
of the features with degree less than or equal to the specified degree.
For example, if an input sample is two dimensional and of the form
[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>The degree of the polynomial features. Default = 2.</dd>
<dt>interaction_only</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default = False</span><dd>If true, only interaction features are produced: features that are
products of at most <code class="docutils literal"><span class="pre">degree</span></code> <em>distinct</em> input features (so not
<code class="docutils literal"><span class="pre">x[1]</span> <span class="pre">**</span> <span class="pre">2</span></code>, <code class="docutils literal"><span class="pre">x[0]</span> <span class="pre">*</span> <span class="pre">x[2]</span> <span class="pre">**</span> <span class="pre">3</span></code>, etc.).</dd>
<dt>include_bias</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>If True (default), then include a bias column, the feature in which
all polynomial powers are zero (i.e. a column of ones - acts as an
intercept term in a linear model).</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [2, 3],</span>
<span class="go">       [4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span>
<span class="go">       [  1.,   2.,   3.,   4.,   6.,   9.],</span>
<span class="go">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[  1.,   0.,   1.,   0.],</span>
<span class="go">       [  1.,   2.,   3.,   6.],</span>
<span class="go">       [  1.,   4.,   5.,  20.]])</span>
</pre></div>
</div>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">powers_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_input_features, n_output_features)</span><dd>powers_[i, j] is the exponent of the jth input in the ith output.</dd>
<dt><code class="docutils literal"><span class="pre">n_input_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The total number of input features.</dd>
<dt><code class="docutils literal"><span class="pre">n_output_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The total number of polynomial output features. The number of output
features is computed by iterating over all suitably sized combinations
of input features.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Be aware that the number of features in the output array scales
polynomially in the number of features of the input array, and
exponentially in the degree. High degrees can cause overfitting.</p>
<p>See <span class="xref std std-ref">examples/linear_model/plot_polynomial_interpolation.py</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>PolynomialFeaturesTransformerSklearnNode</strong></li>
<li><strong>PolynomialFeaturesTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-projectedgradientnmftransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.ProjectedGradientNMFTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.ProjectedGradientNMFTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.ProjectedGradientNMFTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-projectedgradientnmftransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.ProjectedGradientNMFTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">ProjectedGradientNMFTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.ProjectedGradientNMFTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Non-Negative Matrix Factorization (NMF)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.nmf.ProjectedGradientNMF.html">sklearn.decomposition.nmf.ProjectedGradientNMF</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Find two non-negative matrices (W, H) whose product approximates the non-
negative matrix X. This factorization can be used for example for
dimensionality reduction, source separation or topic extraction.</p>
<p>The objective function is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mf">0.5</span> <span class="o">*</span> <span class="o">||</span><span class="n">X</span> <span class="o">-</span> <span class="n">WH</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">H</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">A</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span> <span class="o">=</span> \<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">}</span> <span class="n">A_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span> <span class="p">(</span><span class="n">Frobenius</span> <span class="n">norm</span><span class="p">)</span>
<span class="o">||</span><span class="n">vec</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">||</span><span class="n">_1</span> <span class="o">=</span> \<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">}</span> <span class="nb">abs</span><span class="p">(</span><span class="n">A_</span><span class="p">{</span><span class="n">ij</span><span class="p">})</span> <span class="p">(</span><span class="n">Elementwise</span> <span class="n">L1</span> <span class="n">norm</span><span class="p">)</span>
</pre></div>
</div>
<p>The objective function is minimized with an alternating minimization of W
and H.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span><dd>Number of components, if n_components is not set all features
are kept.</dd>
<dt>init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;random&#8217; | &#8216;nndsvd&#8217; |  &#8216;nndsvda&#8217; | &#8216;nndsvdar&#8217; | &#8216;custom&#8217;</span><dd><p class="first">Method used to initialize the procedure.
Default: &#8216;nndsvdar&#8217; if n_components &lt; n_features, otherwise random.
Valid options:</p>
<ul class="last">
<li><p class="first">&#8216;random&#8217;: non-negative random matrices, scaled with:</p>
<blockquote>
<div><ul class="simple">
<li>sqrt(X.mean() / n_components)</li>
</ul>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>&#8216;nndsvd&#8217;: Nonnegative Double Singular Value Decomposition (NNDSVD)</dt>
<dd><p class="first last">initialization (better for sparseness)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>&#8216;nndsvda&#8217;: NNDSVD with zeros filled with the average of X</dt>
<dd><p class="first last">(better when sparsity is not desired)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>&#8216;nndsvdar&#8217;: NNDSVD with zeros filled with small random values</dt>
<dd><p class="first last">(generally faster, less accurate alternative to NNDSVDa
for when sparsity is not desired)</p>
</dd>
</dl>
</li>
<li><p class="first">&#8216;custom&#8217;: use custom matrices W and H</p>
</li>
</ul>
</dd>
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;pg&#8217; | &#8216;cd&#8217;</span><dd><p class="first">Numerical solver to use:</p>
<ul class="simple">
<li>&#8216;pg&#8217; is a Projected Gradient solver (deprecated).</li>
<li>&#8216;cd&#8217; is a Coordinate Descent solver (recommended).</li>
</ul>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Coordinate Descent solver.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver.</p>
</div>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1e-4</span><dd>Tolerance value used in stopping conditions.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, default: 200</span><dd>Number of iterations to compute.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer seed, RandomState instance, or None (default)</span><dd>Random number generator seed control.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.</span><dd><p class="first">Constant that multiplies the regularization terms. Set it to zero to
have no regularization.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>alpha</em> used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.</span><dd><p class="first">The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
For l1_ratio = 0 the penalty is an elementwise L2 penalty
(aka Frobenius Norm).
For l1_ratio = 1 it is an elementwise L1 penalty.
For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Regularization parameter <em>l1_ratio</em> used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default: False</span><dd><p class="first">If true, randomize the order of coordinates in the CD solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>shuffle</em> parameter used in the Coordinate Descent solver.</p>
</div>
</dd>
<dt>nls_max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, default: 2000</span><dd><p class="first">Number of iterations in NLS subproblem.
Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
<dt>sparseness</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;data&#8217; | &#8216;components&#8217; | None, default: None</span><dd><p class="first">Where to enforce sparsity in the model.
Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
<dt>beta</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 1</span><dd><p class="first">Degree of sparseness, if sparseness is not None. Larger values mean
more sparseness. Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
<dt>eta</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, default: 0.1</span><dd><p class="first">Degree of correctness to maintain, if sparsity is not None. Smaller
values mean larger error. Used only in the deprecated &#8216;pg&#8217; solver.</p>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated Projected Gradient solver. Use Coordinate Descent solver
instead.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Non-negative components of the data.</dd>
<dt><code class="docutils literal"><span class="pre">reconstruction_err_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">number</span><dd>Frobenius norm of the matrix difference between
the training data and the reconstructed data from
the fit produced by the model. <code class="docutils literal"><span class="pre">||</span> <span class="pre">X</span> <span class="pre">-</span> <span class="pre">WH</span> <span class="pre">||_2</span></code></dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Actual number of iterations.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">NMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">NMF(alpha=0.0, beta=1, eta=0.1, init=&#39;random&#39;, l1_ratio=0.0, max_iter=200,</span>
<span class="go">  n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,</span>
<span class="go">  solver=&#39;cd&#39;, sparseness=None, tol=0.0001, verbose=0)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="go">array([[ 2.09783018,  0.30560234],</span>
<span class="go">       [ 2.13443044,  2.13171694]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">reconstruction_err_</span> 
<span class="go">0.00115993...</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<p>C.-J. Lin. Projected gradient methods for non-negative matrix
factorization. Neural Computation, 19(2007), 2756-2779.
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/nmf/">http://www.csie.ntu.edu.tw/~cjlin/nmf/</a></p>
<p>Cichocki, Andrzej, and P. H. A. N. Anh-Huy. &#8220;Fast local algorithms for
large scale nonnegative matrix and tensor factorizations.&#8221;
IEICE transactions on fundamentals of electronics, communications and
computer sciences 92.3: 708-721, 2009.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>ProjectedGradientNMFTransformerSklearnNode</strong></li>
<li><strong>ProjectedGradientNMFTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-quadraticdiscriminantanalysisclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.QuadraticDiscriminantAnalysisClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.QuadraticDiscriminantAnalysisClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.QuadraticDiscriminantAnalysisClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-quadraticdiscriminantanalysisclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.QuadraticDiscriminantAnalysisClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">QuadraticDiscriminantAnalysisClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.QuadraticDiscriminantAnalysisClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Quadratic Discriminant Analysis</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html">sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A classifier with a quadratic decision boundary, generated
by fitting class conditional densities to the data
and using Bayes&#8217; rule.</p>
<p>The model fits a Gaussian density to each class.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>QuadraticDiscriminantAnalysis</em></p>
</div>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <code class="xref py py-class docutils literal"><span class="pre">qda.QDA</span></code> have been moved to <em>QuadraticDiscriminantAnalysis</em>.</p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>priors</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, optional, shape = [n_classes]</span><dd>Priors on classes</dd>
<dt>reg_param</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Regularizes the covariance estimate as
<code class="docutils literal"><span class="pre">(1-reg_param)*Sigma</span> <span class="pre">+</span> <span class="pre">reg_param*np.eye(n_features)</span></code></dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">covariances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of array-like, shape = [n_features, n_features]</span><dd>Covariance matrices of each class.</dd>
<dt><code class="docutils literal"><span class="pre">means_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes, n_features]</span><dd>Class means.</dd>
<dt><code class="docutils literal"><span class="pre">priors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classes]</span><dd>Class priors (sum to 1).</dd>
<dt><code class="docutils literal"><span class="pre">rotations_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span><dd>For each class k an array of shape [n_features, n_k], with
<code class="docutils literal"><span class="pre">n_k</span> <span class="pre">=</span> <span class="pre">min(n_features,</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">elements</span> <span class="pre">in</span> <span class="pre">class</span> <span class="pre">k)</span></code>
It is the rotation of the Gaussian distribution, i.e. its
principal axis.</dd>
<dt><code class="docutils literal"><span class="pre">scalings_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of arrays</span><dd>For each class k an array of shape [n_k]. It contains the scaling
of the Gaussian distributions along its principal axes, i.e. the
variance in the rotated coordinate system.</dd>
<dt>store_covariances</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd><p class="first">If True the covariance matrices are computed and stored in the
<cite>self.covariances_</cite> attribute.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1.0e-4</span><dd><p class="first">Threshold used for rank estimation.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="k">import</span> <span class="n">QuadraticDiscriminantAnalysis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">QuadraticDiscriminantAnalysis</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,</span>
<span class="go">                              store_covariances=False, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>sklearn.discriminant_analysis.LinearDiscriminantAnalysis: Linear</dt>
<dd>Discriminant Analysis</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>QuadraticDiscriminantAnalysisClassifierSklearn</strong></li>
<li><strong>QuadraticDiscriminantAnalysisClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-ransacregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RANSACRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RANSACRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RANSACRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-ransacregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RANSACRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RANSACRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RANSACRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>RANSAC (RANdom SAmple Consensus) algorithm.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ransac.RANSACRegressor.html">sklearn.linear_model.ransac.RANSACRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>RANSAC is an iterative algorithm for the robust estimation of parameters
from a subset of inliers from the complete data set. More information can
be found in the general documentation of linear models.</p>
<p>A detailed description of the algorithm can be found in the documentation
of the <code class="docutils literal"><span class="pre">linear_model</span></code> sub-package.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>base_estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object, optional</span><dd><p class="first">Base estimator object which implements the following methods:</p>
<blockquote>
<div><ul class="simple">
<li><cite>fit(X, y)</cite>: Fit model to given training data and target values.</li>
<li><cite>score(X, y)</cite>: Returns the mean accuracy on the given test data,
which is used for the stop criterion defined by <cite>stop_score</cite>.
Additionally, the score is used to decide which of two equally
large consensus sets is chosen as the better one.</li>
</ul>
</div></blockquote>
<p>If <cite>base_estimator</cite> is None, then
<code class="docutils literal"><span class="pre">base_estimator=sklearn.linear_model.LinearRegression()</span></code> is used for
target values of dtype float.</p>
<p class="last">Note that the current implementation only supports regression
estimators.</p>
</dd>
<dt>min_samples</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int (&gt;= 1) or float ([0, 1]), optional</span><dd>Minimum number of samples chosen randomly from original data. Treated
as an absolute number of samples for <cite>min_samples &gt;= 1</cite>, treated as a
relative number <cite>ceil(min_samples * X.shape[0]</cite>) for
<cite>min_samples &lt; 1</cite>. This is typically chosen as the minimal number of
samples necessary to estimate the given <cite>base_estimator</cite>. By default a
<code class="docutils literal"><span class="pre">sklearn.linear_model.LinearRegression()</span></code> estimator is assumed and
<cite>min_samples</cite> is chosen as <code class="docutils literal"><span class="pre">X.shape[1]</span> <span class="pre">+</span> <span class="pre">1</span></code>.</dd>
<dt>residual_threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Maximum residual for a data sample to be classified as an inlier.
By default the threshold is chosen as the MAD (median absolute
deviation) of the target values <cite>y</cite>.</dd>
<dt>is_data_valid</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span><dd>This function is called with the randomly selected data before the
model is fitted to it: <cite>is_data_valid(X, y)</cite>. If its return value is
False the current randomly chosen sub-sample is skipped.</dd>
<dt>is_model_valid</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span><dd>This function is called with the estimated model and the randomly
selected data: <cite>is_model_valid(model, X, y)</cite>. If its return value is
False the current randomly chosen sub-sample is skipped.
Rejecting samples with this function is computationally costlier than
with <cite>is_data_valid</cite>. <cite>is_model_valid</cite> should therefore only be used if
the estimated model is needed for making the rejection decision.</dd>
<dt>max_trials</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of iterations for random sample selection.</dd>
<dt>stop_n_inliers</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Stop iteration if at least this number of inliers are found.</dd>
<dt>stop_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Stop iteration if score is greater equal than this threshold.</dd>
<dt>stop_probability</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0, 1], optional</span><dd><p class="first">RANSAC iteration stops if at least one outlier-free set of the training
data is sampled in RANSAC. This requires to generate at least N
samples (iterations):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">&gt;=</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probability</span><span class="p">)</span> <span class="o">/</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">e</span><span class="o">**</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">where the probability (confidence) is typically set to high value such
as 0.99 (the default) and e is the current fraction of inliers w.r.t.
the total number of samples.</p>
</dd>
<dt>residual_metric</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable, optional</span><dd><p class="first">Metric to reduce the dimensionality of the residuals to 1 for
multi-dimensional target values <code class="docutils literal"><span class="pre">y.shape[1]</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>. By default the sum
of absolute differences is used:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="k">lambda</span> <span class="n">dy</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">dy</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or numpy.RandomState, optional</span><dd>The generator used to initialize the centers. If an integer is
given, it fixes the seed. Defaults to the global numpy random
number generator.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimator_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd>Best fitted model (copy of the <cite>base_estimator</cite> object).</dd>
<dt><code class="docutils literal"><span class="pre">n_trials_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of random selection trials until one of the stop criteria is
met. It is always <code class="docutils literal"><span class="pre">&lt;=</span> <span class="pre">max_trials</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">inlier_mask_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool array of shape [n_samples]</span><dd>Boolean mask of inliers classified as <code class="docutils literal"><span class="pre">True</span></code>.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id52" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="http://en.wikipedia.org/wiki/RANSAC">http://en.wikipedia.org/wiki/RANSAC</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id53" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf">http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id54" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RANSACRegressorSklearn</strong></li>
<li><strong>RANSACRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-rbfsamplertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RBFSamplerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RBFSamplerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RBFSamplerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-rbfsamplertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RBFSamplerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RBFSamplerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RBFSamplerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Approximates feature map of an RBF kernel by Monte Carlo approximation
of its Fourier transform.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html">sklearn.kernel_approximation.RBFSampler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>It implements a variant of Random Kitchen Sinks.[1]</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Parameter of RBF kernel: exp(-gamma * x^2)</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of Monte Carlo samples per original feature.
Equals the dimensionality of the computed feature space.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{int, RandomState}, optional</span><dd>If int, random_state is the seed used by the random number generator;
if RandomState instance, random_state is the random number generator.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>See &#8220;Random Features for Large-Scale Kernel Machines&#8221; by A. Rahimi and
Benjamin Recht.</p>
<p>[1] &#8220;Weighted Sums of Random Kitchen Sinks: Replacing
minimization with randomization in learning&#8221; by A. Rahimi and
Benjamin Recht.
(<a class="reference external" href="http://www.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf">http://www.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf</a>)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RBFSamplerTransformerSklearnNode</strong></li>
<li><strong>RBFSamplerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-rfecvtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RFECVTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RFECVTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RFECVTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-rfecvtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RFECVTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RFECVTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RFECVTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Feature ranking with recursive feature elimination and cross-validated
selection of the best number of features.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.rfe.RFECV.html">sklearn.feature_selection.rfe.RFECV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd><p class="first">A supervised learning estimator with a <cite>fit</cite> method that updates a
<cite>coef_</cite> attribute that holds the fitted parameters. Important features
must correspond to high absolute values in the <cite>coef_</cite> array.</p>
<p class="last">For instance, this is the case for most supervised learning
algorithms such as Support Vector Classifiers and Generalized
Linear Models from the <cite>svm</cite> and <cite>linear_model</cite> modules.</p>
</dd>
<dt>step</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span><dd>If greater than or equal to 1, then <cite>step</cite> corresponds to the (integer)
number of features to remove at each iteration.
If within (0.0, 1.0), then <cite>step</cite> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> used. If the estimator is a classifier
or if <code class="docutils literal"><span class="pre">y</span></code> is neither binary nor multiclass, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>scoring</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span><dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</dd>
<dt>estimator_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>Parameters for the external estimator.
This attribute is deprecated as of version 0.16 and will be removed in
0.18. Use estimator initialisation or set_params method instead.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default=0</span><dd>Controls verbosity of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of selected features with cross-validation.</dd>
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span><dd>The mask of selected features.</dd>
<dt><code class="docutils literal"><span class="pre">ranking_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span><dd>The feature ranking, such that <cite>ranking_[i]</cite>
corresponds to the ranking
position of the i-th feature.
Selected (i.e., estimated best)
features are assigned rank 1.</dd>
<dt><code class="docutils literal"><span class="pre">grid_scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_subsets_of_features]</span><dd>The cross-validation scores such that
<code class="docutils literal"><span class="pre">grid_scores_[i]</span></code> corresponds to
the CV score of the i-th subset of features.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The size of <code class="docutils literal"><span class="pre">grid_scores_</span></code> is equal to ceil((n_features - 1) / step) + 1,
where step is the number of features removed at each iteration.</p>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the a-priori not known 5
informative features in the Friedman #1 dataset.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">RFECV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFECV</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True,</span>
<span class="go">        False, False, False, False, False], dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id55" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &#8220;Gene selection
for cancer classification using support vector machines&#8221;,
Mach. Learn., 46(1-3), 389&#8211;422, 2002.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RFECVTransformerSklearn</strong></li>
<li><strong>RFECVTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-rfetransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RFETransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RFETransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RFETransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-rfetransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RFETransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RFETransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RFETransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Feature ranking with recursive feature elimination.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.rfe.RFE.html">sklearn.feature_selection.rfe.RFE</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination
(RFE) is to select features by recursively considering smaller and smaller
sets of features. First, the estimator is trained on the initial set of
features and weights are assigned to each one of them. Then, features whose
absolute weights are the smallest are pruned from the current set features.
That procedure is recursively repeated on the pruned set until the desired
number of features to select is eventually reached.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd><p class="first">A supervised learning estimator with a <cite>fit</cite> method that updates a
<cite>coef_</cite> attribute that holds the fitted parameters. Important features
must correspond to high absolute values in the <cite>coef_</cite> array.</p>
<p class="last">For instance, this is the case for most supervised learning
algorithms such as Support Vector Classifiers and Generalized
Linear Models from the <cite>svm</cite> and <cite>linear_model</cite> modules.</p>
</dd>
<dt>n_features_to_select</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None (default=None)</span><dd>The number of features to select. If <cite>None</cite>, half of the features
are selected.</dd>
<dt>step</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or float, optional (default=1)</span><dd>If greater than or equal to 1, then <cite>step</cite> corresponds to the (integer)
number of features to remove at each iteration.
If within (0.0, 1.0), then <cite>step</cite> corresponds to the percentage
(rounded down) of features to remove at each iteration.</dd>
<dt>estimator_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>Parameters for the external estimator.
This attribute is deprecated as of version 0.16 and will be removed in
0.18. Use estimator initialisation or set_params method instead.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default=0</span><dd>Controls verbosity of output.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of selected features.</dd>
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span><dd>The mask of selected features.</dd>
<dt><code class="docutils literal"><span class="pre">ranking_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape [n_features]</span><dd>The feature ranking, such that <code class="docutils literal"><span class="pre">ranking_[i]</span></code> corresponds to the
ranking position of the i-th feature. Selected (i.e., estimated
best) features are assigned rank 1.</dd>
<dt><code class="docutils literal"><span class="pre">estimator_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd>The external estimator fit on the reduced dataset.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following example shows how to retrieve the 5 right informative
features in the Friedman #1 dataset.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="k">import</span> <span class="n">RFE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([ True,  True,  True,  True,  True,</span>
<span class="go">        False, False, False, False, False], dtype=bool)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">ranking_</span>
<span class="go">array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])</span>
</pre></div>
</div>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id56" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V., &#8220;Gene selection
for cancer classification using support vector machines&#8221;,
Mach. Learn., 46(1-3), 389&#8211;422, 2002.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RFETransformerSklearnNode</strong></li>
<li><strong>RFETransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-radiusneighborsclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-radiusneighborsclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RadiusNeighborsClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Classifier implementing a vote among neighbors within a given radius</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.classification.RadiusNeighborsClassifier.html">sklearn.neighbors.classification.RadiusNeighborsClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>radius</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = 1.0)</span><dd>Range of parameter space to use by default for :meth`radius_neighbors`
queries.</dd>
<dt>weights</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span><dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span><dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>&#8216;kd_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">KDtree</span></code></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span><dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>metric</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or DistanceMetric object (default=&#8217;minkowski&#8217;)</span><dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>p</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span><dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>outlier_label</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = None)</span><dd>Label, which is given for outlier samples (samples with no
neighbors on given radius).
If set to None, ValueError is raised, when outlier is detected.</dd>
<dt>metric_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span><dd>Additional keyword arguments for the metric function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">RadiusNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsClassifier</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">RadiusNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0]</span>
</pre></div>
</div>
<p>See also</p>
<p>KNeighborsClassifier
RadiusNeighborsRegressor
KNeighborsRegressor
NearestNeighbors</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RadiusNeighborsClassifierSklearnNode</strong></li>
<li><strong>RadiusNeighborsClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-radiusneighborsregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-radiusneighborsregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RadiusNeighborsRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RadiusNeighborsRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Regression based on neighbors within a fixed radius.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.regression.RadiusNeighborsRegressor.html">sklearn.neighbors.regression.RadiusNeighborsRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>radius</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default = 1.0)</span><dd>Range of parameter space to use by default for :meth`radius_neighbors`
queries.</dd>
<dt>weights</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str or callable</span><dd><p class="first">weight function used in prediction.  Possible values:</p>
<ul class="simple">
<li>&#8216;uniform&#8217; : uniform weights.  All points in each neighborhood
are weighted equally.</li>
<li>&#8216;distance&#8217; : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</li>
<li>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</li>
</ul>
<p class="last">Uniform weights are used by default.</p>
</dd>
<dt>algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;ball_tree&#8217;, &#8216;kd_tree&#8217;, &#8216;brute&#8217;}, optional</span><dd><p class="first">Algorithm used to compute the nearest neighbors:</p>
<ul class="simple">
<li>&#8216;ball_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">BallTree</span></code></li>
<li>&#8216;kd_tree&#8217; will use <code class="xref py py-class docutils literal"><span class="pre">KDtree</span></code></li>
<li>&#8216;brute&#8217; will use a brute-force search.</li>
<li>&#8216;auto&#8217; will attempt to decide the most appropriate algorithm
based on the values passed to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method.</li>
</ul>
<p class="last">Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt>leaf_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default = 30)</span><dd>Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</dd>
<dt>metric</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string or DistanceMetric object (default=&#8217;minkowski&#8217;)</span><dd>the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt>p</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default = 2)</span><dd>Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</dd>
<dt>metric_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional (default = None)</span><dd>Additional keyword arguments for the metric function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">RadiusNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsRegressor</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">RadiusNeighborsRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[ 0.5]</span>
</pre></div>
</div>
<p>See also</p>
<p>NearestNeighbors
KNeighborsRegressor
KNeighborsClassifier
RadiusNeighborsClassifier</p>
<p><strong>Notes</strong></p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal"><span class="pre">algorithm</span></code> and <code class="docutils literal"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RadiusNeighborsRegressorSklearn</strong></li>
<li><strong>RadiusNeighborsRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomforestclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomForestClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomForestClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomForestClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomforestclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomForestClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomForestClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomForestClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>A random forest classifier.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.forest.RandomForestClassifier.html">sklearn.ensemble.forest.RandomForestClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and use averaging to
improve the predictive accuracy and control over-fitting.
The sub-sample size is always the same as the original
input sample size but the samples are drawn with replacement if
<cite>bootstrap=True</cite> (default).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span><dd>The number of trees in the forest.</dd>
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;gini&#8221;)</span><dd>The function to measure the quality of a split. Supported criteria are
&#8220;gini&#8221; for the Gini impurity and &#8220;entropy&#8221; for the information gain.
Note: this parameter is tree-specific.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a percentage and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If &#8220;auto&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite> (same as &#8220;auto&#8221;).</li>
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.
Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span><dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<code class="docutils literal"><span class="pre">min_samples_leaf</span></code> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.
Note: this parameter is tree-specific.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether bootstrap samples are used when building trees.</dd>
<dt>oob_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the tree building process.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest.</dd>
</dl>
<p>class_weight : dict, list of dicts, &#8220;balanced&#8221;, &#8220;balanced_subsample&#8221; or None, optional</p>
<blockquote>
<div><p>Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The &#8220;balanced_subsample&#8221; mode is the same as &#8220;balanced&#8221; except that weights are
computed based on the bootstrap sample for every tree grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</div></blockquote>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span><dd>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">n_classes_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span><dd>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_decision_function_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span><dd>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id57" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeClassifier, ExtraTreesClassifier</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomForestClassifierSklearn</strong></li>
<li><strong>RandomForestClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomforestregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomForestRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomForestRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomForestRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomforestregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomForestRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomForestRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomForestRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>A random forest regressor.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.forest.RandomForestRegressor.html">sklearn.ensemble.forest.RandomForestRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>A random forest is a meta estimator that fits a number of classifying
decision trees on various sub-samples of the dataset and use averaging
to improve the predictive accuracy and control over-fitting.
The sub-sample size is always the same as the original
input sample size but the samples are drawn with replacement if
<cite>bootstrap=True</cite> (default).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span><dd>The number of trees in the forest.</dd>
<dt>criterion</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8221;mse&#8221;)</span><dd>The function to measure the quality of a split. The only supported
criterion is &#8220;mse&#8221; for the mean squared error.
Note: this parameter is tree-specific.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=&#8221;auto&#8221;)</span><dd><p class="first">The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li>If int, then consider <cite>max_features</cite> features at each split.</li>
<li>If float, then <cite>max_features</cite> is a percentage and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</li>
<li>If &#8220;auto&#8221;, then <cite>max_features=n_features</cite>.</li>
<li>If &#8220;sqrt&#8221;, then <cite>max_features=sqrt(n_features)</cite>.</li>
<li>If &#8220;log2&#8221;, then <cite>max_features=log2(n_features)</cite>.</li>
<li>If None, then <cite>max_features=n_features</cite>.</li>
</ul>
<p class="last">Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.
Note: this parameter is tree-specific.</p>
</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span><dd>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.
Note: this parameter is tree-specific.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<code class="docutils literal"><span class="pre">min_samples_leaf</span></code> samples.
Note: this parameter is tree-specific.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.
Note: this parameter is tree-specific.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.
Note: this parameter is tree-specific.</dd>
<dt>bootstrap</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether bootstrap samples are used when building trees.</dd>
<dt>oob_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>whether to use out-of-bag samples to estimate
the generalization error.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the tree building process.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeRegressor</span><dd>The collection of fitted sub-estimators.</dd>
<dt><code class="docutils literal"><span class="pre">feature_importances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span><dd>The feature importances (the higher, the more important the feature).</dd>
<dt><code class="docutils literal"><span class="pre">n_features_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">n_outputs_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</dd>
<dt><code class="docutils literal"><span class="pre">oob_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of the training dataset obtained using an out-of-bag estimate.</dd>
<dt><code class="docutils literal"><span class="pre">oob_prediction_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples]</span><dd>Prediction computed with out-of-bag estimate on the training set.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id58" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<p>See also</p>
<p>DecisionTreeRegressor, ExtraTreesRegressor</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomForestRegressorSklearn</strong></li>
<li><strong>RandomForestRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomtreesembeddingtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomTreesEmbeddingTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomTreesEmbeddingTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomTreesEmbeddingTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomtreesembeddingtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomTreesEmbeddingTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomTreesEmbeddingTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomTreesEmbeddingTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>An ensemble of totally random trees.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.forest.RandomTreesEmbedding.html">sklearn.ensemble.forest.RandomTreesEmbedding</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as there are trees in
the forest.</p>
<p>The dimensionality of the resulting representation is
<code class="docutils literal"><span class="pre">n_out</span> <span class="pre">&lt;=</span> <span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">max_leaf_nodes</span></code>. If <code class="docutils literal"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">None</span></code>,
the number of leaf nodes is at most <code class="docutils literal"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of trees in the forest.</dd>
<dt>max_depth</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The maximum depth of each tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.
Ignored if <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> is not None.</dd>
<dt>min_samples_split</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=2)</span><dd>The minimum number of samples required to split an internal node.</dd>
<dt>min_samples_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The minimum number of samples in newly created leaves.  A split is
discarded if after the split, one of the leaves would contain less then
<code class="docutils literal"><span class="pre">min_samples_leaf</span></code> samples.</dd>
<dt>min_weight_fraction_leaf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span><dd>The minimum weighted fraction of the input samples required to be at a
leaf node.</dd>
<dt>max_leaf_nodes</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span><dd>Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.
If not None then <code class="docutils literal"><span class="pre">max_depth</span></code> will be ignored.</dd>
<dt>sparse_output</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=True)</span><dd>Whether or not to return a sparse CSR matrix, as default behavior,
or to return a dense array compatible with dense pipeline operators.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=1)</span><dd>The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
If -1, then the number of jobs is set to the number of cores.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span><dd>Controls the verbosity of the tree building process.</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span><dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">estimators_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span><dd>The collection of fitted sub-estimators.</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id59" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized trees&#8221;,
Machine Learning, 63(1), 3-42, 2006.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id60" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Moosmann, F. and Triggs, B. and Jurie, F.  &#8220;Fast discriminative
visual codebooks using randomized clustering forests&#8221;
NIPS 2007</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomTreesEmbeddingTransformerSklearn</strong></li>
<li><strong>RandomTreesEmbeddingTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomizedlassotransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedLassoTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomizedLassoTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedLassoTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomizedlassotransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomizedLassoTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomizedLassoTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedLassoTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Randomized Lasso.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.randomized_l1.RandomizedLasso.html">sklearn.linear_model.randomized_l1.RandomizedLasso</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Randomized Lasso works by resampling the train data and computing
a Lasso on each resampling. In short, the features selected more
often are good features. It is also known as stability selection.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, &#8216;aic&#8217;, or &#8216;bic&#8217;, optional</span><dd>The regularization parameter alpha parameter in the Lasso.
Warning: this is not the alpha parameter in the stability selection
article which is scaling.</dd>
<dt>scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The alpha parameter in the stability selection article used to
randomly scale the features. Should be between 0 and 1.</dd>
<dt>sample_fraction</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>n_resampling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of randomized models.</dd>
<dt>selection_threshold: float, optional</dt>
<dd>The score above which features should be selected.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>precompute</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">True | False | &#8216;auto&#8217;</span><dd>Whether to use a precomputed Gram matrix to speed up
calculations. If set to &#8216;auto&#8217; let us decide. The Gram
matrix can also be passed as argument.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Maximum number of iterations to perform in the Lars algorithm.</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the &#8216;tol&#8217; parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the resampling. If &#8216;-1&#8217;, use
all the CPUs</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>pre_dispatch</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span><dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>memory</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span><dd>Used for internal caching. By default, no caching is done.
If a string is given, it is the path to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span><dd>Feature scores between 0 and 1.</dd>
<dt><code class="docutils literal"><span class="pre">all_scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span><dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <code class="docutils literal"><span class="pre">scores_</span></code> is the max of         <code class="docutils literal"><span class="pre">all_scores_</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">RandomizedLasso</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_lasso</span> <span class="o">=</span> <span class="n">RandomizedLasso</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_sparse_recovery.py for an example.</p>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLogisticRegression, LogisticRegression</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomizedLassoTransformerSklearn</strong></li>
<li><strong>RandomizedLassoTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomizedlogisticregressiontransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedLogisticRegressionTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomizedLogisticRegressionTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedLogisticRegressionTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomizedlogisticregressiontransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomizedLogisticRegressionTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomizedLogisticRegressionTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedLogisticRegressionTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Randomized Logistic Regression</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.html">sklearn.linear_model.randomized_l1.RandomizedLogisticRegression</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Randomized Regression works by resampling the train data and computing
a LogisticRegression on each resampling. In short, the features selected
more often are good features. It is also known as stability selection.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=1</span><dd>The regularization parameter C in the LogisticRegression.</dd>
<dt>scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.5</span><dd>The alpha parameter in the stability selection article used to
randomly scale the features. Should be between 0 and 1.</dd>
<dt>sample_fraction</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.75</span><dd>The fraction of samples to be used in each randomized design.
Should be between 0 and 1. If 1, all samples are used.</dd>
<dt>n_resampling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=200</span><dd>Number of randomized models.</dd>
<dt>selection_threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=0.25</span><dd>The score above which features should be selected.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default=True</span><dd>whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean or integer, optional</span><dd>Sets the verbosity amount</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default=True</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default=1e-3</span><dd>tolerance for stopping criteria of LogisticRegression</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>Number of CPUs to use during the resampling. If &#8216;-1&#8217;, use
all the CPUs</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span><dd>If int, random_state is the seed used by the random number generator;
If RandomState instance, random_state is the random number generator;
If None, the random number generator is the RandomState instance used
by <cite>np.random</cite>.</dd>
<dt>pre_dispatch</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span><dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>memory</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Instance of joblib.Memory or string</span><dd>Used for internal caching. By default, no caching is done.
If a string is given, it is the path to the caching directory.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features]</span><dd>Feature scores between 0 and 1.</dd>
<dt><code class="docutils literal"><span class="pre">all_scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features, n_reg_parameter]</span><dd>Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests <code class="docutils literal"><span class="pre">scores_</span></code> is the max         of <code class="docutils literal"><span class="pre">all_scores_</span></code>.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">RandomizedLogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">randomized_logistic</span> <span class="o">=</span> <span class="n">RandomizedLogisticRegression</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Notes</strong></p>
<p>See examples/linear_model/plot_sparse_recovery.py for an example.</p>
<p><strong>References</strong></p>
<p>Stability selection
Nicolai Meinshausen, Peter Buhlmann
Journal of the Royal Statistical Society: Series B
Volume 72, Issue 4, pages 417-473, September 2010
DOI: 10.1111/j.1467-9868.2010.00740.x</p>
<p>See also</p>
<p>RandomizedLasso, Lasso, ElasticNet</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomizedLogisticRegressionTransformerSklearnNode</strong></li>
<li><strong>RandomizedLogisticRegressionTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomizedpcatransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedPCATransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomizedPCATransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedPCATransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomizedpcatransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomizedPCATransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomizedPCATransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedPCATransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Principal component analysis (PCA) using randomized SVD</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.RandomizedPCA.html">sklearn.decomposition.pca.RandomizedPCA</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Linear dimensionality reduction using approximated Singular Value
Decomposition of the data and keeping only the most significant
singular vectors to project the data to a lower dimensional space.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of components to keep. When not given or None, this
is set to n_features (the second dimension of the training data).</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt>iterated_power</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of iterations for the power method. 3 by default.</dd>
<dt>whiten</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd><p class="first">When True (False by default) the <cite>components_</cite> vectors are divided
by the singular values to ensure uncorrelated outputs with unit
component-wise variances.</p>
<p class="last">Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState instance or None (default)</span><dd>Pseudo Random Number generator seed control. If None, use the
numpy.random singleton.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Components with maximum variance.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span><dd>Percentage of variance explained by each of the selected components.         k is not set then all components are stored and the sum of explained         variances is equal to 1.0</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_features]</span><dd>Per-feature empirical mean, estimated from the training set.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">RandomizedPCA</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                 
<span class="go">RandomizedPCA(copy=True, iterated_power=3, n_components=2,</span>
<span class="go">       random_state=None, whiten=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.99244...  0.00755...]</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA
TruncatedSVD</p>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="halko2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Halko2009]</td><td><cite>Finding structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions Halko, et al., 2009
(arXiv:909)</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mrt" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MRT]</td><td><cite>A randomized algorithm for the decomposition of matrices
Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomizedPCATransformerSklearnNode</strong></li>
<li><strong>RandomizedPCATransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-randomizedsearchcvtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedSearchCVTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RandomizedSearchCVTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RandomizedSearchCVTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-randomizedsearchcvtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RandomizedSearchCVTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RandomizedSearchCVTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RandomizedSearchCVTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Randomized search on hyper parameters.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html">sklearn.grid_search.RandomizedSearchCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>RandomizedSearchCV implements a &#8220;fit&#8221; and a &#8220;score&#8221; method.
It also implements &#8220;predict&#8221;, &#8220;predict_proba&#8221;, &#8220;decision_function&#8221;,
&#8220;transform&#8221; and &#8220;inverse_transform&#8221; if they are implemented in the
estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated search over parameter settings.</p>
<p>In contrast to GridSearchCV, not all parameter values are tried out, but
rather a fixed number of parameter settings is sampled from the specified
distributions. The number of parameter settings that are tried is
given by n_iter.</p>
<p>If all parameters are presented as a list,
sampling without replacement is performed. If at least one parameter
is given as a distribution, sampling with replacement is used.
It is highly recommended to use continuous distributions for continuous
parameters.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator object.</span><dd>A object of that type is instantiated for each grid point.
This is assumed to implement the scikit-learn estimator interface.
Either estimator needs to provide a <code class="docutils literal"><span class="pre">score</span></code> function,
or <code class="docutils literal"><span class="pre">scoring</span></code> must be passed.</dd>
<dt>param_distributions</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>Dictionary with parameters names (string) as keys and distributions
or lists of parameters to try. Distributions must provide a <code class="docutils literal"><span class="pre">rvs</span></code>
method for sampling (such as those from scipy.stats.distributions).
If a list is given, it is sampled uniformly.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default=10</span><dd>Number of parameter settings that are sampled. n_iter trades
off runtime vs quality of the solution.</dd>
<dt>scoring</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, default=None</span><dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.
If <code class="docutils literal"><span class="pre">None</span></code>, the <code class="docutils literal"><span class="pre">score</span></code> method of the estimator is used.</dd>
<dt>fit_params</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, optional</span><dd>Parameters to pass to the fit method.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default=1</span><dd>Number of jobs to run in parallel.</dd>
<dt>pre_dispatch</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, or string, optional</span><dd><p class="first">Controls the number of jobs that get dispatched during parallel
execution. Reducing this number can be useful to avoid an
explosion of memory consumption when more jobs get dispatched
than CPUs can process. This parameter can be:</p>
<blockquote class="last">
<div><ul class="simple">
<li>None, in which case all the jobs are immediately
created and spawned. Use this for lightweight and
fast-running jobs, to avoid delays due to on-demand
spawning of the jobs</li>
<li>An int, giving the exact number of total jobs that are
spawned</li>
<li>A string, giving an expression as a function of n_jobs,
as in &#8216;2*n_jobs&#8217;</li>
</ul>
</div></blockquote>
</dd>
<dt>iid</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>If True, the data is assumed to be identically distributed across
the folds, and the loss minimized is the total loss per sample,
and not the mean loss across the folds.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the default 3-fold cross-validation,</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> used. If the estimator is a classifier
or if <code class="docutils literal"><span class="pre">y</span></code> is neither binary nor multiclass, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>refit</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Refit the best estimator with the entire dataset.
If &#8220;False&#8221;, it is impossible to make predictions using
this RandomizedSearchCV instance after fitting.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer</span><dd>Controls the verbosity: the higher, the more messages.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo random number generator state used for random uniform sampling
from lists of possible values instead of scipy.stats distributions.</dd>
<dt>error_score</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;raise&#8217; (default) or numeric</span><dd>Value to assign to the score if an error occurs in estimator fitting.
If set to &#8216;raise&#8217;, the error is raised. If a numeric value is given,
FitFailedWarning is raised. This parameter does not affect the refit
step, which will always raise the error.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">grid_scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of named tuples</span><dd><p class="first">Contains scores for all parameter combinations in param_grid.
Each entry corresponds to one parameter setting.
Each named tuple has the attributes:</p>
<blockquote class="last">
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">parameters</span></code>, a dict of parameter settings</li>
<li><code class="docutils literal"><span class="pre">mean_validation_score</span></code>, the mean score over the
cross-validation folds</li>
<li><code class="docutils literal"><span class="pre">cv_validation_scores</span></code>, the list of scores for each fold</li>
</ul>
</div></blockquote>
</dd>
<dt><code class="docutils literal"><span class="pre">best_estimator_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">estimator</span><dd>Estimator that was chosen by the search, i.e. estimator
which gave highest score (or smallest loss if specified)
on the left out data. Not available if refit=False.</dd>
<dt><code class="docutils literal"><span class="pre">best_score_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Score of best_estimator on the left out data.</dd>
<dt><code class="docutils literal"><span class="pre">best_params_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict</span><dd>Parameter setting that gave the best results on the hold out data.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The parameters selected are those that maximize the score of the held-out
data, according to the scoring parameter.</p>
<p>If <cite>n_jobs</cite> was set to a value higher than one, the data is copied for each
parameter setting(and not <cite>n_jobs</cite> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <cite>pre_dispatch</cite>. Then, the memory is copied only
<cite>pre_dispatch</cite> many times. A reasonable value for <cite>pre_dispatch</cite> is <cite>2 *
n_jobs</cite>.</p>
<p>See Also</p>
<p><code class="xref py py-class docutils literal"><span class="pre">GridSearchCV</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>Does exhaustive search over a grid of parameters.</li>
</ul>
</div></blockquote>
<p><code class="xref py py-class docutils literal"><span class="pre">ParameterSampler</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li>A generator over parameter settins, constructed from</li>
<li>param_distributions.</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RandomizedSearchCVTransformerSklearn</strong></li>
<li><strong>RandomizedSearchCVTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-ridgecvregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RidgeCVRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RidgeCVRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeCVRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-ridgecvregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RidgeCVRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RidgeCVRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RidgeCVRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Ridge regression with built-in cross-validation.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge.RidgeCV.html">sklearn.linear_model.ridge.RidgeCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>By default, it performs Generalized Cross-Validation, which is a form of
efficient Leave-One-Out cross-validation.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_alphas]</span><dd>Array of alpha values to try.
Small positive values of alpha improve the conditioning of the
problem and reduce the variance of the estimates.
Alpha corresponds to <code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>scoring</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span><dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the efficient Leave-One-Out cross-validation</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal"><span class="pre">y</span></code> is binary or multiclass,
<code class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></code> used, else, <code class="xref py py-class docutils literal"><span class="pre">KFold</span></code> is used.</p>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>gcv_mode</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{None, &#8216;auto&#8217;, &#8216;svd&#8217;, eigen&#8217;}, optional</span><dd><p class="first">Flag indicating which strategy to use when performing
Generalized Cross-Validation. Options are:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="s1">&#39;auto&#39;</span> <span class="p">:</span> <span class="n">use</span> <span class="n">svd</span> <span class="k">if</span> <span class="n">n_samples</span> <span class="o">&gt;</span> <span class="n">n_features</span> <span class="ow">or</span> <span class="n">when</span> <span class="n">X</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">sparse</span>
         <span class="n">matrix</span><span class="p">,</span> <span class="n">otherwise</span> <span class="n">use</span> <span class="n">eigen</span>
<span class="s1">&#39;svd&#39;</span> <span class="p">:</span> <span class="n">force</span> <span class="n">computation</span> <span class="n">via</span> <span class="n">singular</span> <span class="n">value</span> <span class="n">decomposition</span> <span class="n">of</span> <span class="n">X</span>
        <span class="p">(</span><span class="n">does</span> <span class="ow">not</span> <span class="n">work</span> <span class="k">for</span> <span class="n">sparse</span> <span class="n">matrices</span><span class="p">)</span>
<span class="s1">&#39;eigen&#39;</span> <span class="p">:</span> <span class="n">force</span> <span class="n">computation</span> <span class="n">via</span> <span class="n">eigendecomposition</span> <span class="n">of</span> <span class="n">X</span><span class="o">^</span><span class="n">T</span> <span class="n">X</span>
</pre></div>
</div>
<p class="last">The &#8216;auto&#8217; mode is the default and is intended to pick the cheaper
option of the two depending upon the shape and format of the training
data.</p>
</dd>
<dt>store_cv_values</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span><dd>Flag indicating if the cross-validation values corresponding to
each alpha should be stored in the <cite>cv_values_</cite> attribute (see
below). This flag is only compatible with <cite>cv=None</cite> (i.e. using
Generalized Cross-Validation).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cv_values_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional</span><dd>Cross-validation values for each alpha (if <cite>store_cv_values=True</cite> and         <cite>cv=None</cite>). After <cite>fit()</cite> has been called, this attribute will         contain the mean squared errors (by default) or the values of the         <cite>{loss,score}_func</cite> function (if provided in the constructor).</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span><dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span><dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Estimated regularization parameter.</dd>
</dl>
<p>See also</p>
<p>Ridge: Ridge regression
RidgeClassifier: Ridge classifier
RidgeClassifierCV: Ridge classifier with built-in cross validation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RidgeCVRegressorSklearnNode</strong></li>
<li><strong>RidgeCVRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-ridgeclassifiercvsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RidgeClassifierCVSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RidgeClassifierCVSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeClassifierCVSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-ridgeclassifiercvsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RidgeClassifierCVSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RidgeClassifierCVSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RidgeClassifierCVSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Ridge classifier with built-in cross-validation.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge.RidgeClassifierCV.html">sklearn.linear_model.ridge.RidgeClassifierCV</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>By default, it performs Generalized Cross-Validation, which is a form of
efficient Leave-One-Out cross-validation. Currently, only the n_features &gt;
n_samples case is handled efficiently.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alphas</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">numpy array of shape [n_alphas]</span><dd>Array of alpha values to try.
Small positive values of alpha improve the conditioning of the
problem and reduce the variance of the estimates.
Alpha corresponds to <code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as
LogisticRegression or LinearSVC.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>scoring</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, callable or None, optional, default: None</span><dd>A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</dd>
<dt>cv</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, cross-validation generator or an iterable, optional</span><dd><p class="first">Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul class="simple">
<li>None, to use the efficient Leave-One-Out cross-validation</li>
<li>integer, to specify the number of folds.</li>
<li>An object to be used as a cross-validation generator.</li>
<li>An iterable yielding train/test splits.</li>
</ul>
<p class="last">Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict or &#8216;balanced&#8217;, optional</span><dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p class="last">The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">cv_values_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional</span><dd>Cross-validation values for each alpha (if <cite>store_cv_values=True</cite> and</dd>
</dl>
<p><cite>cv=None</cite>). After <cite>fit()</cite> has been called, this attribute will contain     the mean squared errors (by default) or the values of the     <cite>{loss,score}_func</cite> function (if provided in the constructor).</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features] or [n_targets, n_features]</span><dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span><dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">alpha_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Estimated regularization parameter</dd>
</dl>
<p>See also</p>
<p>Ridge: Ridge regression
RidgeClassifier: Ridge classifier
RidgeCV: Ridge regression with built-in cross validation</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RidgeClassifierCVSklearnNode</strong></li>
<li><strong>RidgeClassifierCVSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-ridgeclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RidgeClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RidgeClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-ridgeclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RidgeClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RidgeClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RidgeClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Classifier using Ridge regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge.RidgeClassifier.html">sklearn.linear_model.ridge.RidgeClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Small positive values of alpha improve the conditioning of the problem
and reduce the variance of the estimates.  Alpha corresponds to
<code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as LogisticRegression or
LinearSVC.</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict or &#8216;balanced&#8217;, optional</span><dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p class="last">The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to calculate the intercept for this model. If set to false, no
intercept will be used in calculations (e.g. data is expected to be
already centered).</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of iterations for conjugate gradient solver.
The default value is determined by scipy.sparse.linalg.</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;svd&#8217;, &#8216;cholesky&#8217;, &#8216;lsqr&#8217;, &#8216;sparse_cg&#8217;, &#8216;sag&#8217;}</span><dd><p class="first">Solver to use in the computational routines:</p>
<ul class="last">
<li><p class="first">&#8216;auto&#8217; chooses the solver automatically based on the type of data.</p>
</li>
<li><p class="first">&#8216;svd&#8217; uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than
&#8216;cholesky&#8217;.</p>
</li>
<li><p class="first">&#8216;cholesky&#8217; uses the standard scipy.linalg.solve function to
obtain a closed-form solution.</p>
</li>
<li><p class="first">&#8216;sparse_cg&#8217; uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than &#8216;cholesky&#8217; for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</p>
</li>
<li><p class="first">&#8216;lsqr&#8217; uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fatest but may not be available
in old scipy versions. It also uses an iterative procedure.</p>
</li>
<li><p class="first">&#8216;sag&#8217; uses a Stochastic Average Gradient descent. It also uses an
iterative procedure, and is faster than other solvers when both
n_samples and n_features are large.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
</li>
</ul>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Precision of the solution.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data. Used in &#8216;sag&#8217; solver.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_classes, n_features)</span><dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span><dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array or None, shape (n_targets,)</span><dd>Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.</dd>
</dl>
<p>See also</p>
<p>Ridge, RidgeClassifierCV</p>
<p><strong>Notes</strong></p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RidgeClassifierSklearn</strong></li>
<li><strong>RidgeClassifierSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-ridgeregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RidgeRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RidgeRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RidgeRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-ridgeregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RidgeRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RidgeRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RidgeRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear least squares with l2 regularization.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge.Ridge.html">sklearn.linear_model.ridge.Ridge</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape [n_samples, n_targets]).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{float, array-like}, shape (n_targets)</span><dd>Small positive values of alpha improve the conditioning of the problem
and reduce the variance of the estimates.  Alpha corresponds to
<code class="docutils literal"><span class="pre">C^-1</span></code> in other linear models such as LogisticRegression or
LinearSVC. If an array is passed, penalties are assumed to be specific
to the targets. Hence they must correspond in number.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(e.g. data is expected to be already centered).</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Maximum number of iterations for conjugate gradient solver.
For &#8216;sparse_cg&#8217; and &#8216;lsqr&#8217; solvers, the default value is determined
by scipy.sparse.linalg. For &#8216;sag&#8217; solver, the default value is 1000.</dd>
<dt>normalize</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>If True, the regressors X will be normalized before regression.</dd>
<dt>solver</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;auto&#8217;, &#8216;svd&#8217;, &#8216;cholesky&#8217;, &#8216;lsqr&#8217;, &#8216;sparse_cg&#8217;, &#8216;sag&#8217;}</span><dd><p class="first">Solver to use in the computational routines:</p>
<ul class="simple">
<li>&#8216;auto&#8217; chooses the solver automatically based on the type of data.</li>
<li>&#8216;svd&#8217; uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than
&#8216;cholesky&#8217;.</li>
<li>&#8216;cholesky&#8217; uses the standard scipy.linalg.solve function to
obtain a closed-form solution.</li>
<li>&#8216;sparse_cg&#8217; uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than &#8216;cholesky&#8217; for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</li>
<li>&#8216;lsqr&#8217; uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fatest but may not be available
in old scipy versions. It also uses an iterative procedure.</li>
<li>&#8216;sag&#8217; uses a Stochastic Average Gradient descent. It also uses an
iterative procedure, and is often faster than other solvers when
both n_samples and n_features are large. Note that &#8216;sag&#8217; fast
convergence is only guaranteed on features with approximately the
same scale. You can preprocess the data with a scaler from
sklearn.preprocessing.</li>
</ul>
<p>All last four solvers support both dense and sparse data. However,
only &#8216;sag&#8217; supports sparse input when <cite>fit_intercept</cite> is True.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Precision of the solution.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd><p class="first">The seed of the pseudo random number generator to use when
shuffling the data. Used in &#8216;sag&#8217; solver.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>random_state</em> to support Stochastic Average Gradient.</p>
</div>
</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) or (n_targets, n_features)</span><dd>Weight vector(s).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape = (n_targets,)</span><dd>Independent term in decision function. Set to 0.0 if
<code class="docutils literal"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array or None, shape (n_targets,)</span><dd>Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.</dd>
</dl>
<p>See also</p>
<p>RidgeClassifier, RidgeCV, KernelRidge</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Ridge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,</span>
<span class="go">      normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001)</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RidgeRegressorSklearnNode</strong></li>
<li><strong>RidgeRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-robustscalertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.RobustScalerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.RobustScalerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.RobustScalerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-robustscalertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.RobustScalerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">RobustScalerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.RobustScalerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Scale features using statistics that are robust to outliers.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.RobustScaler.html">sklearn.preprocessing.data.RobustScaler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This Scaler removes the median and scales the data according to
the Interquartile Range (IQR). The IQR is the range between the 1st
quartile (25th quantile) and the 3rd quartile (75th quantile).</p>
<p>Centering and scaling happen independently on each feature (or each
sample, depending on the <cite>axis</cite> argument) by computing the relevant
statistics on the samples in the training set. Median and  interquartile
range are then stored to be used on later data using the <cite>transform</cite>
method.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators. Typically this is done by removing the mean
and scaling to unit variance. However, outliers can often influence the
sample mean / variance in a negative way. In such cases, the median and
the interquartile range often give better results.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>with_centering</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span><dd>If True, center the data before scaling.
This does not work (and will raise an exception) when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</dd>
<dt>with_scaling</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span><dd>If True, scale the data to interquartile range.</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default is True</span><dd>If False, try to avoid a copy and do inplace scaling instead.
This is not guaranteed to always work inplace; e.g. if the data is
not a NumPy array or scipy.sparse CSR matrix, a copy may still be
returned.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">center_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span><dd>The median value for each feature in the training set.</dd>
<dt><code class="docutils literal"><span class="pre">scale_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats</span><dd><p class="first">The (scaled) interquartile range for each feature in the training set.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> attribute.</p>
</div>
</dd>
</dl>
<p>See also</p>
<p><code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code> to perform centering
and scaling using mean and variance.</p>
<p><code class="xref py py-class docutils literal"><span class="pre">sklearn.decomposition.RandomizedPCA</span></code> with <cite>whiten=True</cite>
to further remove the linear correlation across features.</p>
<p><strong>Notes</strong></p>
<p>See examples/preprocessing/plot_robust_scaling.py for an example.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Median_(statistics">http://en.wikipedia.org/wiki/Median_(statistics</a>)
<a class="reference external" href="http://en.wikipedia.org/wiki/Interquartile_range">http://en.wikipedia.org/wiki/Interquartile_range</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>RobustScalerTransformerSklearn</strong></li>
<li><strong>RobustScalerTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-sgdclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SGDClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SGDClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SGDClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-sgdclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SGDClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SGDClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SGDClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.stochastic_gradient.SGDClassifier.html">sklearn.linear_model.stochastic_gradient.SGDClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning, see the partial_fit method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.</p>
<p>This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;hinge&#8217;, &#8216;log&#8217;, &#8216;modified_huber&#8217;, &#8216;squared_hinge&#8217;,                &#8216;perceptron&#8217;, or a regression loss: &#8216;squared_loss&#8217;, &#8216;huber&#8217;,                &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;</span><dd>The loss function to be used. Defaults to &#8216;hinge&#8217;, which gives a
linear SVM.
The &#8216;log&#8217; loss gives logistic regression, a probabilistic classifier.
&#8216;modified_huber&#8217; is another smooth loss that brings tolerance to
outliers as well as probability estimates.
&#8216;squared_hinge&#8217; is like hinge but is quadratically penalized.
&#8216;perceptron&#8217; is the linear loss used by the perceptron algorithm.
The other losses are designed for regression but can be useful in
classification as well; see SGDRegressor for a description.</dd>
<dt>penalty</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;none&#8217;, &#8216;l2&#8217;, &#8216;l1&#8217;, or &#8216;elasticnet&#8217;</span><dd>The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;
which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and
&#8216;elasticnet&#8217; might bring sparsity to the model (feature selection)
not achievable with &#8216;l2&#8217;.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Constant that multiplies the regularization term. Defaults to 0.0001
Also used to compute learning_rate when set to &#8216;optimal&#8217;.</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The number of passes over the training data (aka epochs). The number
of iterations is set to 1 if using partial_fit.
Defaults to 5.</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>Whether or not the training data should be shuffled after each epoch.
Defaults to True.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The verbosity level</dd>
<dt>epsilon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
&#8216;huber&#8217;, &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;.
For &#8216;huber&#8217;, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation. -1 means &#8216;all CPUs&#8217;. Defaults
to 1.</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd><p class="first">The learning rate schedule:</p>
<ul class="last simple">
<li>constant: eta = eta0</li>
<li>optimal: eta = 1.0 / (alpha * (t + t0)) [default]</li>
<li>invscaling: eta = eta0 / pow(t, power_t)</li>
<li>where t0 is chosen by a heuristic proposed by Leon Bottou.</li>
</ul>
</dd>
<dt>eta0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double</span><dd>The initial learning rate for the &#8216;constant&#8217; or &#8216;invscaling&#8217;
schedules. The default value is 0.0 as eta0 is not used by the
default schedule &#8216;optimal&#8217;.</dd>
<dt>power_t</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double</span><dd>The exponent for inverse scaling learning rate [default 0.5].</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">dict, {class_label: weight} or &#8220;balanced&#8221; or None, optional</span><dd><p class="first">Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p class="last">The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>average</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or int, optional</span><dd>When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So average=10 will begin averaging after seeing 10 samples.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features)</span><dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,) if n_classes == 2 else (n_classes,)</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span>
<span class="go">        eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span>
<span class="go">        learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, n_iter=5, n_jobs=1,</span>
<span class="go">        penalty=&#39;l2&#39;, power_t=0.5, random_state=None, shuffle=True,</span>
<span class="go">        verbose=0, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<p>LinearSVC, LogisticRegression, Perceptron</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SGDClassifierSklearnNode</strong></li>
<li><strong>SGDClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-sgdregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SGDRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SGDRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SGDRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-sgdregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SGDRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SGDRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SGDRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Linear model fitted by minimizing a regularized empirical loss with SGD</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.stochastic_gradient.SGDRegressor.html">sklearn.linear_model.stochastic_gradient.SGDRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense numpy arrays of
floating point values for the features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>loss</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;squared_loss&#8217;, &#8216;huber&#8217;, &#8216;epsilon_insensitive&#8217;,                 or &#8216;squared_epsilon_insensitive&#8217;</span><dd>The loss function to be used. Defaults to &#8216;squared_loss&#8217; which refers
to the ordinary least squares fit. &#8216;huber&#8217; modifies &#8216;squared_loss&#8217; to
focus less on getting outliers correct by switching from squared to
linear loss past a distance of epsilon. &#8216;epsilon_insensitive&#8217; ignores
errors less than epsilon and is linear past that; this is the loss
function used in SVR. &#8216;squared_epsilon_insensitive&#8217; is the same but
becomes squared loss past a tolerance of epsilon.</dd>
<dt>penalty</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, &#8216;none&#8217;, &#8216;l2&#8217;, &#8216;l1&#8217;, or &#8216;elasticnet&#8217;</span><dd>The penalty (aka regularization term) to be used. Defaults to &#8216;l2&#8217;
which is the standard regularizer for linear SVM models. &#8216;l1&#8217; and
&#8216;elasticnet&#8217; might bring sparsity to the model (feature selection)
not achievable with &#8216;l2&#8217;.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Constant that multiplies the regularization term. Defaults to 0.0001
Also used to compute learning_rate when set to &#8216;optimal&#8217;.</dd>
<dt>l1_ratio</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Defaults to 0.15.</dd>
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><dd>Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>The number of passes over the training data (aka epochs). The number
of iterations is set to 1 if using partial_fit.
Defaults to 5.</dd>
<dt>shuffle</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>Whether or not the training data should be shuffled after each epoch.
Defaults to True.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional</span><dd>The verbosity level.</dd>
<dt>epsilon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
&#8216;huber&#8217;, &#8216;epsilon_insensitive&#8217;, or &#8216;squared_epsilon_insensitive&#8217;.
For &#8216;huber&#8217;, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</dd>
<dt>learning_rate</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional</span><dd><p class="first">The learning rate:</p>
<ul class="last simple">
<li>constant: eta = eta0</li>
<li>optimal: eta = 1.0/(alpha * t)</li>
<li>invscaling: eta = eta0 / pow(t, power_t) [default]</li>
</ul>
</dd>
<dt>eta0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, optional</span><dd>The initial learning rate [default 0.01].</dd>
<dt>power_t</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">double, optional</span><dd>The exponent for inverse scaling learning rate [default 0.25].</dd>
<dt>warm_start</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span><dd>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>average</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool or int, optional</span><dd>When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So <code class="docutils literal"><span class="pre">average=10</span> <span class="pre">will</span></code> begin averaging after seeing 10
samples.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>Weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,)</span><dd>The intercept term.</dd>
<dt><code class="docutils literal"><span class="pre">average_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>Averaged weights assigned to the features.</dd>
<dt><code class="docutils literal"><span class="pre">average_intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (1,)</span><dd>The averaged intercept term.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>
<span class="go">SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,</span>
<span class="go">             fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;,</span>
<span class="go">             loss=&#39;squared_loss&#39;, n_iter=5, penalty=&#39;l2&#39;, power_t=0.25,</span>
<span class="go">             random_state=None, shuffle=True, verbose=0, warm_start=False)</span>
</pre></div>
</div>
<p>See also</p>
<p>Ridge, ElasticNet, Lasso, SVR</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SGDRegressorSklearn</strong></li>
<li><strong>SGDRegressorSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-svcclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SVCClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SVCClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SVCClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-svcclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SVCClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SVCClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SVCClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>C-Support Vector Classification.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.SVC.html">sklearn.svm.classes.SVC</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The implementation is based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to dataset with more than a couple of 10000 samples.</p>
<p>The multiclass support is handled according to a one-vs-one scheme.</p>
<p>For details on the precise mathematical formulation of the provided
kernel functions and how <cite>gamma</cite>, <cite>coef0</cite> and <cite>degree</cite> affect each
other, see the corresponding section in the narrative documentation:</p>
<p><span class="xref std std-ref">svm_kernels</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Penalty parameter C of the error term.</dd>
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span><dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to pre-compute the kernel matrix from data matrices; that matrix
should be an array of shape <code class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_samples)</span></code>.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span><dd>Degree of the polynomial kernel function (&#8216;poly&#8217;).
Ignored by all other kernels.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=&#8217;auto&#8217;)</span><dd>Kernel coefficient for &#8216;rbf&#8217;, &#8216;poly&#8217; and &#8216;sigmoid&#8217;.
If gamma is &#8216;auto&#8217; then 1/n_features will be used instead.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span><dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>probability</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd>Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, and will slow down that method.</dd>
<dt>shrinking</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to use the shrinking heuristic.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span><dd>Tolerance for stopping criterion.</dd>
<dt>cache_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Specify the size of the kernel cache (in MB).</dd>
<dt>class_weight</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{dict, &#8216;balanced&#8217;}, optional</span><dd>Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one.
The &#8220;balanced&#8221; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span><dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
<dt>decision_function_shape</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;ovo&#8217;, &#8216;ovr&#8217; or None, default=None</span><dd><p class="first">Whether to return a one-vs-rest (&#8216;ovr&#8217;) ecision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (&#8216;ovo&#8217;) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2).
The default of None will currently behave as &#8216;ovo&#8217; for backward
compatibility and raise a deprecation warning, but will change &#8216;ovr&#8217;
in 0.18.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>decision_function_shape=&#8217;ovr&#8217;</em> is recommended.</p>
</div>
<div class="last versionchanged">
<p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=&#8217;ovo&#8217; and None</em>.</p>
</div>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int seed, RandomState instance, or None (default)</span><dd>The seed of the pseudo random number generator to use when
shuffling the data for probability estimation.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span><dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span><dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">n_support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span><dd>Number of support vectors for each class.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span><dd>Coefficients of the support vector in the decision function.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the section about multi-class classification in the
SVM section of the User Guide for details.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is a readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">    decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="go">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="go">    tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>SVR</dt>
<dd>Support Vector Machine for Regression implemented using libsvm.</dd>
<dt>LinearSVC</dt>
<dd>Scalable Linear Support Vector Machine for classification
implemented using liblinear. Check the See also section of
LinearSVC for more comparison element.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SVCClassifierSklearnNode</strong></li>
<li><strong>SVCClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-svrregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-svrregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SVRRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SVRRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Epsilon-Support Vector Regression.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.classes.SVR.html">sklearn.svm.classes.SVR</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The free parameters in the model are C and epsilon.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>C</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span><dd>Penalty parameter C of the error term.</dd>
<dt>epsilon</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.1)</span><dd>Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
within which no penalty is associated in the training loss function
with points predicted within a distance epsilon from the actual
value.</dd>
<dt>kernel</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=&#8217;rbf&#8217;)</span><dd>Specifies the kernel type to be used in the algorithm.
It must be one of &#8216;linear&#8217;, &#8216;poly&#8217;, &#8216;rbf&#8217;, &#8216;sigmoid&#8217;, &#8216;precomputed&#8217; or
a callable.
If none is given, &#8216;rbf&#8217; will be used. If a callable is given it is
used to precompute the kernel matrix.</dd>
<dt>degree</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span><dd>Degree of the polynomial kernel function (&#8216;poly&#8217;).
Ignored by all other kernels.</dd>
<dt>gamma</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=&#8217;auto&#8217;)</span><dd>Kernel coefficient for &#8216;rbf&#8217;, &#8216;poly&#8217; and &#8216;sigmoid&#8217;.
If gamma is &#8216;auto&#8217; then 1/n_features will be used instead.</dd>
<dt>coef0</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span><dd>Independent term in kernel function.
It is only significant in &#8216;poly&#8217; and &#8216;sigmoid&#8217;.</dd>
<dt>shrinking</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span><dd>Whether to use the shrinking heuristic.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span><dd>Tolerance for stopping criterion.</dd>
<dt>cache_size</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Specify the size of the kernel cache (in MB).</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span><dd>Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span><dd>Hard limit on iterations within solver, or -1 for no limit.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">support_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span><dd>Indices of support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">support_vectors_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [nSV, n_features]</span><dd>Support vectors.</dd>
<dt><code class="docutils literal"><span class="pre">dual_coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_SV]</span><dd>Coefficients of the support vector in the decision function.</dd>
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1, n_features]</span><dd><p class="first">Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p class="last"><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [1]</span><dd>Constants in decision function.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=&#39;auto&#39;,</span>
<span class="go">    kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)</span>
</pre></div>
</div>
<p>See also</p>
<dl class="docutils">
<dt>NuSVR</dt>
<dd>Support Vector Machine for regression implemented using libsvm
using a parameter to control the number of support vectors.</dd>
<dt>LinearSVR</dt>
<dd>Scalable Linear Support Vector Machine for regression
implemented using liblinear.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SVRRegressorSklearnNode</strong></li>
<li><strong>SVRRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-selectfdrtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SelectFdrTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SelectFdrTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFdrTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-selectfdrtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SelectFdrTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SelectFdrTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SelectFdrTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Filter: Select the p-values for an estimated false discovery rate</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.univariate_selection.SelectFdr.html">sklearn.feature_selection.univariate_selection.SelectFdr</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This uses the Benjamini-Hochberg procedure. <code class="docutils literal"><span class="pre">alpha</span></code> is an upper bound
on the expected false discovery rate.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable</span><dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>p-values of feature scores.</dd>
</dl>
<p><strong>References</strong></p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/False_discovery_rate">http://en.wikipedia.org/wiki/False_discovery_rate</a></p>
<p>See also</p>
<p>f_classif: ANOVA F-value between labe/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SelectFdrTransformerSklearn</strong></li>
<li><strong>SelectFdrTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-selectfprtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SelectFprTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SelectFprTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFprTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-selectfprtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SelectFprTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SelectFprTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SelectFprTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Filter: Select the pvalues below alpha based on a FPR test.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.univariate_selection.SelectFpr.html">sklearn.feature_selection.univariate_selection.SelectFpr</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>FPR test stands for False Positive Rate test. It controls the total
amount of false detections.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable</span><dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The highest p-value for features to be kept.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>p-values of feature scores.</dd>
</dl>
<p>See also</p>
<p>f_classif: ANOVA F-value between labe/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SelectFprTransformerSklearn</strong></li>
<li><strong>SelectFprTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-selectfrommodeltransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SelectFromModelTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SelectFromModelTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFromModelTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-selectfrommodeltransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SelectFromModelTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SelectFromModelTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SelectFromModelTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Meta-transformer for selecting features based on importance weights.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.from_model.SelectFromModel.html">sklearn.feature_selection.from_model.SelectFromModel</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimator</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">object</span><dd>The base estimator from which the transformer is built.
This can be both a fitted (if <code class="docutils literal"><span class="pre">prefit</span></code> is set to True)
or a non-fitted estimator.</dd>
<dt>threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, float, optional default None</span><dd>The threshold value to use for feature selection. Features whose
importance is greater or equal are kept while the others are
discarded. If &#8220;median&#8221; (resp. &#8220;mean&#8221;), then the <code class="docutils literal"><span class="pre">threshold</span></code> value is
the median (resp. the mean) of the feature importances. A scaling
factor (e.g., &#8220;1.25*mean&#8221;) may also be used. If None and if the
estimator has a parameter penalty set to l1, either explicitly
or implicity (e.g, Lasso), the threshold is used is 1e-5.
Otherwise, &#8220;mean&#8221; is used by default.</dd>
<dt>prefit</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, default False</span><dd>Whether a prefit model is expected to be passed into the constructor
directly or not. If True, <code class="docutils literal"><span class="pre">transform</span></code> must be called directly
and SelectFromModel cannot be used with <code class="docutils literal"><span class="pre">cross_val_score</span></code>,
<code class="docutils literal"><span class="pre">GridSearchCV</span></code> and similar utilities that clone the estimator.
Otherwise train the model using <code class="docutils literal"><span class="pre">fit</span></code> and then <code class="docutils literal"><span class="pre">transform</span></code> to do
feature selection.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><cite>estimator_</cite>: an estimator</dt>
<dd>The base estimator from which the transformer is built.
This is stored only when a non-fitted estimator is passed to the
<code class="docutils literal"><span class="pre">SelectFromModel</span></code>, i.e when prefit is False.</dd>
<dt><cite>threshold_</cite>: float</dt>
<dd>The threshold value used for feature selection.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SelectFromModelTransformerSklearn</strong></li>
<li><strong>SelectFromModelTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-selectfwetransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SelectFweTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SelectFweTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectFweTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-selectfwetransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SelectFweTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SelectFweTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SelectFweTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Filter: Select the p-values corresponding to Family-wise error rate</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.univariate_selection.SelectFwe.html">sklearn.feature_selection.univariate_selection.SelectFwe</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable</span><dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>The highest uncorrected p-value for features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>p-values of feature scores.</dd>
</dl>
<p>See also</p>
<p>f_classif: ANOVA F-value between labe/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectPercentile: Select features based on percentile of the highest scores.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SelectFweTransformerSklearnNode</strong></li>
<li><strong>SelectFweTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-selectkbesttransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SelectKBestTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SelectKBestTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectKBestTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-selectkbesttransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SelectKBestTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SelectKBestTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SelectKBestTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Select features according to the k highest scores.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.univariate_selection.SelectKBest.html">sklearn.feature_selection.univariate_selection.SelectKBest</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable</span><dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>k</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or &#8220;all&#8221;, optional, default=10</span><dd>Number of top features to select.
The &#8220;all&#8221; option bypasses selection, for use in a parameter search.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>p-values of feature scores.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<p>See also</p>
<p>f_classif: ANOVA F-value between labe/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectPercentile: Select features based on percentile of the highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SelectKBestTransformerSklearn</strong></li>
<li><strong>SelectKBestTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-selectpercentiletransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SelectPercentileTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SelectPercentileTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SelectPercentileTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-selectpercentiletransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SelectPercentileTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SelectPercentileTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SelectPercentileTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Select features according to a percentile of the highest scores.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.univariate_selection.SelectPercentile.html">sklearn.feature_selection.univariate_selection.SelectPercentile</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>score_func</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable</span><dd>Function taking two arrays X and y, and returning a pair of arrays
(scores, pvalues).</dd>
<dt>percentile</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default=10</span><dd>Percent of features to keep.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scores_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>Scores of features.</dd>
<dt><code class="docutils literal"><span class="pre">pvalues_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape=(n_features,)</span><dd>p-values of feature scores.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>Ties between features with equal scores will be broken in an unspecified
way.</p>
<p>See also</p>
<p>f_classif: ANOVA F-value between labe/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
f_regression: F-value between label/feature for regression tasks.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
GenericUnivariateSelect: Univariate feature selector with configurable mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SelectPercentileTransformerSklearn</strong></li>
<li><strong>SelectPercentileTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-skewedchi2samplertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SkewedChi2SamplerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SkewedChi2SamplerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SkewedChi2SamplerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-skewedchi2samplertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SkewedChi2SamplerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SkewedChi2SamplerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SkewedChi2SamplerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Approximates feature map of the &#8220;skewed chi-squared&#8221; kernel by Monte
Carlo approximation of its Fourier transform.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.SkewedChi2Sampler.html">sklearn.kernel_approximation.SkewedChi2Sampler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>skewedness</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>&#8220;skewedness&#8221; parameter of the kernel. Needs to be cross-validated.</dd>
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>number of Monte Carlo samples per original feature.
Equals the dimensionality of the computed feature space.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{int, RandomState}, optional</span><dd>If int, random_state is the seed used by the random number generator;
if RandomState instance, random_state is the random number generator.</dd>
</dl>
<p><strong>References</strong></p>
<p>See &#8220;Random Fourier Approximations for Skewed Multiplicative Histogram
Kernels&#8221; by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.</p>
<p>See also</p>
<dl class="docutils">
<dt>AdditiveChi2Sampler</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">A different approach for approximating an additive</span><dd>variant of the chi squared kernel.</dd>
</dl>
<p>sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SkewedChi2SamplerTransformerSklearnNode</strong></li>
<li><strong>SkewedChi2SamplerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-sparsecodertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SparseCoderTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SparseCoderTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SparseCoderTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-sparsecodertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SparseCoderTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SparseCoderTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SparseCoderTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Sparse coding</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.dict_learning.SparseCoder.html">sklearn.decomposition.dict_learning.SparseCoder</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Finds a sparse representation of data against a fixed, precomputed
dictionary.</p>
<p>Each row of the result is the solution to a sparse coding problem.
The goal is to find a sparse array <cite>code</cite> such that:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">~=</span> <span class="n">code</span> <span class="o">*</span> <span class="n">dictionary</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>dictionary</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>The dictionary atoms used for sparse coding. Lines are assumed to be
normalized to unit norm.</dd>
<dt>transform_algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lasso_lars&#8217;, &#8216;lasso_cd&#8217;, &#8216;lars&#8217;, &#8216;omp&#8217;,     &#8216;threshold&#8217;}</span><dd><p class="first">Algorithm used to transform the data:</p>
<ul class="last simple">
<li>lars: uses the least angle regression method (linear_model.lars_path)</li>
<li>lasso_lars: uses Lars to compute the Lasso solution</li>
<li>lasso_cd: uses the coordinate descent method to compute the</li>
<li>Lasso solution (linear_model.Lasso). lasso_lars will be faster if</li>
<li>the estimated components are sparse.</li>
<li>omp: uses orthogonal matching pursuit to estimate the sparse solution</li>
<li>threshold: squashes to zero all coefficients less than alpha from</li>
<li>the projection <code class="docutils literal"><span class="pre">dictionary</span> <span class="pre">*</span> <span class="pre">X'</span></code></li>
</ul>
</dd>
<dt>transform_n_nonzero_coefs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, <code class="docutils literal"><span class="pre">0.1</span> <span class="pre">*</span> <span class="pre">n_features</span></code> by default</span><dd>Number of nonzero coefficients to target in each column of the
solution. This is only used by <cite>algorithm=&#8217;lars&#8217;</cite> and <cite>algorithm=&#8217;omp&#8217;</cite>
and is overridden by <cite>alpha</cite> in the <cite>omp</cite> case.</dd>
<dt>transform_alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, 1. by default</span><dd>If <cite>algorithm=&#8217;lasso_lars&#8217;</cite> or <cite>algorithm=&#8217;lasso_cd&#8217;</cite>, <cite>alpha</cite> is the
penalty applied to the L1 norm.
If <cite>algorithm=&#8217;threshold&#8217;</cite>, <cite>alpha</cite> is the absolute value of the
threshold below which coefficients will be squashed to zero.
If <cite>algorithm=&#8217;omp&#8217;</cite>, <cite>alpha</cite> is the tolerance parameter: the value of
the reconstruction error targeted. In this case, it overrides
<cite>n_nonzero_coefs</cite>.</dd>
<dt>split_sign</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">bool, False by default</span><dd>Whether to split the sparse feature vector into the concatenation of
its negative part and its positive part. This can improve the
performance of downstream classifiers.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>number of parallel jobs to run</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>The unchanged dictionary atoms</dd>
</dl>
<p>See also</p>
<p>DictionaryLearning
MiniBatchDictionaryLearning
SparsePCA
MiniBatchSparsePCA
sparse_encode</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SparseCoderTransformerSklearnNode</strong></li>
<li><strong>SparseCoderTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-sparsepcatransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SparsePCATransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SparsePCATransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SparsePCATransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-sparsepcatransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SparsePCATransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SparsePCATransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SparsePCATransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Sparse Principal Components Analysis (SparsePCA)</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.sparse_pca.SparsePCA.html">sklearn.decomposition.sparse_pca.SparsePCA</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Finds the set of sparse components that can optimally reconstruct
the data.  The amount of sparseness is controllable by the coefficient
of the L1 penalty, given by the parameter alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>Number of sparse atoms to extract.</dd>
<dt>alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>Sparsity controlling parameter. Higher values lead to sparser
components.</dd>
<dt>ridge_alpha</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>Amount of ridge shrinkage to apply in order to improve
conditioning when calling the transform method.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>Maximum number of iterations to perform.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float,</span><dd>Tolerance for the stopping condition.</dd>
<dt>method</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;lars&#8217;, &#8216;cd&#8217;}</span><dd>lars: uses the least angle regression method to solve the lasso problem
(linear_model.lars_path)
cd: uses the coordinate descent method to compute the
Lasso solution (linear_model.Lasso). Lars will be faster if
the estimated components are sparse.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int,</span><dd>Number of parallel jobs to run.</dd>
<dt>U_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_samples, n_components),</span><dd>Initial values for the loadings for warm restart scenarios.</dd>
<dt>V_init</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of shape (n_components, n_features),</span><dd>Initial values for the components for warm restart scenarios.</dd>
</dl>
<p>verbose :</p>
<blockquote>
<div><ul class="simple">
<li>Degree of verbosity of the printed output.</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState</span><dd>Pseudo number generator state used for random sampling.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components, n_features]</span><dd>Sparse components extracted from the data.</dd>
<dt><code class="docutils literal"><span class="pre">error_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array</span><dd>Vector of errors at each iteration.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations run.</dd>
</dl>
<p>See also</p>
<p>PCA
MiniBatchSparsePCA
DictionaryLearning</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SparsePCATransformerSklearnNode</strong></li>
<li><strong>SparsePCATransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-sparserandomprojectiontransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.SparseRandomProjectionTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.SparseRandomProjectionTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.SparseRandomProjectionTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-sparserandomprojectiontransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.SparseRandomProjectionTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">SparseRandomProjectionTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.SparseRandomProjectionTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Reduce dimensionality through sparse random projection</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html">sklearn.random_projection.SparseRandomProjection</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Sparse random matrix is an alternative to dense random
projection matrix that guarantees similar embedding quality while being
much more memory efficient and allowing faster computation of the
projected data.</p>
<p>If we note <cite>s = 1 / density</cite> the components of the random matrix are
drawn from:</p>
<blockquote>
<div><ul class="simple">
<li>-sqrt(s) / sqrt(n_components)   with probability 1 / 2s</li>
<li>0                              with probability 1 - 1 / s</li>
<li>+sqrt(s) / sqrt(n_components)   with probability 1 / 2s</li>
</ul>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or &#8216;auto&#8217;, optional (default = &#8216;auto&#8217;)</span><dd><p class="first">Dimensionality of the target projection space.</p>
<p>n_components can be automatically adjusted according to the
number of samples in the dataset and the bound given by the
Johnson-Lindenstrauss lemma. In that case the quality of the
embedding is controlled by the <code class="docutils literal"><span class="pre">eps</span></code> parameter.</p>
<p class="last">It should be noted that Johnson-Lindenstrauss lemma can yield
very conservative estimated of the required number of components
as it makes no assumption on the structure of the dataset.</p>
</dd>
<dt>density</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range ]0, 1], optional (default=&#8217;auto&#8217;)</span><dd><p class="first">Ratio of non-zero component in the random projection matrix.</p>
<p>If density = &#8216;auto&#8217;, the value is set to the minimum density
as recommended by Ping Li et al.: 1 / sqrt(n_features).</p>
<p class="last">Use density = 1 / 3.0 if you want to reproduce the results from
Achlioptas, 2001.</p>
</dd>
<dt>eps</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">strictly positive float, optional, (default=0.1)</span><dd><p class="first">Parameter to control the quality of the embedding according to
the Johnson-Lindenstrauss lemma when n_components is set to
&#8216;auto&#8217;.</p>
<p class="last">Smaller values lead to better embedding and higher number of
dimensions (n_components) in the target projection space.</p>
</dd>
<dt>dense_output</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span><dd><p class="first">If True, ensure that the output of the random projection is a
dense numpy array even if the input and random projection matrix
are both sparse. In practice, if the number of components is
small the number of zero components in the projected data will
be very small and it will be more CPU and memory efficient to
use a dense representation.</p>
<p class="last">If False, the projected data uses a sparse representation if
the input is sparse.</p>
</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, RandomState instance or None (default=None)</span><dd>Control the pseudo random number generator used to generate the
matrix at fit time.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">n_component_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Concrete number of components computed when n_components=&#8221;auto&#8221;.</dd>
<dt><code class="docutils literal"><span class="pre">components_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">CSR matrix with shape [n_components, n_features]</span><dd>Random matrix used for the projection.</dd>
<dt><code class="docutils literal"><span class="pre">density_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range 0.0 - 1.0</span><dd>Concrete density computed from when density = &#8220;auto&#8221;.</dd>
</dl>
<p>See Also</p>
<p>GaussianRandomProjection</p>
<p><strong>References</strong></p>
<table class="docutils footnote" frame="void" id="id62" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Ping Li, T. Hastie and K. W. Church, 2006,
&#8220;Very Sparse Random Projections&#8221;.
<a class="reference external" href="http://www.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf">http://www.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id63" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>D. Achlioptas, 2001, &#8220;Database-friendly random projections&#8221;,
<a class="reference external" href="http://www.cs.ucsc.edu/~optas/papers/jl.pdf">http://www.cs.ucsc.edu/~optas/papers/jl.pdf</a></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>SparseRandomProjectionTransformerSklearn</strong></li>
<li><strong>SparseRandomProjectionTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-standardscalertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.StandardScalerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.StandardScalerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.StandardScalerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-standardscalertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.StandardScalerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">StandardScalerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.StandardScalerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Standardize features by removing the mean and scaling to unit variance</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.data.StandardScaler.html">sklearn.preprocessing.data.StandardScaler</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Centering and scaling happen independently on each feature by computing
the relevant statistics on the samples in the training set. Mean and
standard deviation are then stored to be used on later data using the
<cite>transform</cite> method.</p>
<p>Standardization of a dataset is a common requirement for many
machine learning estimators: they might behave badly if the
individual feature do not more or less look like standard normally
distributed data (e.g. Gaussian with 0 mean and unit variance).</p>
<p>For instance many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the L1 and L2 regularizers of linear models) assume that
all features are centered around 0 and have variance in the same
order. If a feature has a variance that is orders of magnitude larger
that others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.</p>
<p>This scaler can also be applied to sparse CSR or CSC matrices by passing
<cite>with_mean=False</cite> to avoid breaking the sparsity structure of the data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>with_mean</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span><dd>If True, center the data before scaling.
This does not work (and will raise an exception) when attempted on
sparse matrices, because centering them entails building a dense
matrix which in common use cases is likely to be too large to fit in
memory.</dd>
<dt>with_std</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, True by default</span><dd>If True, scale the data to unit variance (or equivalently,
unit standard deviation).</dd>
<dt>copy</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If False, try to avoid a copy and do inplace scaling instead.
This is not guaranteed to always work inplace; e.g. if the data is
not a NumPy array or scipy.sparse CSR matrix, a copy may still be
returned.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">scale_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_features,)</span><dd><p class="first">Per feature relative scaling of the data.</p>
<div class="last versionadded">
<p><span class="versionmodified">New in version 0.17: </span><em>scale_</em> is recommended instead of deprecated <em>std_</em>.</p>
</div>
</dd>
<dt><code class="docutils literal"><span class="pre">mean_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats with shape [n_features]</span><dd>The mean value for each feature in the training set.</dd>
<dt><code class="docutils literal"><span class="pre">var_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array of floats with shape [n_features]</span><dd>The variance for each feature in the training set. Used to compute
<cite>scale_</cite></dd>
<dt><code class="docutils literal"><span class="pre">n_samples_seen_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>The number of samples processed by the estimator. Will be reset on
new calls to fit, but increments across <code class="docutils literal"><span class="pre">partial_fit</span></code> calls.</dd>
</dl>
<p>See also</p>
<p><code class="xref py py-func docutils literal"><span class="pre">sklearn.preprocessing.scale()</span></code> to perform centering and
scaling without using the <code class="docutils literal"><span class="pre">Transformer</span></code> object oriented API</p>
<p><code class="xref py py-class docutils literal"><span class="pre">sklearn.decomposition.RandomizedPCA</span></code> with <cite>whiten=True</cite>
to further remove the linear correlation across features.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>StandardScalerTransformerSklearn</strong></li>
<li><strong>StandardScalerTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-tfidftransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.TfidfTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.TfidfTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TfidfTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-tfidftransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.TfidfTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">TfidfTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.TfidfTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Transform a count matrix to a normalized tf or tf-idf representation</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html">sklearn.feature_extraction.text.TfidfTransformer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Tf means term-frequency while tf-idf means term-frequency times inverse
document-frequency. This is a common term weighting scheme in information
retrieval, that has also found good use in document classification.</p>
<p>The goal of using tf-idf instead of the raw frequencies of occurrence of a
token in a given document is to scale down the impact of tokens that occur
very frequently in a given corpus and that are hence empirically less
informative than features that occur in a small fraction of the training
corpus.</p>
<p>The actual formula used for tf-idf is tf * (idf + 1) = tf + tf * idf,
instead of tf * idf. The effect of this is that terms with zero idf, i.e.
that occur in all documents of a training set, will not be entirely
ignored. The formulas used to compute tf and idf depend on parameter
settings that correspond to the SMART notation used in IR, as follows:</p>
<p>Tf is &#8220;n&#8221; (natural) by default, &#8220;l&#8221; (logarithmic) when sublinear_tf=True.
Idf is &#8220;t&#8221; when use_idf is given, &#8220;n&#8221; (none) otherwise.
Normalization is &#8220;c&#8221; (cosine) when norm=&#8217;l2&#8217;, &#8220;n&#8221; (none) when norm=None.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>norm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span><dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>use_idf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span><dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p><strong>References</strong></p>
<table class="docutils citation" frame="void" id="yates2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Yates2011]</td><td><cite>R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern
Information Retrieval. Addison Wesley, pp. 68-74.</cite></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mrs2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[MRS2008]</td><td><cite>C.D. Manning, P. Raghavan and H. Schuetze  (2008).
Introduction to Information Retrieval. Cambridge University
Press, pp. 118-120.</cite></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>TfidfTransformerSklearnNode</strong></li>
<li><strong>TfidfTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-tfidfvectorizertransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.TfidfVectorizerTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.TfidfVectorizerTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TfidfVectorizerTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-tfidfvectorizertransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.TfidfVectorizerTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">TfidfVectorizerTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.TfidfVectorizerTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Convert a collection of raw documents to a matrix of TF-IDF features.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">sklearn.feature_extraction.text.TfidfVectorizer</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>Equivalent to CountVectorizer followed by TfidfTransformer.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>input</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;filename&#8217;, &#8216;file&#8217;, &#8216;content&#8217;}</span><dd><p class="first">If &#8216;filename&#8217;, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If &#8216;file&#8217;, the sequence items must have a &#8216;read&#8217; method (file-like
object) that is called to fetch the bytes in memory.</p>
<p class="last">Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</dd>
<dt>encoding</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, &#8216;utf-8&#8217; by default.</span><dd>If bytes or files are given to analyze, this encoding is used to
decode.</dd>
<dt>decode_error</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;strict&#8217;, &#8216;ignore&#8217;, &#8216;replace&#8217;}</span><dd>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>encoding</cite>. By default, it is
&#8216;strict&#8217;, meaning that a UnicodeDecodeError will be raised. Other
values are &#8216;ignore&#8217; and &#8216;replace&#8217;.</dd>
<dt>strip_accents</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">{&#8216;ascii&#8217;, &#8216;unicode&#8217;, None}</span><dd>Remove accents during the preprocessing step.
&#8216;ascii&#8217; is a fast method that only works on characters that have
an direct ASCII mapping.
&#8216;unicode&#8217; is a slightly slower method that works on any characters.
None (default) does nothing.</dd>
<dt>analyzer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, {&#8216;word&#8217;, &#8216;char&#8217;} or callable</span><dd><p class="first">Whether the feature should be made of word or character n-grams.</p>
<p class="last">If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</dd>
<dt>preprocessor</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span><dd>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</dd>
<dt>tokenizer</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">callable or None (default)</span><dd>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</dd>
<dt>ngram_range</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">tuple (min_n, max_n)</span><dd>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</dd>
<dt>stop_words</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string {&#8216;english&#8217;}, list, or None (default)</span><dd><p class="first">If a string, it is passed to _check_stop_list and the appropriate stop
list is returned. &#8216;english&#8217; is currently the only supported string
value.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.
Only applies if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>.</p>
<p class="last">If None, no stop words will be used. max_df can be set to a value
in the range [0.7, 1.0) to automatically detect and filter stop
words based on intra corpus document frequency of terms.</p>
</dd>
<dt>lowercase</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default True</span><dd>Convert all characters to lowercase before tokenizing.</dd>
<dt>token_pattern</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string</span><dd>Regular expression denoting what constitutes a &#8220;token&#8221;, only used
if <code class="docutils literal"><span class="pre">analyzer</span> <span class="pre">==</span> <span class="pre">'word'</span></code>. The default regexp selects tokens of 2
or more alphanumeric characters (punctuation is completely ignored
and always treated as a token separator).</dd>
<dt>max_df</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, default=1.0</span><dd>When building the vocabulary ignore terms that have a document
frequency strictly higher than the given threshold (corpus-specific
stop words).
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>min_df</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float in range [0.0, 1.0] or int, default=1</span><dd>When building the vocabulary ignore terms that have a document
frequency strictly lower than the given threshold. This value is also
called cut-off in the literature.
If float, the parameter represents a proportion of documents, integer
absolute counts.
This parameter is ignored if vocabulary is not None.</dd>
<dt>max_features</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or None, default=None</span><dd><p class="first">If not None, build a vocabulary that only consider the top
max_features ordered by term frequency across the corpus.</p>
<p class="last">This parameter is ignored if vocabulary is not None.</p>
</dd>
<dt>vocabulary</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">Mapping or iterable, optional</span><dd>Either a Mapping (e.g., a dict) where keys are terms and values are
indices in the feature matrix, or an iterable over terms. If not
given, a vocabulary is determined from the input documents.</dd>
<dt>binary</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span><dd>If True, all non-zero term counts are set to 1. This does not mean
outputs will have only 0/1 values, only that the tf term in tf-idf
is binary. (Set idf and normalization to False to get 0/1 outputs.)</dd>
<dt>dtype</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">type, optional</span><dd>Type of the matrix returned by fit_transform() or transform().</dd>
<dt>norm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">&#8216;l1&#8217;, &#8216;l2&#8217; or None, optional</span><dd>Norm used to normalize term vectors. None for no normalization.</dd>
<dt>use_idf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Enable inverse-document-frequency reweighting.</dd>
<dt>smooth_idf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=True</span><dd>Smooth idf weights by adding one to document frequencies, as if an
extra document was seen containing every term in the collection
exactly once. Prevents zero divisions.</dd>
<dt>sublinear_tf</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, default=False</span><dd>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">idf_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_features], or None</span><dd>The learned idf vector (global term weights)
when <code class="docutils literal"><span class="pre">use_idf</span></code> is set to True, None otherwise.</dd>
<dt><code class="docutils literal"><span class="pre">stop_words_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">set</span><dd><p class="first">Terms that were ignored because they either:</p>
<blockquote>
<div><ul class="simple">
<li>occurred in too many documents (<cite>max_df</cite>)</li>
<li>occurred in too few documents (<cite>min_df</cite>)</li>
<li>were cut off by feature selection (<cite>max_features</cite>).</li>
</ul>
</div></blockquote>
<p class="last">This is only available if no vocabulary was given.</p>
</dd>
</dl>
<p>See also</p>
<dl class="docutils">
<dt>CountVectorizer</dt>
<dd>Tokenize the documents and count the occurrences of token and return
them as a sparse matrix</dd>
<dt>TfidfTransformer</dt>
<dd>Apply Term Frequency Inverse Document Frequency normalization to a
sparse matrix of occurrence counts.</dd>
</dl>
<p><strong>Notes</strong></p>
<p>The <code class="docutils literal"><span class="pre">stop_words_</span></code> attribute can get large and increase the model size
when pickling. This attribute is provided only for introspection and can
be safely removed using delattr or set to None before pickling.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>TfidfVectorizerTransformerSklearnNode</strong></li>
<li><strong>TfidfVectorizerTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-theilsenregressorsklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.TheilSenRegressorSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.TheilSenRegressorSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TheilSenRegressorSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-theilsenregressorsklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.TheilSenRegressorSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">TheilSenRegressorSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.TheilSenRegressorSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Theil-Sen Estimator: robust multivariate regression model.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.theil_sen.TheilSenRegressor.html">sklearn.linear_model.theil_sen.TheilSenRegressor</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>The algorithm calculates least square solutions on subsets with size
n_subsamples of the samples in X. Any value of n_subsamples between the
number of features and samples leads to an estimator with a compromise
between robustness and efficiency. Since the number of least square
solutions is &#8220;n_samples choose n_subsamples&#8221;, it can be extremely large
and can therefore be limited with max_subpopulation. If this limit is
reached, the subsets are chosen randomly. In a final step, the spatial
median (or L1 median) is calculated of all least square solutions.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>fit_intercept</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations.</dd>
<dt>copy_X</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span><dd>If True, X will be copied; else, it may be overwritten.</dd>
<dt>max_subpopulation</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 1e4</span><dd>Instead of computing with a set of cardinality &#8216;n choose k&#8217;, where n is
the number of samples and k is the number of subsamples (at least
number of features), consider only a stochastic subpopulation of a
given maximal size if &#8216;n choose k&#8217; is larger than max_subpopulation.
For other than small problem sizes this parameter will determine
memory usage and runtime if n_subsamples is not changed.</dd>
<dt>n_subsamples</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default None</span><dd>Number of samples to calculate the parameters. This is at least the
number of features (plus 1 if fit_intercept=True) and the number of
samples as a maximum. A lower number leads to a higher breakdown
point and a low efficiency while a high number leads to a low
breakdown point and a high efficiency. If None, take the
minimum number of subsamples leading to maximal robustness.
If n_subsamples is set to n_samples, Theil-Sen is identical to least
squares.</dd>
<dt>max_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default 300</span><dd>Maximum number of iterations for the calculation of spatial median.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional, default 1.e-3</span><dd>Tolerance when calculating spatial median.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">RandomState or an int seed, optional, default None</span><dd>A random number generator instance to define the state of the
random permutations generator.</dd>
<dt>n_jobs</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional, default 1</span><dd>Number of CPUs to use during the cross validation. If <code class="docutils literal"><span class="pre">-1</span></code>, use
all the CPUs.</dd>
<dt>verbose</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span><dd>Verbose mode when fitting the model.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">coef_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_features)</span><dd>Coefficients of the regression model (median of distribution).</dd>
<dt><code class="docutils literal"><span class="pre">intercept_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Estimated intercept of regression model.</dd>
<dt><code class="docutils literal"><span class="pre">breakdown_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float</span><dd>Approximated breakdown point.</dd>
<dt><code class="docutils literal"><span class="pre">n_iter_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of iterations needed for the spatial median.</dd>
<dt><code class="docutils literal"><span class="pre">n_subpopulation_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int</span><dd>Number of combinations taken into account from &#8216;n choose k&#8217;, where n is
the number of samples and k is the number of subsamples.</dd>
</dl>
<p><strong>References</strong></p>
<ul class="simple">
<li>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009
Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang
<a class="reference external" href="http://www.math.iupui.edu/~hpeng/MTSE_0908.pdf">http://www.math.iupui.edu/~hpeng/MTSE_0908.pdf</a></li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>TheilSenRegressorSklearnNode</strong></li>
<li><strong>TheilSenRegressorSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-truncatedsvdtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.TruncatedSVDTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.TruncatedSVDTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.TruncatedSVDTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-truncatedsvdtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.TruncatedSVDTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">TruncatedSVDTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.TruncatedSVDTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Dimensionality reduction using truncated SVD (aka LSA).</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.truncated_svd.TruncatedSVD.html">sklearn.decomposition.truncated_svd.TruncatedSVD</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This transformer performs linear dimensionality reduction by means of
truncated singular value decomposition (SVD). It is very similar to PCA,
but operates on sample vectors directly, instead of on a covariance matrix.
This means it can work with scipy.sparse matrices efficiently.</p>
<p>In particular, truncated SVD works on term count/tf-idf matrices as
returned by the vectorizers in sklearn.feature_extraction.text. In that
context, it is known as latent semantic analysis (LSA).</p>
<p>This estimator supports two algorithm: a fast randomized SVD solver, and
a &#8220;naive&#8221; algorithm that uses ARPACK as an eigensolver on (X * X.T) or
(X.T * X), whichever is more efficient.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>n_components</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, default = 2</span><dd>Desired dimensionality of output data.
Must be strictly less than the number of features.
The default value is useful for visualisation. For LSA, a value of
100 is recommended.</dd>
<dt>algorithm</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">string, default = &#8220;randomized&#8221;</span><dd>SVD solver to use. Either &#8220;arpack&#8221; for the ARPACK wrapper in SciPy
(scipy.sparse.linalg.svds), or &#8220;randomized&#8221; for the randomized
algorithm due to Halko (2009).</dd>
<dt>n_iter</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span><dd>Number of iterations for randomized SVD solver. Not used by ARPACK.</dd>
<dt>random_state</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">int or RandomState, optional</span><dd>(Seed for) pseudo-random number generator. If not given, the
numpy.random singleton is used.</dd>
<dt>tol</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Tolerance for ARPACK. 0 means machine precision. Ignored by randomized
SVD solver.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><code class="docutils literal"><span class="pre">components_</span></code> : array, shape (n_components, n_features)</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">explained_variance_ratio_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span><dd>Percentage of variance explained by each of the selected components.</dd>
<dt><code class="docutils literal"><span class="pre">explained_variance_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_components]</span><dd>The variance of the training samples transformed by a projection to
each component.</dd>
</dl>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">TruncatedSVD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="k">import</span> <span class="n">sparse_random_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">sparse_random_matrix</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
<span class="go">TruncatedSVD(algorithm=&#39;randomized&#39;, n_components=5, n_iter=5,</span>
<span class="go">        random_state=42, tol=0.0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span> 
<span class="go">[ 0.0782... 0.0552... 0.0544... 0.0499... 0.0413...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> 
<span class="go">0.279...</span>
</pre></div>
</div>
<p>See also</p>
<p>PCA
RandomizedPCA</p>
<p><strong>References</strong></p>
<p>Finding structure with randomness: Stochastic algorithms for constructing
approximate matrix decompositions
Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</p>
<p><strong>Notes</strong></p>
<p>SVD suffers from a problem called &#8220;sign indeterminancy&#8221;, which means the
sign of the <code class="docutils literal"><span class="pre">components_</span></code> and the output from transform depend on the
algorithm and random state. To work around this, fit instances of this
class to data once, then keep the instance around to do transformations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>TruncatedSVDTransformerSklearnNode</strong></li>
<li><strong>TruncatedSVDTransformerSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-variancethresholdtransformersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.VarianceThresholdTransformerSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.VarianceThresholdTransformerSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.VarianceThresholdTransformerSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-variancethresholdtransformersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.VarianceThresholdTransformerSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">VarianceThresholdTransformerSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.VarianceThresholdTransformerSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Feature selector that removes all low-variance features.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.variance_threshold.VarianceThreshold.html">sklearn.feature_selection.variance_threshold.VarianceThreshold</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<p>This feature selection algorithm looks only at the features (X), not the
desired outputs (y), and can thus be used for unsupervised learning.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>threshold</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span><dd>Features with a training-set variance lower than this threshold will
be removed. The default is to keep all features with non-zero variance,
i.e. remove the features that have the same value in all samples.</dd>
</dl>
<p><strong>Attributes</strong></p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">variances_</span></code></dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,)</span><dd>Variances of individual features.</dd>
</dl>
<p><strong>Examples</strong></p>
<p>The following dataset has integer features, two of which are the same
in every sample. These are removed with the default setting for threshold:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[2, 0],</span>
<span class="go">       [1, 4],</span>
<span class="go">       [1, 1]])</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>VarianceThresholdTransformerSklearn</strong></li>
<li><strong>VarianceThresholdTransformerSklearnNode</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pyspace-missions-nodes-scikit-nodes-votingclassifiersklearnnode">
<h3><a class="reference internal" href="#pySPACE.missions.nodes.scikit_nodes.VotingClassifierSklearnNode" title="pySPACE.missions.nodes.scikit_nodes.VotingClassifierSklearnNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.scikit_nodes.VotingClassifierSklearnNode</span></code></a><a class="headerlink" href="#pyspace-missions-nodes-scikit-nodes-votingclassifiersklearnnode" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pySPACE.missions.nodes.scikit_nodes.VotingClassifierSklearnNode">
<em class="property">class </em><code class="descclassname">pySPACE.missions.nodes.scikit_nodes.</code><code class="descname">VotingClassifierSklearnNode</code><span class="sig-paren">(</span><em>input_dim=None</em>, <em>output_dim=None</em>, <em>dtype=None</em>, <em>class_labels=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pySPACE.missions.nodes.scikit_nodes.VotingClassifierSklearnNode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="api/generated/pySPACE.missions.nodes.base_node.html#pySPACE.missions.nodes.base_node.BaseNode" title="pySPACE.missions.nodes.base_node.BaseNode"><code class="xref py py-class docutils literal"><span class="pre">pySPACE.missions.nodes.base_node.BaseNode</span></code></a></p>
<p>Soft Voting/Majority Rule classifier for unfitted estimators.</p>
<p>This node has been automatically generated by wrapping the
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.voting_classifier.VotingClassifier.html">sklearn.ensemble.voting_classifier.VotingClassifier</a> class
from the <code class="docutils literal"><span class="pre">sklearn</span></code> library.  The wrapped instance can be accessed
through the <code class="docutils literal"><span class="pre">scikit_alg</span></code> attribute.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p><strong>Parameters</strong></p>
<dl class="docutils">
<dt>estimators</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">list of (string, estimator) tuples</span><dd>Invoking the <code class="docutils literal"><span class="pre">fit</span></code> method on the <code class="docutils literal"><span class="pre">VotingClassifier</span></code> will fit clones
of those original estimators that will be stored in the class attribute
<cite>self.estimators_</cite>.</dd>
<dt>voting</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">str, {&#8216;hard&#8217;, &#8216;soft&#8217;} (default=&#8217;hard&#8217;)</span><dd>If &#8216;hard&#8217;, uses predicted class labels for majority rule voting.
Else if &#8216;soft&#8217;, predicts the class label based on the argmax of
the sums of the predicted probalities, which is recommended for
an ensemble of well-calibrated classifiers.</dd>
<dt>weights</dt>
 <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_classifiers], optional (default=`None`)</span><dd>Sequence of weights (<cite>float</cite> or <cite>int</cite>) to weight the occurances of
predicted class labels (<cite>hard</cite> voting) or class probabilities
before averaging (<cite>soft</cite> voting). Uses uniform weights if <cite>None</cite>.</dd>
</dl>
<p><strong>Attributes</strong></p>
<p><code class="docutils literal"><span class="pre">classes_</span></code> : array-like, shape = [n_predictions]</p>
<p><strong>Examples</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">eclf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">eclf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>       <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>       <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">eclf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">POSSIBLE NODE NAMES:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>VotingClassifierSklearnNode</strong></li>
<li><strong>VotingClassifierSklearn</strong></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">POSSIBLE INPUT TYPES:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first last simple">
<li><strong>FeatureVector</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="api/generated/pySPACE.tools.live.ipmarkers.html" title="ipmarkers"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">pySPACE documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="content.html" >Table of Contents</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, pySPACE Developer Team.
      Last updated on Mar 14, 2017.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
    </div>
  </body>
</html>